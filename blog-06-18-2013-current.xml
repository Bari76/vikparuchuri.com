<?xml version='1.0' encoding='UTF-8'?><?xml-stylesheet href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns='http://www.w3.org/2005/Atom' xmlns:openSearch='http://a9.com/-/spec/opensearchrss/1.0/' xmlns:georss='http://www.georss.org/georss' xmlns:gd='http://schemas.google.com/g/2005' xmlns:thr='http://purl.org/syndication/thread/1.0'><id>tag:blogger.com,1999:blog-8712623323474199769.archive</id><updated>2013-06-03T09:05:29.289-07:00</updated><title type='text'>R, Ruby, and Finance</title><link rel='http://schemas.google.com/g/2005#feed' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/archive'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/archive'/><link rel='http://schemas.google.com/g/2005#post' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/archive'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><generator version='7.00' uri='http://www.blogger.com'>Blogger</generator><entry><id>tag:blogger.com,1999:blog-8712623323474199769.layout</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#template'/><title type='text'>Template: R, Ruby, and Finance</title><content type='text'>&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html expr:dir='data:blog.languageDirection' lang='en' xml:lang='en' xmlns='http://www.w3.org/1999/xhtml' xmlns:b='http://www.google.com/2005/gml/b' xmlns:data='http://www.google.com/2005/gml/data' xmlns:expr='http://www.google.com/2005/gml/expr' xmlns:fb='http://ogp.me/ns/fb#' xmlns:og='http://ogp.me/ns#'&gt;
&lt;head profile='http://a9.com/-/spec/opensearch/1.1/'&gt;
&lt;title&gt;&lt;data:blog.title/&gt;&lt;/title&gt;
&lt;b:if cond='data:blog.pageType == &amp;quot;index&amp;quot;'&gt;
&lt;link href='http://vikparuchuri.com/' rel='canonical'/&gt;
&lt;b:else/&gt;
&lt;link expr:href='&amp;quot;http://vikparuchuri.com/?br=&amp;quot; + data:blog.url' rel='canonical'/&gt;		
&lt;/b:if&gt;
&lt;script type='text/javascript'&gt;
var wpblog = &amp;quot;http://vikparuchuri.com/?br=&amp;quot;;
wpblog = wpblog  + window.location.href;
window.location.replace(wpblog);
&lt;/script&gt;
&lt;b:skin&gt;&lt;![CDATA[/*
-----------------------------------------------
Blogger Template Style
Name:     Blogger 301 redirect
Designer: Sudipto Pratap Mahato
URL:     http://techxt.com
Started:     07:41 1/1/2013
----------------------------------------------- */
/*
]]&gt;&lt;/b:skin&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;b:section id='header' showaddelement='no'&gt;
  &lt;b:widget id='Header1' locked='true' title='R, Ruby, and Finance (Header)' type='Header'&gt;
    &lt;b:includable id='main'&gt;

  &lt;b:if cond='data:useImage'&gt;
    &lt;b:if cond='data:imagePlacement == &amp;quot;BEHIND&amp;quot;'&gt;
      &lt;!--
      Show image as background to text. You can't really calculate the width
      reliably in JS because margins are not taken into account by any of
      clientWidth, offsetWidth or scrollWidth, so we don't force a minimum
      width if the user is using shrink to fit.
      This results in a margin-width's worth of pixels being cropped. If the
      user is not using shrink to fit then we expand the header.
      --&gt;
      &lt;b:if cond='data:mobile'&gt;
          &lt;div id='header-inner'&gt;
            &lt;div class='titlewrapper' style='background: transparent'&gt;
              &lt;h1 class='title' style='background: transparent; border-width: 0px'&gt;
                &lt;b:include name='title'/&gt;
              &lt;/h1&gt;
            &lt;/div&gt;
            &lt;b:include name='description'/&gt;
          &lt;/div&gt;
        &lt;b:else/&gt;
          &lt;div expr:style='&amp;quot;background-image: url(\&amp;quot;&amp;quot; + data:sourceUrl + &amp;quot;\&amp;quot;); &amp;quot;                        + &amp;quot;background-position: &amp;quot;                        + data:backgroundPositionStyleStr + &amp;quot;; &amp;quot;                        + data:widthStyleStr                        + &amp;quot;min-height: &amp;quot; + data:height                        + &amp;quot;_height: &amp;quot; + data:height                        + &amp;quot;background-repeat: no-repeat; &amp;quot;' id='header-inner'&gt;
            &lt;div class='titlewrapper' style='background: transparent'&gt;
              &lt;h1 class='title' style='background: transparent; border-width: 0px'&gt;
                &lt;b:include name='title'/&gt;
              &lt;/h1&gt;
            &lt;/div&gt;
            &lt;b:include name='description'/&gt;
          &lt;/div&gt;
        &lt;/b:if&gt;
    &lt;b:else/&gt;
      &lt;!--Show the image only--&gt;
      &lt;div id='header-inner'&gt;
        &lt;a expr:href='data:blog.homepageUrl' style='display: block'&gt;
          &lt;img expr:alt='data:title' expr:height='data:height' expr:id='data:widget.instanceId + &amp;quot;_headerimg&amp;quot;' expr:src='data:sourceUrl' expr:width='data:width' style='display: block'/&gt;
        &lt;/a&gt;
        &lt;!--Show the description--&gt;
        &lt;b:if cond='data:imagePlacement == &amp;quot;BEFORE_DESCRIPTION&amp;quot;'&gt;
          &lt;b:include name='description'/&gt;
        &lt;/b:if&gt;
      &lt;/div&gt;
    &lt;/b:if&gt;
  &lt;b:else/&gt;
    &lt;!--No header image --&gt;
    &lt;div id='header-inner'&gt;
      &lt;div class='titlewrapper'&gt;
        &lt;h1 class='title'&gt;
          &lt;b:include name='title'/&gt;
        &lt;/h1&gt;
      &lt;/div&gt;
      &lt;b:include name='description'/&gt;
    &lt;/div&gt;
  &lt;/b:if&gt;
&lt;/b:includable&gt;
    &lt;b:includable id='description'&gt;
  &lt;div class='descriptionwrapper'&gt;
    &lt;p class='description'&gt;&lt;span&gt;&lt;data:description/&gt;&lt;/span&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/b:includable&gt;
    &lt;b:includable id='title'&gt;
  &lt;b:if cond='data:blog.url == data:blog.homepageUrl'&gt;
    &lt;data:title/&gt;
  &lt;b:else/&gt;
    &lt;a expr:href='data:blog.homepageUrl'&gt;&lt;data:title/&gt;&lt;/a&gt;
  &lt;/b:if&gt;
&lt;/b:includable&gt;
  &lt;/b:widget&gt;
&lt;/b:section&gt;

&lt;div style='border:#ccc 1px solid; background:#eee; padding:20px; margin:80px;'&gt;
&lt;p&gt;This page has moved to a new address.&lt;/p&gt;
  &lt;a href='http://vikparuchuri.com/'&gt;&lt;data:blog.title/&gt;&lt;/a&gt;
  &lt;a expr:href='&amp;quot;http://vikparuchuri.com/br=&amp;quot; + data:blog.url'/&gt;
&lt;/div&gt;
&lt;a href='http://techxt.com'&gt;Blogger 301 Redirect Plugin&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/template/default'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/template/default'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_PUBLISHING_MODE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The type of publishing done for this blog.</title><content type='text'>PUBLISH_MODE_BLOGSPOT</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PUBLISHING_MODE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PUBLISHING_MODE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ADMIN_PERMISSION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The list of administrators' emails for the blog.</title><content type='text'>vik.paruchuri@gmail.com</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ADMIN_PERMISSION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ADMIN_PERMISSION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ADULT_CONTENT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether this blog contains adult content</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ADULT_CONTENT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ADULT_CONTENT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ALTERNATE_JSRENDER_ALLOWED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether alternate JS renderings are allowed</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ALTERNATE_JSRENDER_ALLOWED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ALTERNATE_JSRENDER_ALLOWED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ANALYTICS_ACCOUNT_NUMBER</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Blog's Google Analytics account number</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ANALYTICS_ACCOUNT_NUMBER'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ANALYTICS_ACCOUNT_NUMBER'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ARCHIVE_DATE_FORMAT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The number of the archive index date format</title><content type='text'>9</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ARCHIVE_DATE_FORMAT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ARCHIVE_DATE_FORMAT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_ARCHIVE_FREQUENCY</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>How frequently this blog should be archived</title><content type='text'>MONTHLY</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ARCHIVE_FREQUENCY'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_ARCHIVE_FREQUENCY'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_AUTHOR_PERMISSION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The list of authors' emails who have permission to publish.</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_AUTHOR_PERMISSION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_AUTHOR_PERMISSION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_BACKLINKS_ALLOWED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show comment backlinks on the blog</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_BACKLINKS_ALLOWED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_BACKLINKS_ALLOWED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_BY_POST_ARCHIVING</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to provide an archive page for each post</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_BY_POST_ARCHIVING'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_BY_POST_ARCHIVING'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_ACCESS</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Who can comment</title><content type='text'>REGISTERED</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_ACCESS'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_ACCESS'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_CAPTCHA</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to require commenters to complete a Captcha</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_CAPTCHA'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_CAPTCHA'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_EMAIL</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>List of e-mail addresses to send notifications of new comments to</title><content type='text'>vik.paruchuri@gmail.com</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_EMAIL'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_EMAIL'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_FEED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The type of feed to provide for blog comments</title><content type='text'>FULL</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_FEED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_FEED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_FORM_LOCATION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Blog comment form location</title><content type='text'>EMBEDDED_IFRAME</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_FORM_LOCATION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_FORM_LOCATION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_MESSAGE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Blog comment message</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MESSAGE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MESSAGE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_MODERATION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to enable comment moderation</title><content type='text'>DISABLED</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_MODERATION_DELAY</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Number of days after which new comments are subject to moderation</title><content type='text'>14</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION_DELAY'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION_DELAY'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_MODERATION_EMAIL</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Email address to send notifications of new comments needing moderation to</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION_EMAIL'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_MODERATION_EMAIL'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENT_PROFILE_IMAGES</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show profile images in comments</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_PROFILE_IMAGES'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENT_PROFILE_IMAGES'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENTS_ALLOWED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show comments</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENTS_ALLOWED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENTS_ALLOWED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_COMMENTS_TIME_STAMP_FORMAT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Comment time stamp format number</title><content type='text'>29</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENTS_TIME_STAMP_FORMAT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_COMMENTS_TIME_STAMP_FORMAT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_CONVERT_LINE_BREAKS</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to convert line breaks into &lt;br /&gt; tags in post editor</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CONVERT_LINE_BREAKS'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CONVERT_LINE_BREAKS'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_CUSTOM_PAGE_NOT_FOUND</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The content served when the requested post or page is not found.</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_PAGE_NOT_FOUND'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_PAGE_NOT_FOUND'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_CUSTOM_ROBOTS_TXT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The custom robots.txt content of the blog served to search engines.</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_ROBOTS_TXT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_ROBOTS_TXT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_CUSTOM_ROBOTS_TXT_ENABLED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether this blog serves custom robots.txt content to search engines.</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_ROBOTS_TXT_ENABLED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_CUSTOM_ROBOTS_TXT_ENABLED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_DATE_FORMAT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The number of the date header format</title><content type='text'>26</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DATE_FORMAT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DATE_FORMAT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_DEFAULT_BACKLINKS_MODE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Default backlinks mode for posts</title><content type='text'>DEFAULT_HAVE_BACKLINKS</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DEFAULT_BACKLINKS_MODE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DEFAULT_BACKLINKS_MODE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_DEFAULT_COMMENTS_MODE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Default comment mode for posts</title><content type='text'>DEFAULT_HAVE_COMMENTS</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DEFAULT_COMMENTS_MODE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DEFAULT_COMMENTS_MODE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_DESCRIPTION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>A description of the blog</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DESCRIPTION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_DESCRIPTION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_EMAIL_POST_LINKS</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show a link for users to e-mail posts</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_EMAIL_POST_LINKS'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_EMAIL_POST_LINKS'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_FEED_REDIRECT_URL</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>URL to redirect post feed requests to</title><content type='text'>http://vikparuchuri.com/feed</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_FEED_REDIRECT_URL'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_FEED_REDIRECT_URL'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_FLOAT_ALIGNMENT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether float alignment is enabled for the blog</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_FLOAT_ALIGNMENT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_FLOAT_ALIGNMENT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_LOCALE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Language for this blog</title><content type='text'>en</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_LOCALE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_LOCALE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_MAX_NUM</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Maximum number of things to show on the main page"</title><content type='text'>7</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_MAX_NUM'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_MAX_NUM'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_MAX_UNIT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Unit of things to show on the main page</title><content type='text'>POSTS</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_MAX_UNIT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_MAX_UNIT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_META_DESCRIPTION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The meta description of the blog served to search engines.</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_META_DESCRIPTION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_META_DESCRIPTION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_META_DESCRIPTION_ENABLED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether this blog is served with meta descriptions.</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_META_DESCRIPTION_ENABLED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_META_DESCRIPTION_ENABLED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_NAME</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The name of the blog</title><content type='text'>R, Ruby, and Finance</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_NAME'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_NAME'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_PER_POST_FEED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The type of feed to provide for per-post comments</title><content type='text'>FULL</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PER_POST_FEED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PER_POST_FEED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_POST_FEED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The type of feed to provide for blog posts</title><content type='text'>FULL</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_FEED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_FEED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_POST_FEED_FOOTER</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Footer to append to the end of each entry in the post feed</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_FEED_FOOTER'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_FEED_FOOTER'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_POST_TEMPLATE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The template for blog posts</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_TEMPLATE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_POST_TEMPLATE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_PROMOTED</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether this blog can be promoted on Blogger</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PROMOTED'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_PROMOTED'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_QUICK_EDITING</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether Quick Editing is enabled</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_QUICK_EDITING'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_QUICK_EDITING'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_READ_ACCESS_MODE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The access type for the readers of the blog.</title><content type='text'>PUBLIC</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_READ_ACCESS_MODE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_READ_ACCESS_MODE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_READER_PERMISSION</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The list of emails for users who have permission to read the blog.</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_READER_PERMISSION'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_READER_PERMISSION'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_SEARCHABLE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether this blog should be indexed by search engines</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SEARCHABLE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SEARCHABLE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_SEND_EMAIL</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Comma separated list of emails to send new blog posts to</title><content type='text'></content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SEND_EMAIL'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SEND_EMAIL'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_SHOW_TITLE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show the title field</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SHOW_TITLE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SHOW_TITLE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_SHOW_URL</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show a related link box in the post composer</title><content type='text'>false</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SHOW_URL'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SHOW_URL'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_SUBDOMAIN</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The BlogSpot subdomain under which to publish your blog</title><content type='text'>viksalgorithms</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SUBDOMAIN'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_SUBDOMAIN'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_TIME_STAMP_FORMAT</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The number of the time stamp format</title><content type='text'>27</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_TIME_STAMP_FORMAT'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_TIME_STAMP_FORMAT'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_TIME_ZONE</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>The time zone for this blog</title><content type='text'>America/Los_Angeles</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_TIME_ZONE'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_TIME_ZONE'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.settings.BLOG_USE_LIGHTBOX</id><published>2012-01-10T17:34:11.492-08:00</published><updated>2013-06-03T09:05:29.289-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#settings'/><title type='text'>Whether to show images in the Lightbox when clicked</title><content type='text'>true</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_USE_LIGHTBOX'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/settings/BLOG_USE_LIGHTBOX'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7711723266060788067</id><published>2012-08-09T13:38:00.000-07:00</published><updated>2012-08-09T13:38:51.640-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='ggplot2'/><category scheme='http://www.blogger.com/atom/ns#' term='data science'/><category scheme='http://www.blogger.com/atom/ns#' term='data analysis'/><category scheme='http://www.blogger.com/atom/ns#' term='Statistics'/><category scheme='http://www.blogger.com/atom/ns#' term='regression'/><category scheme='http://www.blogger.com/atom/ns#' term='chart'/><category scheme='http://www.blogger.com/atom/ns#' term='Kaggle'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>How Many Data Scientists Are There?</title><content type='html'>&lt;br /&gt;&lt;title&gt;How Many Data Scientists Are There?&lt;/title&gt;&lt;br /&gt;I've seen a lot of articles lately about “Big Data” and the looming “talent gap.”  &lt;a href="http://online.wsj.com/article/SB10001424052702304723304577365700368073674.html"&gt;This article&lt;/a&gt; from the Wall Street Journal is a good example.  It cites a McKinsey estimate that states that we will need 1.5 million more managers and analysts who are conversant with “big data.”  Of course, some of this is the media latching on the the next “big thing” (data), but some of it is true.  Even anecdotal evidence, such as the number of &lt;a href="http://www.indeed.com/jobs?q=data+science&amp;amp;l="&gt;job postings&lt;/a&gt; you find when you search for “data science,” indicates that there is a significant unmet demand for data analysis skills.&lt;br /&gt;&lt;br /&gt;This led me to wonder how we could quantify this gap, and once we figure out how to quantify it, if we can figure out if there has been a commensurate increase in the number of people with the skills to work with big data.&lt;br /&gt;&lt;br /&gt;This is interesting from the perspective of someone who works with data simply because I want to know the state of the field.  I am a pretty recent entrant into the area, and I would like to see more people get into it.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Potential Ways to Quantify Data Analysis Supply and Demand&lt;/span&gt;&lt;/h2&gt;There are a few different ways we can go in terms of this.&lt;br /&gt;&lt;ol&gt;&lt;li&gt;We can use Google Search trends to find the trend for the term “big data.”&lt;br /&gt;-Pros:  Easy.  &lt;a href="http://www.google.com/trends/?q=big+data&amp;amp;ctab=0&amp;amp;geo=all&amp;amp;date=all&amp;amp;sort=0"&gt;Look, I already did it.&lt;/a&gt;&lt;br /&gt;-Cons:  Trivial, doesn't really give us a way to disambiguate supply (number of data scientists) and demand (companies looking for analytical skills).&lt;br /&gt;&lt;/li&gt;&lt;li&gt;We can crawl job websites looking for jobs that mention data in some way.&lt;br /&gt;-Pros:  Probably a pretty comprehensive way to look at the “demand” side of the equation.  We can use number of days a job posting is up as a proxy for supply.&lt;br /&gt;-Cons:  Our proxy for supply will be pretty noisy because some job listings stay up forever, and when they are taken down doesn't necessarily correlate with when they are filled.  Also, this will give us very unstructured data, and will be very dependent on which job sites we crawl.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;We can look at Kaggle.com, which is a website that hosts data science competitions, to see how many people enter each competition, and how that count changes over time.&lt;br /&gt;-Pros:  Relatively structured data (competitor count).  Gives us a window into both sides of the market (supply is number of competitors, demand is number of companies hosting competitions).&lt;br /&gt;-Cons:  Not all companies have heard of Kaggle, and not all people who work with data have heard of Kaggle.  The numbers will be biased because competitor count is dependent on two things: people hearing about Kaggle, and people being interested in/having the skills to work with data.  We are only really interested in the second part, but it will be affected by the first part.&lt;/li&gt;&lt;/ol&gt;Ultimately, I chose to go with option 3, for a few reasons:&lt;br /&gt;&lt;br /&gt;-It is simpler than creating a crawler, and much less trivial than relying on Google Trends.&lt;br /&gt;-I am familiar with the platform.&lt;br /&gt;-While the competitor count is dependent to some extent on who has heard of Kaggle, I think that it is fair to say that as of a few months ago, most people who work with data had heard of Kaggle in some form (this is strictly anecdotal, though).&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Defining and Solving the Problem&lt;/span&gt;&lt;/h2&gt;So now, we still want to look at the supply of data scientists, and the demand for data scientists, but we want to do it within the context of Kaggle.com.&lt;br /&gt;&lt;br /&gt;If you are unfamiliar with Kaggle, it is a crowdsourcing platform that allows companies to host competitions on various aspects of their data.  For example, one company wanted individuals to predict bond prices in the future.  Competitors register for these competitions, and are ultimately awarded prize money based on their final position in the standings.&lt;br /&gt;&lt;br /&gt;So, this gives us an easy way to define supply and demand.  Supply can be defined as the number of competitors that are actively engaged in the Kaggle platform at any one time.  As each competition has an end date, a competitor will only be counted as “active” if he or she has an entry in a competition that has not ended yet.&lt;br /&gt;&lt;br /&gt;Demand can simply be counted as the number of competitions that are currently active.  This is a bit tricky, because it seems that demand for the Kaggle platform from the company side might be increasing faster than the overall demand for Big Data (given that Kaggle is relatively new and it takes time for word to circulate).&lt;br /&gt;&lt;br /&gt;Now, to define our procedure:&lt;br /&gt;&lt;br /&gt;Each Kaggle competition has a &lt;a href="http://www.kaggle.com/c/digit-recognizer/leaderboard"&gt;leaderboard&lt;/a&gt; associated with it.  The leaderboard lists all active participants, along with their rank in the competition.  Additionally, Kaggle allows for old leaderboards to be seen.&lt;br /&gt;&lt;br /&gt;Because we can see the leaderboard at various points in time, we can easily figure out how many active participants each competition had at any given time point.  Adding the unique users for each active competition will give us a count of active users at that time point.  If we do this for multiple time points across all competitions, we can figure out how the number of active users changed over time.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Active Participants by Competition&lt;/span&gt;&lt;/h2&gt;This leads us to a (fairly messy) chart that shows how each competition gained active participants over time.  As data was only scraped on a weekly basis, the figures might not be 100% accurate in terms of competition start times.  Once the number of competitors line becomes flat, it indicates that the competition is closed.  Generally, competitions that are higher up in the legend are more recent.&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-baD3VorptSA/UCQcXtq3DJI/AAAAAAAAANU/MXLejH3Tz9Q/s1600/chart1" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://4.bp.blogspot.com/-baD3VorptSA/UCQcXtq3DJI/AAAAAAAAANU/MXLejH3Tz9Q/s1600/chart1" /&gt;&lt;/a&gt;&lt;/div&gt;This is interesting within the context of Kaggle, but it doesn't really tell us much about the overall supply and demand for data scientists.  We can see that some of the older competitions gained participants faster than some of the newer ones, which may indicate that demand is outstripping supply, but we will need to look at the aggregate numbers across competitions to make judgements there.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Active Participants Overall&lt;/span&gt;&lt;/h2&gt;Now, we can aggregate the numbers, and look at unique active users across all competitions.  So if “Bob” is active in competition A and competition B at the same time, he is only counted once.&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-B3QeYeHO5rQ/UCQdEf3e39I/AAAAAAAAANg/P62kiLfwzrU/s1600/chart2" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-B3QeYeHO5rQ/UCQdEf3e39I/AAAAAAAAANg/P62kiLfwzrU/s1600/chart2" /&gt;&lt;/a&gt;&lt;/div&gt;As participants are no longer counted as “active” when a competition ends, we see some rather dramatic oscillations.  We can better understand these oscillations if we graph number of active competitions alongside number of unique participants.  I am scaling the number of competitions by a factor of 100 to make them appear legibly on the same chart as user count.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-xEDp4SUNMp4/UCQdPLSAQxI/AAAAAAAAANs/TuMLHudPL1A/s1600/chart3" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://3.bp.blogspot.com/-xEDp4SUNMp4/UCQdPLSAQxI/AAAAAAAAANs/TuMLHudPL1A/s1600/chart3" /&gt;&lt;/a&gt;&lt;/div&gt;We can see how closely related the number of active competitions is to the number of unique competitors.  In fact, the linear correlation between the two is .71.&lt;br /&gt;We can fit a linear model to the data and figure out what the expected number of users based solely on the number of active competitions should be.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-UIem55evKic/UCQdah8fBTI/AAAAAAAAAN4/bkfMdTu6KAk/s1600/chart4" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://4.bp.blogspot.com/-UIem55evKic/UCQdah8fBTI/AAAAAAAAAN4/bkfMdTu6KAk/s1600/chart4" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;We can see that they are pretty well correlated.  Most of the variation in the number of users seems to be explained by the number of competitions, although we do see that when competitions are close to ending, there appears to be large rush of participants.  Also, very recently (the month of July), we see that the actual number of unique active users is far below the expected number (ignore the very end of the chart, where all numbers drop to zero).&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;How Quickly Competitions Attract Participants&lt;/span&gt;&lt;/h2&gt;Another way to look at the supply of data scientists versus demand is to see if more recent competitions (that have come about in a time when there are more active competitions overall) gain participants more slowly or more quickly than previous competitions.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-Q-qiywz0qMA/UCQdkg0xBOI/AAAAAAAAAOE/zU30befvDyg/s1600/chart5" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-Q-qiywz0qMA/UCQdkg0xBOI/AAAAAAAAAOE/zU30befvDyg/s1600/chart5" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This plot shows us how many users per day each competition attracts, and how that has changed over time.  Although it may look like there is a trend in this plot (particularly towards the very end, when slopes are small), there is no significant correlation between date of competition launch and number of users gained per day.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Total Participants Over Time&lt;/span&gt;&lt;/h2&gt;In previous charts, we looked at unique active users or unique participants over time.  We can also look at aggregate number of unique users over time–the total number of unique individuals who have submitted an entry to any Kaggle competition.  This shows us how the platform is growing.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-UXPNIZ3G3i8/UCQduky16NI/AAAAAAAAAOQ/JPqIVCcHaLY/s1600/chart6" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-UXPNIZ3G3i8/UCQduky16NI/AAAAAAAAAOQ/JPqIVCcHaLY/s1600/chart6" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;New Users Over Time&lt;/span&gt;&lt;/h2&gt;We can figure out new users at each weekly time period (users who have submitted entries in the week who did not submit any previous entries).&lt;br /&gt;Graphing this allows us to see how the community is growing and expanding.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-0KXpf3i015Y/UCQd45R9vII/AAAAAAAAAOc/5MFe7bQ4tEI/s1600/chart7" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://3.bp.blogspot.com/-0KXpf3i015Y/UCQd45R9vII/AAAAAAAAAOc/5MFe7bQ4tEI/s1600/chart7" /&gt;&lt;/a&gt;&lt;/div&gt;We can see that the number of new users each week is somewhat correlated with the competition count, and has remained somewhat steady over the past few months.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Some Random Observations&lt;/span&gt;&lt;/h2&gt;Okay, so we have some data and some (maybe) pretty pictures, but what does it tell us?  We can gain some insight from this.&lt;br /&gt;&lt;ol&gt;&lt;li&gt;There are clearly some competitions that are favored strongly over others.  As someone who has participated in a lot of Kaggle competitions, I can safely say that these are competitions that have an interesting premise, the potential for an interesting opportunity (KDD Cup/Facebook Recruiting), or have data that is easy to work with in terms of format and presentation (Bio response). &lt;/li&gt;&lt;li&gt;It seems like the average number of competitors per competition has been pretty constant, even as Kaggle has ramped up their number of concurrent competitions.  Some recent competitions have not seen much uptake, but that could be the combination of several relatively insignificant factors (summer, uninteresting competitions, etc), rather than signalling that we have reached capacity. &lt;/li&gt;&lt;li&gt;As we can see by the number of people who have submitted an entry overall vs the number of people who are active at any given time, there is a large data science community that only uses Kaggle when they see something interesting.  As fresh people begin to compete, it seems that older users stop using the platform, whether it be from boredom, lack of time, or another issue.  This keeps the number of active unique users much more constant (slow growth) than the aggregate number of users.&lt;br /&gt;This points to there being constant new entrants into the data science world, at least within the context of Kaggle, but it is hard to figure out if these entrants are new to data science entirely, or simply new to the Kaggle platform.  The Kaggle forums suggest a mixture of the two.&lt;/li&gt;&lt;li&gt;Along with a rising overall supply of data scientists, we have also seen rising demand, as the number of competitions has been steadily increasing.  This could simply reflect rising interest in the Kaggle platform, but it might also point to a rising interest in data science at the corporate level.&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;span style="font-size: large;"&gt;Inconclusive Conclusions&lt;/span&gt;&lt;/h2&gt;It's hard to generalize from this data, as it call came within the context of a platform.  You can think of Kaggle as a fisherman that has gradually invested in better technology and better bait.  Over time, more data scientists and more companies have been “caught,” but whether that reflects the better bait, or the fact that the number of fish in the ocean is increasing, it is hard to say.&lt;br /&gt;So what can we conclude?  A few key items jump off the page:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;People will enter the field of data science, but only if they can find something interesting/rewarding to work on.  We see a lot of active unique entrants in a few competitions that have low barriers to entry or offer commensurately high rewards.  We also see a rising amount of new users surrounding particularly interesting competitions.&lt;/li&gt;&lt;li&gt;Problems that are less exciting, or perhaps less accessible, may need to be reformulated to appeal to the mainstream data community, and crossovers from other fields.  If a company wants to attract high quality talent, they need to interest and engage them.  We see a lot of competitions get very little traction.&lt;/li&gt;&lt;li&gt;The amount of new users on Kaggle seems fairly steady.  This may indicate that demand may soon outstrip supply, as more competitions are run without a commensurate increase in the number of participants, but it does seem like the number of participants and competition count is pretty correlated.&lt;br /&gt;The fact that there is a constant stream of new users is also encouraging, because, anecdotally, most people in the data community heard about Kaggle months ago.  This indicates that both existing data scientists are always looking for interesting problems to tackle, and that new people are moving into data science as they see interesting problems.&lt;/li&gt;&lt;li&gt;Corporate interest in data science overall seems to be increasing more quickly than the supply of new data scientists.&lt;/li&gt;&lt;/ol&gt;None of these are definitive, and the method used for analysis constrains the interpretability of the results.  Nonetheless, I think that there are some interesting threads here, and would love to hear anyone's thoughts on this.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/7711723266060788067/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html#comment-form' title='3 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7711723266060788067'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7711723266060788067'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html' title='How Many Data Scientists Are There?'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://4.bp.blogspot.com/-baD3VorptSA/UCQcXtq3DJI/AAAAAAAAANU/MXLejH3Tz9Q/s72-c/chart1' height='72' width='72'/><thr:total>3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786</id><published>2012-06-18T09:54:00.000-07:00</published><updated>2012-06-18T15:27:56.534-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='text mining'/><category scheme='http://www.blogger.com/atom/ns#' term='random indexing'/><category scheme='http://www.blogger.com/atom/ns#' term='england'/><category scheme='http://www.blogger.com/atom/ns#' term='RI'/><category scheme='http://www.blogger.com/atom/ns#' term='foreign service'/><category scheme='http://www.blogger.com/atom/ns#' term='wikileaks'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='sentiment analysis'/><category scheme='http://www.blogger.com/atom/ns#' term='diplomatic'/><category scheme='http://www.blogger.com/atom/ns#' term='nlp'/><category scheme='http://www.blogger.com/atom/ns#' term='sentiment'/><category scheme='http://www.blogger.com/atom/ns#' term='NER'/><category scheme='http://www.blogger.com/atom/ns#' term='natural language processing'/><category scheme='http://www.blogger.com/atom/ns#' term='US'/><category scheme='http://www.blogger.com/atom/ns#' term='afinn'/><category scheme='http://www.blogger.com/atom/ns#' term='diplomatic cables'/><category scheme='http://www.blogger.com/atom/ns#' term='named entity recognition'/><title type='text'>Tracking US Sentiments Over Time In Wikileaks</title><content type='html'>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I &lt;a href="http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html"&gt;recently posted&lt;/a&gt; about using the Wikileaks cable corpus to find word use patterns, both over time, and in secret cables vs unclassified cables.&lt;/p&gt; &lt;p&gt;I received a lot of good suggestions for further topics to pursue with the corpus, and probably the most interesting was the idea to do sentiment analysis over time on a variety of named entities.&lt;/p&gt; &lt;p&gt;Sentiment analysis is the process of discovering whether a writer feels negatively or positively about a topic.  Named entities in this case would be country names such as China and India, and the names of important world figures, such as Saddam Hussein or Tony Blair.&lt;/p&gt; &lt;p&gt;So, in essence, we are seeing how US diplomats, and by extension the US, felt about a variety of topics, and how those feelings changed over time, from the first available cables (1980&amp;#39;s) to present.&lt;/p&gt; &lt;p&gt;The goal is to get a chart like this one:&lt;/p&gt; &lt;p&gt;&lt;img src="http://1.bp.blogspot.com/-eb2_T1rSN80/T99K6VgXDrI/AAAAAAAAALw/5FoxdSgjQ34/s640/sentiment_teaser.png" alt="US Sentiment over Time"/&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How will we do this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful &lt;a href="http://en.wikipedia.org/wiki/Sentiment_analysis"&gt;sentiment analysis&lt;/a&gt; can be extremely complex at times, requiring a corpus of sentences to be mapped to sentiment scores.&lt;/p&gt; &lt;p&gt;In order to make this exercise simpler, I traded off some accuracy and used a word list instead (the &lt;a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"&gt;AFINN&lt;/a&gt; list).  This word list assigns a &amp;ldquo;sentiment score&amp;rdquo; of -5 to 5 to 2477 English words.  For example, the word adore has a score of 3, denoting a positive sentiment, whereas the word abhorred has a sentiment of -3, indicating negative sentiment.&lt;/p&gt; &lt;p&gt;Our next task is &lt;a href="http://en.wikipedia.org/wiki/Named-entity_recognition"&gt;named entity recognition&lt;/a&gt;.  We will use the AFINN word list in conjunction with a list of named entities.  Named entities in this case would be important topics from the news, so we will use the &lt;a href="http://langtech.jrc.it/JRC-Names.html"&gt;JRC-Names&lt;/a&gt; word list, which pulls out important keywords from news articles.  We will use these keywords to define our topics.  For example, &amp;ldquo;China&amp;rdquo; is a keyword, as is &amp;ldquo;India&amp;rdquo;.  These are the topics that we will analyze sentiment for.&lt;/p&gt; &lt;p&gt;Now, in order to find the sentiment for a given topic, we will need to find out whether it appears in conjunction with negative or positive words.  For example, the phrase &amp;ldquo;China abandoned an environmental project&amp;rdquo; would indicate negative sentiment, whereas &amp;ldquo;China is building partnerships&amp;rdquo; would indicate positive sentiment.  In order to do this, we will need to find out when our topic words (named entities) and our words that indicate sentiment appear together in a sentence.&lt;/p&gt; &lt;p&gt;To accomplish this, we can use a technique called &lt;a href="http://www.idi.ntnu.no/%7Egamback/teaching/TDT4138/sahlgren05.pdf"&gt;random indexing&lt;/a&gt; which allows us to build up a matrix that shows how topic words and sentiment words occur together.  I opted to use random indexing because it builds a relatively small matrix in terms of dimensionality, and it allows us to capture information on a fairly granular level.  The optimal method would be to create a full Term-Document matrix and decompose it to find relations, but it is impractical in this case due to the high sentence count.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our plan&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now that we have all the prelimiaries, here is a high-level look at what we will do:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Get cables for multiple time periods from the database  &lt;ul&gt;&lt;li&gt;Because there are more cables from 2000 onwards than from pre-2000, we will define 5 year time periods from 1985 to 2000, and 1 year time periods after.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Split the cables into sentences.&lt;/li&gt;&lt;li&gt;Build up matrices using random indexing that contain the topic words from JRC-Names and the sentiment words from AFINN.&lt;/li&gt;&lt;li&gt;Use cosine similarity measures to see how often topic words occur with negative/positive words.&lt;/li&gt;&lt;li&gt;Assign a final &amp;ldquo;sentiment score&amp;rdquo; to each topic for each time range.&lt;/li&gt;&lt;/ol&gt; &lt;p&gt;This plan will give us reasonable results.  Because of the way that we are doing sentiment analysis, it won&amp;#39;t be perfect (far from it), but it will show some interesting patterns, at least.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Formatting JRC-Names and AFINN&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;JRC-Names and AFINN are not in the best format for this (you will see when you download them), so we need to reformat them to get a character vector of topics.  The reformatting also needs to be done because cables frequently refer to people by only their last name and JRC-names contains a full name.  We need to make everything into 1-grams.&lt;/p&gt; &lt;pre&gt;&lt;code class="r"&gt;jrc_names &amp;lt;- read.delim(file = &amp;quot;entities.txt&amp;quot;, stringsAsFactors = FALSE)[, &lt;br /&gt;    4]&lt;br /&gt;bad_names &amp;lt;- grep(&amp;quot;[^\\w+]&amp;quot;, jrc_names, perl = TRUE)&lt;br /&gt;jrc_names &amp;lt;- jrc_names[-bad_names]&lt;br /&gt;jrc_names &amp;lt;- sapply(jrc_names, function(x) strsplit(x, &amp;quot;+&amp;quot;, fixed = TRUE))&lt;br /&gt;jrc_tab &amp;lt;- sort(table(tolower(unlist(jrc_names))), decreasing = TRUE)&lt;br /&gt;jrc_names &amp;lt;- names(jrc_tab)[jrc_tab &amp;gt; 2]&lt;br /&gt;jrc_names &amp;lt;- jrc_names[nchar(jrc_names) &amp;lt; 15 &amp;amp; nchar(jrc_names) &amp;gt; &lt;br /&gt;    2]&lt;br /&gt;&lt;br /&gt;afinn_list &amp;lt;- read.delim(file = &amp;quot;AFINN-111.txt&amp;quot;, header = FALSE, &lt;br /&gt;    stringsAsFactors = FALSE)&lt;br /&gt;names(afinn_list) &amp;lt;- c(&amp;quot;word&amp;quot;, &amp;quot;score&amp;quot;)&lt;br /&gt;afinn_list$word &amp;lt;- tolower(afinn_list$word)&lt;br /&gt;&lt;br /&gt;full_term_list &amp;lt;- c(jrc_names, afinn_list$word)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This code will remove non-English words from jrc-names, split it by the + sign that appears in each term, and reconstruct a vector in which only the terms that appear at least twice are included.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Defining Date Ranges&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We now need to define what date ranges we want our cables to come from.  Because there aren&amp;#39;t many cables available pre-2000, we will select 5 years at a time from 1985-2000.&lt;/p&gt; &lt;pre&gt;&lt;code class="r"&gt;date_min_list &amp;lt;- c(&amp;quot;1985&amp;quot;, &amp;quot;1990&amp;quot;, &amp;quot;1995&amp;quot;, &amp;quot;2000&amp;quot;, &amp;quot;2001&amp;quot;, &amp;quot;2002&amp;quot;, &lt;br /&gt;    &amp;quot;2003&amp;quot;, &amp;quot;2004&amp;quot;, &amp;quot;2005&amp;quot;, &amp;quot;2006&amp;quot;, &amp;quot;2007&amp;quot;, &amp;quot;2008&amp;quot;, &amp;quot;2009&amp;quot;)&lt;br /&gt;date_max_list &amp;lt;- c(&amp;quot;1990&amp;quot;, &amp;quot;1995&amp;quot;, &amp;quot;2000&amp;quot;, &amp;quot;2001&amp;quot;, &amp;quot;2002&amp;quot;, &amp;quot;2003&amp;quot;, &lt;br /&gt;    &amp;quot;2004&amp;quot;, &amp;quot;2005&amp;quot;, &amp;quot;2006&amp;quot;, &amp;quot;2007&amp;quot;, &amp;quot;2008&amp;quot;, &amp;quot;2009&amp;quot;, &amp;quot;2010&amp;quot;)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Generating Sentiment Scores&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, we need to follow our plan from above and have the code that generates our final sentiment scores.  The load or install function is documented &lt;a href="http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This code is very inefficient, so please feel free to improve it.  To get it to run on low-memory systems, you can lower the ri_cols or max_cables_to_sample attributes.  A higher ri_cols or max_cables_to_sample setting will be less memory efficient, but more accurate.&lt;/p&gt; &lt;p&gt;You can find the code for this &lt;a href="https://gist.github.com/2949153"&gt;here&lt;/a&gt;, as sentiment_score_generation.R.&lt;/p&gt; &lt;p&gt;This is a very long piece of code, but it is basically doing what our plan stated.  It is getting cables for each time period, splitting them into sentences, and finding out which sentiment words and topic words occur together.  It is then finding out which topic is associated with negative sentiment, and which is associated with positive sentiment, and then assigning a final score to each topic on that basis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Plotting the results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, we are ready to make plots indicating sentiment over time.&lt;/p&gt; &lt;p&gt;You can find the plotting code &lt;a href="https://gist.github.com/2949153"&gt;here&lt;/a&gt;, as sentiment_plot.R.&lt;/p&gt; &lt;p&gt;This generates the following plot:&lt;/p&gt; &lt;p&gt;&lt;img src="http://4.bp.blogspot.com/-JbjHz9LzzMY/T99OHXQUedI/AAAAAAAAAL8/eHMRiOUgkD0/s640/sentiment_middle_east.png" alt="US Sentiments-Middle East"/&gt;&lt;/p&gt; &lt;p&gt;The black line indicates the mean sentiment by year.  You can see that the average US sentiment dips around 2003 (the year on the x-axis is the ending year for the gathered cables, so 2010 would be cables from January 1st, 2009 to January 1st, 2010, for example).  This is likely due to countries not supporting the US war effort in Iraq.  If you have a better interpretation, I would love to hear it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More country plots&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here are US sentiments towards the english speaking world.  &amp;ldquo;New Zealand&amp;rdquo; becomes &amp;ldquo;Zealand&amp;rdquo; because we are only dealing with 1-grams:&lt;/p&gt; &lt;p&gt;&lt;img src="http://2.bp.blogspot.com/-8T9mL8zr84U/T99PXxBpyBI/AAAAAAAAAME/p1tPGH7eWsI/s640/sentiment_english_speaking.png" alt="US Sentiments-English World"/&gt;&lt;/p&gt; &lt;p&gt;You can see that we seem to have much better sentiment towards the English speaking world, overall.&lt;/p&gt; &lt;p&gt;Here are US Sentiments towards some of the countries with recent protests/overthrows.  Tripoli is a proxy for Libya, and Tunis is a proxy for Tunisia, because those terms did not seem to make it into the JRC-names list that we constructed:&lt;/p&gt; &lt;p&gt;&lt;img src="http://4.bp.blogspot.com/-ow05OVpSu9A/T99SQockUQI/AAAAAAAAAMU/di5kEdDCzYY/s640/sentiment_aspring.png" alt="US Sentiments-Arab Spring"/&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="http://4.bp.blogspot.com/-w5RjDsJYU9Q/T99XTTmw_tI/AAAAAAAAAMo/JvdZMpklsh0/s640/sentiment_europe.png" alt="US Sentiments - Europe"/&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="http://3.bp.blogspot.com/-tw-E3dT7Lj0/T99ZOfrMJ0I/AAAAAAAAAMw/8unDtARu9Bk/s640/sentiment_asia.png" alt="US Sentiments - Asia"/&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Country Interpretation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The US seems to have slightly negative sentiment towards every country, particularly after 2003.  This could be due to many factors:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Countries not supporting the Iraq war.&lt;/li&gt;&lt;li&gt;A change from Madeline Albright (1997-2001) to Colin Powell (2001-2005) to Condoleeza Rice (2005-2009).  Perhaps their attitudes shaped the attitudes of the cable writers.&lt;/li&gt;&lt;li&gt;Changes in administration from Bill Clinton (1993-2001) to George Bush (2001-2009) to Barack Obama (2009-).  The attitude of the President can definitely impact cable writing, as I can attest, and you can see some upticks in sentiment from 2009-2010, when Obama took office.&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Personally, I think that the war may have been the biggest factor in the changing cable language, but this is just speculation, so I would love to hear any ideas on this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;World Figure Plots&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, we can also plot major world figures:&lt;/p&gt; &lt;p&gt;&lt;img src="http://1.bp.blogspot.com/-bhiLFzAHYyE/T99aCQHZ_NI/AAAAAAAAAM4/qU91jurRcB0/s640/sentiment_dictators.png" alt="US Sentiments- Dictators"/&gt;&lt;/p&gt; &lt;p&gt;The above are some of the ex-dictators that have been in the news lately.  You can see some very interesting patterns (Hussein becomes associated with very negative sentiment right when the second Iraq war starts, for example).&lt;/p&gt; &lt;p&gt;Here are US Sentiments towards some world leaders:&lt;/p&gt; &lt;p&gt;&lt;img src="http://2.bp.blogspot.com/-W78qQOClwxw/T99bO0xKakI/AAAAAAAAANA/I-K53tbjbDc/s640/sentiment_worldl.png" alt="US Sentiments- World Leaders"/&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;World figure interpretation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The US seems to have some strange sentiments towards world figures/leaders.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The dictators do not seem to have been universally reviled prior to their ousters.&lt;/li&gt;&lt;li&gt;Sentiment seems to be improving from 2009-2010 (perhaps due to Obama taking office).&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Any more interpretation/thoughts would be appreciated!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This has been a very interesting post for me, and I hope that it can be built upon.  Please let me know your thoughts, and/or if you would like to see any different analyses done.&lt;/p&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/5153720746573977786/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html#comment-form' title='6 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' title='Tracking US Sentiments Over Time In Wikileaks'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://1.bp.blogspot.com/-eb2_T1rSN80/T99K6VgXDrI/AAAAAAAAALw/5FoxdSgjQ34/s72-c/sentiment_teaser.png' height='72' width='72'/><thr:total>6</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-2963365471469555314</id><published>2012-06-12T12:05:00.000-07:00</published><updated>2012-06-12T12:05:32.628-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='basketball'/><category scheme='http://www.blogger.com/atom/ns#' term='okc'/><category scheme='http://www.blogger.com/atom/ns#' term='Miami'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='finals'/><category scheme='http://www.blogger.com/atom/ns#' term='Thunder'/><category scheme='http://www.blogger.com/atom/ns#' term='Oklahoma City'/><category scheme='http://www.blogger.com/atom/ns#' term='game 1'/><category scheme='http://www.blogger.com/atom/ns#' term='nba finals'/><category scheme='http://www.blogger.com/atom/ns#' term='mia'/><category scheme='http://www.blogger.com/atom/ns#' term='Heat'/><title type='text'>NBA Predictions -- Finals</title><content type='html'>Now we are on to the finals!  The algorithm enters the finals with a 6-4 record so far.  Here is what we have for tonight:&lt;br/&gt;&lt;br/&gt; &lt;a href="http://3.bp.blogspot.com/-8hffMBdvKuE/T9eSqAxRLLI/AAAAAAAAAJc/hP98sjHToXQ/s1600/update_7.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="65" width="400" src="http://3.bp.blogspot.com/-8hffMBdvKuE/T9eSqAxRLLI/AAAAAAAAAJc/hP98sjHToXQ/s400/update_7.png" /&gt;&lt;/a&gt;&lt;br/&gt;&lt;br/&gt; So, let's see if OKC wins this one.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/2963365471469555314/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-predictions-finals.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/2963365471469555314'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/2963365471469555314'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-predictions-finals.html' title='NBA Predictions -- Finals'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-8hffMBdvKuE/T9eSqAxRLLI/AAAAAAAAAJc/hP98sjHToXQ/s72-c/update_7.png' height='72' width='72'/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-9007696286801530443</id><published>2012-06-12T11:10:00.000-07:00</published><updated>2012-06-18T12:04:11.778-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='text mining'/><category scheme='http://www.blogger.com/atom/ns#' term='cables'/><category scheme='http://www.blogger.com/atom/ns#' term='word cloud'/><category scheme='http://www.blogger.com/atom/ns#' term='ggplot'/><category scheme='http://www.blogger.com/atom/ns#' term='wordcloud'/><category scheme='http://www.blogger.com/atom/ns#' term='unclassified'/><category scheme='http://www.blogger.com/atom/ns#' term='government'/><category scheme='http://www.blogger.com/atom/ns#' term='tm'/><category scheme='http://www.blogger.com/atom/ns#' term='foreign service'/><category scheme='http://www.blogger.com/atom/ns#' term='wikileaks'/><category scheme='http://www.blogger.com/atom/ns#' term='classified'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='diplomatic'/><category scheme='http://www.blogger.com/atom/ns#' term='nlp'/><category scheme='http://www.blogger.com/atom/ns#' term='chisq'/><category scheme='http://www.blogger.com/atom/ns#' term='fso'/><category scheme='http://www.blogger.com/atom/ns#' term='natural language processing'/><category scheme='http://www.blogger.com/atom/ns#' term='diplomatic cables'/><category scheme='http://www.blogger.com/atom/ns#' term='significance'/><title type='text'>Finding word use patterns in Wikileaks cables</title><content type='html'>6/18: A follow-up to this post is now available &lt;a href=http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html&gt;here&lt;/a&gt;.&lt;br/&gt; &lt;br /&gt;&lt;b&gt;Recent Discoveries&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;When I was a diplomat, I was always interested in the Wikileaks cables and what could be done with them.  Unfortunately, I never got a chance to look at the site in depth, due to security policies.  Now that the ex- is firmly prepended to diplomat in my resume, I think that I am finally ready to take that step.  &lt;br /&gt;I recently realized that the wikileaks cables are available in a handy .sql file online.  This of course allowed me to download all 250,000 and import them into a database table (I used psql and the /i command).  &lt;br /&gt;If you are interested in obtaining the cables for yourself, you will need to download the torrent from &lt;a href="http://file.wikileaks.org/torrent/cable_db_full.7z.torrent"&gt;here&lt;/a&gt;.&lt;br /&gt;Let me just clarify here that I will not be printing the text of any of these cables (which has been done in several newspapers), and that I will not be using any data that is not readily publicly available online.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;That's great, but what can we do with them?&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;After I had the cables, I brainstormed to see what I could actually do with them that would be interesting.  I came up with a few ideas:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Find how topics have changed over time.  &lt;ul&gt;&lt;li&gt;It's reasonable to assume that the focus of the cables would have shifted from “Soviet Union” this and “USSR” that to the Middle East.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Find out what words typify State Department writers.  &lt;ul&gt;&lt;li&gt;Anyone who has read cables knows that while they are (mostly) in English, its a strange kind of English.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Find out what words/topics typify secret/classified vs unclassified cables.  &lt;ul&gt;&lt;li&gt;What topics are more likely to be classified?  Does word choice change in classified vs unclassified cables?&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;I will get into these topics and more as we continue on through this post.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Starting to work with the data&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The first thing we need to do is read the data from a database.  I interfaced with my PostgreSQL database via ODBC.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;channel &amp;lt;- odbcConnect(db_name, uid = "", pwd = "")&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;Now, let's get all the cables from 2010 onwards:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;cable_frame &amp;lt;- sqlQuery(channel, "SELECT * from cable WHERE date &amp;gt; '2010-01-01'", &lt;br /&gt;    stringsAsFactors = FALSE, errors = TRUE)&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;We can make a plot of which senders sent the most cables from 2010 onwards:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;last_10 &amp;lt;- tail(sort(table(cable_frame$origin)), 10)&lt;br /&gt;qplot(names(last_10), last_10, geom = "bar") + opts(axis.title.x = theme_blank()) + &lt;br /&gt;    opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))&lt;/code&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br /&gt;&lt;a href="http://4.bp.blogspot.com/-DSL_1sfbs8I/T9eXJyytdbI/AAAAAAAAAJo/OsFzWmmHGvw/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://4.bp.blogspot.com/-DSL_1sfbs8I/T9eXJyytdbI/AAAAAAAAAJo/OsFzWmmHGvw/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;We can see that the Secretary of State and Embassy Baghdad are the two biggest offenders.&lt;br /&gt;&lt;br /&gt;Now, we can get all of the cables in the database and see how cable traffic changed over time (or perhaps Wikileaks had a biased sample):&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;all_cables &amp;lt;- sqlQuery(channel, "SELECT * from cable", stringsAsFactors = FALSE, &lt;br /&gt;    errors = TRUE)&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;date_tab &amp;lt;- table(as.POSIXlt(all_cables$date)$year + 1900)&lt;br /&gt;qplot(names(date_tab), as.numeric(date_tab), geom = "bar") + opts(axis.title.x = theme_blank()) + &lt;br /&gt;    opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))&lt;/code&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br /&gt;&lt;a href="http://2.bp.blogspot.com/-wWYebnBy87c/T9eXSLGuNQI/AAAAAAAAAJw/avRCWMpn1rc/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-wWYebnBy87c/T9eXSLGuNQI/AAAAAAAAAJw/avRCWMpn1rc/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;The amount of cables rises almost exponentially from 2000 until 2009.  I'm assuming that only some of the cables for 2010 were leaked, explaining the low count there.&lt;br /&gt;&lt;br /&gt;We can get rid of the all_cables file, as we won't need it going forward:&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;rm(all_cables)&lt;br /&gt;gc()&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;&lt;b&gt;Comparing word usage in the 80's and 90's to word usage today&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Now, we can get to something interesting:  we can compare how word usage/topics shifted from 1980-1995 to today.  Because there are relatively few cables from early on, we have to specify a 15 year range, which nets us only around 675 cables.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;cable_present &amp;lt;- sqlQuery(channel, "SELECT * from cable WHERE date &amp;gt; '2010-02-15'", &lt;br /&gt;    stringsAsFactors = FALSE, errors = TRUE)&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;cable_past &amp;lt;- sqlQuery(channel, "SELECT * from cable WHERE date &amp;gt; '1980-01-01' AND date &amp;lt; '1995-01-01'", &lt;br /&gt;    stringsAsFactors = FALSE, errors = TRUE)&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;Now, we have two challenges.  The cables all have line breaks and returns (\r and \n), and a lot of the older cables are in all caps.  We will get rid of these issues by removing the breaks/returns and converting everything to all lower case.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;ppatterns &amp;lt;- c("\\n", "\\r")&lt;br /&gt;combined &amp;lt;- tolower(gsub(paste("(", paste(ppatterns, collapse = "|"), &lt;br /&gt;    ")", sep = ""), "", c(cable_past$content, cable_present$content)))&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;Now, we can construct a term document matrix which counts the number of times each term occurs in each document:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;corpus &amp;lt;- Corpus(VectorSource(combined))&lt;br /&gt;corpus &amp;lt;- tm_map(corpus, stripWhitespace)&lt;br /&gt;cable_mat &amp;lt;- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf, &lt;br /&gt;    removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15))))&lt;br /&gt;cable_mat &amp;lt;- cable_mat[rowSums(cable_mat) &amp;gt; 3, ]&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;We remove any words that are under 4 characters or over 15 characters, and additionally remove any terms that appear less than 3 times in the whole group of cables.&lt;br /&gt;&lt;br /&gt;For convenience, we can split the matrix into one containing past cables and one containing current cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;present_mat &amp;lt;- cable_mat[, (nrow(cable_past) + 1):ncol(cable_mat)]&lt;br /&gt;past_mat &amp;lt;- cable_mat[, 1:nrow(cable_past)]&lt;br /&gt;rm(cable_mat)&lt;br /&gt;gc()&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;Now we can get to the good stuff and find differential word usage between the two sets of cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;chisq_vals &amp;lt;- chisq(rowSums(past_mat), ncol(past_mat) * 100, rowSums(present_mat), &lt;br /&gt;    ncol(present_mat) * 100)&lt;br /&gt;chisq_direction &amp;lt;- rep(-1, length(chisq_vals))&lt;br /&gt;mean_frame &amp;lt;- data.frame(past_mean = rowSums(past_mat)/ncol(past_mat), &lt;br /&gt;    present_mean = rowSums(present_mat)/ncol(present_mat))&lt;br /&gt;chisq_direction[mean_frame[, 2] &amp;gt; mean_frame[, 1]] &amp;lt;- 1&lt;br /&gt;chisq_vals &amp;lt;- chisq_vals * chisq_direction&lt;br /&gt;cloud_frame &amp;lt;- data.frame(word = rownames(present_mat), chisq = chisq_vals, &lt;br /&gt;    past_sum = rowSums(past_mat), present_sum = rowSums(present_mat))&lt;br /&gt;pal &amp;lt;- brewer.pal(9, "Set1")&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;The above code will calculate the statistical difference (chisq) between the terms in the first set of cables (1980-1995), and the second set (cables from february 2010).&lt;br /&gt;&lt;br /&gt;Now we can make some word clouds.  This first cloud contains words that appear in the 2010 cables in a more significant way than in the 1980-1995 cables.  A larger size indicates that it more significantly appears in the 2010 cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-B0lmUAwHLd4/T9eXk8kWDiI/AAAAAAAAAJ4/DFj78-BhKSM/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-B0lmUAwHLd4/T9eXk8kWDiI/AAAAAAAAAJ4/DFj78-BhKSM/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;This second cloud indicates the words that appear in a significant way in the 1980-1995 cables, but not in the 2010 cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;/code&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br /&gt;&lt;a href="http://2.bp.blogspot.com/-Bs8aANHfRz8/T9eXv1E_HzI/AAAAAAAAAKA/wIOCy6v71nk/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-Bs8aANHfRz8/T9eXv1E_HzI/AAAAAAAAAKA/wIOCy6v71nk/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;As we can see, february is very significant in the first plot, which is to be expected, because all of the cables are from february.  But, we can also see interesting patterns, like trafficking becoming very important in 2010 vs 1980-1995, and words like development and training gaining prominence.  In the second plot, we see more interest in topics like zagreb, soviet, saudi, and croatia.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Find out what words typify secret/classified cables vs unclassified in 2010&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Let's take a look at what words/topics are more prevalent in secret or classified cables.  Let's first look at how many cables of each type are in our cable_present data frame:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;table(cable_present$classification)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## &lt;br /&gt;##                        CONFIDENTIAL                CONFIDENTIAL//NOFORN &lt;br /&gt;##                                 719                                  67 &lt;br /&gt;##                              SECRET                      SECRET//NOFORN &lt;br /&gt;##                                 188                                  51 &lt;br /&gt;##                        UNCLASSIFIED UNCLASSIFIED//FOR OFFICIAL USE ONLY &lt;br /&gt;##                                 643                                 756 &lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;Now, we will do something similar to what we did above, where the data was split into 2 chunks and the words in each chunk were compared to generate clouds.  I have made the code generic by changing the names to set one and set two.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;cable_set_one &amp;lt;- cable_present[cable_present$classification %in% &lt;br /&gt;    c("SECRET", "SECRET//NOFORN"), ]&lt;br /&gt;cable_set_two &amp;lt;- cable_present[cable_present$classification %in% &lt;br /&gt;    c("UNCLASSIFIED", "UNCLASSIFIED//FOR OFFICIAL USE ONLY"), ]&lt;br /&gt;ppatterns &amp;lt;- c("\\n", "\\r")&lt;br /&gt;combined &amp;lt;- tolower(gsub(paste("(", paste(ppatterns, collapse = "|"), &lt;br /&gt;    ")", sep = ""), "", c(cable_set_one$content, cable_set_two$content)))&lt;br /&gt;corpus &amp;lt;- Corpus(VectorSource(combined))&lt;br /&gt;corpus &amp;lt;- tm_map(corpus, stripWhitespace)&lt;br /&gt;cable_mat &amp;lt;- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf, &lt;br /&gt;    removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15))))&lt;br /&gt;cable_mat &amp;lt;- cable_mat[rowSums(cable_mat) &amp;gt; 3, ]&lt;br /&gt;one_mat &amp;lt;- cable_mat[, 1:nrow(cable_set_one)]&lt;br /&gt;two_mat &amp;lt;- cable_mat[, (nrow(cable_set_one) + 1):ncol(cable_mat)]&lt;br /&gt;rm(cable_mat)&lt;br /&gt;gc()&lt;br /&gt;chisq_vals &amp;lt;- chisq(rowSums(one_mat), ncol(one_mat) * 100, rowSums(two_mat), &lt;br /&gt;    ncol(two_mat) * 100)&lt;br /&gt;chisq_direction &amp;lt;- rep(-1, length(chisq_vals))&lt;br /&gt;mean_frame &amp;lt;- data.frame(one_mean = rowSums(one_mat)/ncol(one_mat), &lt;br /&gt;    two_mean = rowSums(two_mat)/ncol(two_mat))&lt;br /&gt;chisq_direction[mean_frame[, 2] &amp;gt; mean_frame[, 1]] &amp;lt;- 1&lt;br /&gt;chisq_vals &amp;lt;- chisq_vals * chisq_direction&lt;br /&gt;cloud_frame &amp;lt;- data.frame(word = rownames(one_mat), chisq = chisq_vals, &lt;br /&gt;    one_sum = rowSums(one_mat), two_sum = rowSums(two_mat))&lt;br /&gt;pal &amp;lt;- brewer.pal(9, "Set1")&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code class="r"&gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt;We are now ready to plot these new word clouds.  Here are words that are typical of set 1 (secret cables) that separate it from set 2 (unclassified cables):&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-_ZDNy9V_mgc/T9eX2FLc4zI/AAAAAAAAAKI/Sa9EAyAYypg/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-_ZDNy9V_mgc/T9eX2FLc4zI/AAAAAAAAAKI/Sa9EAyAYypg/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;And here are words that are typical of set 2 (unclassified cables) that separate it from set 1 (secret cables) :&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-BXRsR4qrDRg/T9eX8oV7qDI/AAAAAAAAAKQ/z6cjtDp2WAk/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://1.bp.blogspot.com/-BXRsR4qrDRg/T9eX8oV7qDI/AAAAAAAAAKQ/z6cjtDp2WAk/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;This makes sense, as the first cloud has words like icbms and bombers, whereas the second has words like labor and victims, which would be typical of the trafficking in persons/human rights reports.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Find out what words typify secret/classified cables vs unclassified from 1960-2000&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Now, we can look at what words differentiated secret cables from unclassified cables from 1960 to 2000.&lt;br /&gt;&lt;br /&gt;Here is the cloud that shows what words appear significantly in the secret cables, but not in the unclassified cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-UJTOmCdJLfU/T9eYCEJyakI/AAAAAAAAAKY/1qI4WEMx0M0/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-UJTOmCdJLfU/T9eYCEJyakI/AAAAAAAAAKY/1qI4WEMx0M0/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;And here is the cloud that shows what words appear significantly in the unclassified cables, but not in the secret cables:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code class="r"&gt;wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3), &lt;br /&gt;    min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal, &lt;br /&gt;    vfont = c("sans serif", "plain"))&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-CRQ8Fl1-MTQ/T9eYIjgT3eI/AAAAAAAAAKg/WJjEtDeTM7E/s1600/chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-CRQ8Fl1-MTQ/T9eYIjgT3eI/AAAAAAAAAKg/WJjEtDeTM7E/s1600/chart.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;It's very interesting to see how these patterns change over time.  Particularly, seeing what the classified topics were from 1960-2000 versus unclassified is interesting.  I really wanted to see how State Department writers differ from normal english writers, but I don't have the time to do it right now.  It will have to wait for the next post.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/9007696286801530443/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html#comment-form' title='4 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html' title='Finding word use patterns in Wikileaks cables'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://4.bp.blogspot.com/-DSL_1sfbs8I/T9eXJyytdbI/AAAAAAAAAJo/OsFzWmmHGvw/s72-c/chart.png' height='72' width='72'/><thr:total>4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7889460415866570979</id><published>2012-06-09T06:54:00.000-07:00</published><updated>2012-06-09T06:54:48.271-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='gpl'/><category scheme='http://www.blogger.com/atom/ns#' term='Celtics'/><category scheme='http://www.blogger.com/atom/ns#' term='Boston'/><category scheme='http://www.blogger.com/atom/ns#' term='Heat'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='open source'/><category scheme='http://www.blogger.com/atom/ns#' term='basketball'/><category scheme='http://www.blogger.com/atom/ns#' term='playoffs'/><category scheme='http://www.blogger.com/atom/ns#' term='Miami'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='finals'/><category scheme='http://www.blogger.com/atom/ns#' term='lgpl'/><title type='text'>NBA Playoffs Update 5 (5-4)</title><content type='html'>This is the sixth post in my series on &lt;a href="http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html"&gt;predicting the NBA playoffs&lt;/a&gt; with an algorithm.  After the Boston loss in their last game, the algorithm is now 5-4 in the playoffs.  Hopefully it is correct tonight! &lt;br /&gt;&lt;br /&gt;&lt;b&gt;Open Sourcing the Code&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I have had a couple of requests to open source the code, which I had planned to do at the end of this series of posts.  However, there is one stumbling block in that the data I am scraping cannot be redistributed (I think).  If anyone has access to box score data for the 2010-2011 and 2011-2012 seasons that has a public license, please let me know.  You can contact me via the email in my profile, or in the comments section.&lt;br /&gt;&lt;br /&gt;Being able to get the data would simplify things a lot, but I would still have to clean up and comment the code a bit.  Expect to see it out in a week or so.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Predictions&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The algorithm likes Miami tonight:  &lt;a href="http://4.bp.blogspot.com/-2F0-9TZ-yFw/T9NVGmwMcQI/AAAAAAAAAJM/FQPPvOG7WjE/s1600/update_5.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="99" src="http://4.bp.blogspot.com/-2F0-9TZ-yFw/T9NVGmwMcQI/AAAAAAAAAJM/FQPPvOG7WjE/s640/update_5.png" width="640" /&gt;&lt;/a&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/7889460415866570979/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoffs-update-5-5-4.html#comment-form' title='1 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7889460415866570979'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7889460415866570979'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoffs-update-5-5-4.html' title='NBA Playoffs Update 5 (5-4)'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://4.bp.blogspot.com/-2F0-9TZ-yFw/T9NVGmwMcQI/AAAAAAAAAJM/FQPPvOG7WjE/s72-c/update_5.png' height='72' width='72'/><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-9115540751698028905</id><published>2012-06-07T12:24:00.000-07:00</published><updated>2012-06-07T12:24:14.434-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='data'/><category scheme='http://www.blogger.com/atom/ns#' term='Thunder'/><category scheme='http://www.blogger.com/atom/ns#' term='Oklahoma City'/><category scheme='http://www.blogger.com/atom/ns#' term='Celtics'/><category scheme='http://www.blogger.com/atom/ns#' term='Boston'/><category scheme='http://www.blogger.com/atom/ns#' term='Heat'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='Machine Learning'/><category scheme='http://www.blogger.com/atom/ns#' term='playoffs'/><category scheme='http://www.blogger.com/atom/ns#' term='Miami'/><category scheme='http://www.blogger.com/atom/ns#' term='predictions'/><category scheme='http://www.blogger.com/atom/ns#' term='Statistics'/><category scheme='http://www.blogger.com/atom/ns#' term='Spurs'/><category scheme='http://www.blogger.com/atom/ns#' term='San Antonio'/><title type='text'>NBA Playoff Predictions Update 4 (5-3)</title><content type='html'>This is update 4 to my original post about predicting the &lt;a href="http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html"&gt;NBA playoffs with R&lt;/a&gt;.  With the Thunder beating the Spurs and the Heat losing to the Celtics, the algorithm went 1-1 on predictions, making it &lt;b&gt;5-3&lt;/b&gt; so far.&lt;br/&gt;&lt;br/&gt; &lt;b&gt;Making some improvements&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; I have been posting for some time about incorporating more data into the models, and I finally got around to it.  It is a common truism in data science that more (high-quality) data almost always leads to a better model, and it is no exception here.  The fact that the 2011-2012 season was strike-shortened also meant that only relying on data from this season really limited the potential of the algorithm.&lt;br/&gt;&lt;br/&gt; I decided to start slowly, and incorporate data from both the 2010-2011 season, and the 2011-2012 season.  Due to the aforementioned strike, this actually increases the data available by 128% overall.  I made no other tweaks to the algorithm in this time, so this is a good test of how much value additional data on its own can add.  &lt;br/&gt;&lt;br/&gt; The new accuracy value across both seasons is &lt;b&gt;65.6%&lt;/b&gt;, which means that it is predicting 1.91 times as many winners as losers.  Here is the confusion matrix:  &lt;a href="http://2.bp.blogspot.com/-ri892mJsYTA/T9D2rAa7wMI/AAAAAAAAAIw/-QNs2S5fV_A/s1600/conf_mat_new.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="347" width="400" src="http://2.bp.blogspot.com/-ri892mJsYTA/T9D2rAa7wMI/AAAAAAAAAIw/-QNs2S5fV_A/s400/conf_mat_new.png" /&gt;&lt;/a&gt;&lt;br/&gt;&lt;br/&gt; &lt;b&gt;Differences between seasons&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; I thought it would be interesting to look at p-values between different season statistics to see if there were any significant differences between the 2010-2011 season and the 2011-2012 season.  A big deal is always made about how the lockout affected different statistics, but I haven't seen any analysis on it yet.&lt;br/&gt;&lt;br/&gt; We can easily do a t-test on each column of the data frame with all of the per team statistics:  &lt;pre&gt;&lt;br /&gt;p_vals&lt;-foreach(i=7:ncol(frame_2011)) %do%&lt;br /&gt;{&lt;br /&gt;   t.test(frame_2011[,i],frame_2012[,i])$p.value&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; We end up with a table of each calculated statistic the p-values associated with each one.  A p-value indicates if there is a statistically significant difference between two distributions.  In this case, we might be looking to see whether there is a statistically significant difference between rebounding in the 2010-2011 season, and the 2011-2012 season.  The test gave back some interesting results.  Here are some of them: &lt;br/&gt;&lt;br/&gt; 1.  There was a very significant difference in the number of players who fouled out between the two seasons.  In 2012, far fewer players fouled out than in 2011.  Also, less personal fouls were assessed overall, which also dropped the number of free throws attempted.&lt;br/&gt;&lt;br/&gt; 2.  Starters played significantly more unique positions in 2012.  For example, if the starting lineup consists of a C, a PF, an SF, an SG/SF, and a PG, there are 5 unique positions.  On the other hand, if it consists of a PF, a PF, an SF/SG, an SF/SG, and a PG, that is only 3 unique positions.  I am not sure why this increased between seasons, but maybe it indicates the rise of more true centers?  Maybe a different way of keeping track of positions?&lt;br/&gt;&lt;br/&gt; 3.  3 point percentage went down significantly from 2010-2011 to 2011-2012.  Less practice time?  Not clear why this happened.&lt;br/&gt;&lt;br/&gt;  4.  Rebounding went up overall, as did defensive rebounding.&lt;br/&gt;&lt;br/&gt;  5.  Starters played less minutes in 2011-2012 than in 2010-2011.&lt;br/&gt;&lt;br/&gt;  &lt;b&gt;Predictions for Tonight&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; And finally, the algorithm is predicting Boston to win tonight:  &lt;a href="http://3.bp.blogspot.com/-AK0sJ_ud8ig/T9D4QIyMfMI/AAAAAAAAAI8/hyDtjgxaT1Y/s1600/update_4.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="55" width="400" src="http://3.bp.blogspot.com/-AK0sJ_ud8ig/T9D4QIyMfMI/AAAAAAAAAI8/hyDtjgxaT1Y/s400/update_4.png" /&gt;&lt;/a&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/9115540751698028905/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-4-5-3.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/9115540751698028905'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/9115540751698028905'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-4-5-3.html' title='NBA Playoff Predictions Update 4 (5-3)'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://2.bp.blogspot.com/-ri892mJsYTA/T9D2rAa7wMI/AAAAAAAAAIw/-QNs2S5fV_A/s72-c/conf_mat_new.png' height='72' width='72'/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6109649427445817068</id><published>2012-06-05T12:05:00.000-07:00</published><updated>2012-06-05T15:08:16.612-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='predictions.series'/><category scheme='http://www.blogger.com/atom/ns#' term='Thunder'/><category scheme='http://www.blogger.com/atom/ns#' term='Oklahoma City'/><category scheme='http://www.blogger.com/atom/ns#' term='Celtics'/><category scheme='http://www.blogger.com/atom/ns#' term='Boston'/><category scheme='http://www.blogger.com/atom/ns#' term='Heat'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='basketball'/><category scheme='http://www.blogger.com/atom/ns#' term='playoffs'/><category scheme='http://www.blogger.com/atom/ns#' term='Miami'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='finals'/><category scheme='http://www.blogger.com/atom/ns#' term='statics'/><category scheme='http://www.blogger.com/atom/ns#' term='Spurs'/><category scheme='http://www.blogger.com/atom/ns#' term='San Antonio'/><title type='text'>NBA Playoff Predictions Update 3 (4-2)</title><content type='html'>This is my third update to my original post on &lt;a href="http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html"&gt;predicting the NBA playoffs with an algorithm.&lt;/a&gt;  Here are updates &lt;a href="http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html"&gt;1&lt;/a&gt; and &lt;a href="http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-2-and.html"&gt;2&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt; The algorithm correctly predicted a Boston win, but missed on the Spurs/Thunder game, so it is currently &lt;b&gt;4-2&lt;/b&gt;.  Haven't had any time to update yet, so I will only be able to give you predictions for the next games, unfortunately:  &lt;a href="http://1.bp.blogspot.com/-SE8MDotNcEk/T85YCXN7Z3I/AAAAAAAAAIg/Sa4MNo0HdJI/s1600/update_3.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="171" src="http://1.bp.blogspot.com/-SE8MDotNcEk/T85YCXN7Z3I/AAAAAAAAAIg/Sa4MNo0HdJI/s640/update_3.png" width="640" /&gt;&lt;/a&gt; Predicting a Miami win and an Oklahoma City win.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/6109649427445817068/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-3-4-2.html#comment-form' title='1 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/6109649427445817068'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/6109649427445817068'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-3-4-2.html' title='NBA Playoff Predictions Update 3 (4-2)'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://1.bp.blogspot.com/-SE8MDotNcEk/T85YCXN7Z3I/AAAAAAAAAIg/Sa4MNo0HdJI/s72-c/update_3.png' height='72' width='72'/><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4409129673173821400</id><published>2012-06-03T12:53:00.001-07:00</published><updated>2012-06-03T12:54:44.679-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Thunder'/><category scheme='http://www.blogger.com/atom/ns#' term='algorithm'/><category scheme='http://www.blogger.com/atom/ns#' term='Oklahoma City'/><category scheme='http://www.blogger.com/atom/ns#' term='Celtics'/><category scheme='http://www.blogger.com/atom/ns#' term='Boston'/><category scheme='http://www.blogger.com/atom/ns#' term='Heat'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='basketballs'/><category scheme='http://www.blogger.com/atom/ns#' term='Miami'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='Spurs'/><category scheme='http://www.blogger.com/atom/ns#' term='San Antonio'/><category scheme='http://www.blogger.com/atom/ns#' term='predicting'/><category scheme='http://www.blogger.com/atom/ns#' term='predictive'/><title type='text'>NBA Playoff Predictions Update 2 and Results (3-1)</title><content type='html'>This is my second follow-up to my previous two posts which were about &lt;a href="http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html"&gt;predicting NBA games with an algorithm&lt;/a&gt;, and my &lt;a href="http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html"&gt;first update to the algorithm&lt;/a&gt;.  The algorithm's record is now &lt;b&gt;3-1&lt;/b&gt;, as it correctly predicted Boston and Oklahoma City as winners of their past games.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Upcoming things to do&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Sadly, I have been a bit busy, and I have not been able to do any work on the algorithm the past couple of days.  My next steps are going to be to add in more historical data, and to implement a good backtesting framework to get a more reliable error estimate.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Predictions for the Next Games&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-aeAgllqHrkw/T8vAP8daQII/AAAAAAAAAIQ/j3Fz9yB-kc0/s1600/update_2_predictions.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="160" src="http://3.bp.blogspot.com/-aeAgllqHrkw/T8vAP8daQII/AAAAAAAAAIQ/j3Fz9yB-kc0/s640/update_2_predictions.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;So, here the algorithm is predicting that San Antonio and Boston will win their next games (Boston's game is tonight, and San Antonio's is tomorrow).  Let's see how it plays out, first game is in a few hours!</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4409129673173821400/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-2-and.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4409129673173821400'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4409129673173821400'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-2-and.html' title='NBA Playoff Predictions Update 2 and Results (3-1)'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-aeAgllqHrkw/T8vAP8daQII/AAAAAAAAAIQ/j3Fz9yB-kc0/s72-c/update_2_predictions.png' height='72' width='72'/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5415591293840639618</id><published>2012-06-01T10:12:00.001-07:00</published><updated>2012-06-01T10:12:58.531-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Machine Learning'/><category scheme='http://www.blogger.com/atom/ns#' term='basketball'/><category scheme='http://www.blogger.com/atom/ns#' term='predictive analytics'/><category scheme='http://www.blogger.com/atom/ns#' term='predictions'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='ggplot'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Predicting NBA Playoff Games - Results and Update 1</title><content type='html'>&lt;b&gt;Game Results&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I recently made a post about &lt;a href="http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html"&gt;developing an algorithm&lt;/a&gt; to predict the NBA playoffs, and I concluded with 2 predictions.  Although Miami beat the Celtics to make my algorithm &lt;b&gt;1-0&lt;/b&gt; in terms of predictions, it fell to &lt;b&gt;1-1&lt;/b&gt; when the Thunder beat the Spurs.  So, we are now at .500 .  Considering that the algorithm was about 61.5% accurate over the whole season, this is to be expected.&lt;br /&gt;&lt;br /&gt;I made some improvements to the algorithm which improved accuracy, and then used this to make new predictions for the next game in each of the current series (Spurs vs. Thunder and Celtics vs. Heat).  You can scroll down all the way to see the predictions, or read through to see what I did.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Improvements in Variables&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I created rough code initially, and I didn't fully utilize all of the information that I had.  The first step in making improvements was to add some variables relating to different player positions and bench vs. starter performance. &lt;br /&gt;&lt;br /&gt;For example, this plot shows that the average number of seconds bench players played over the last 10 games has a decent correlation with winning percentage:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-5M9fONppT1k/T8jz5U7cLbI/AAAAAAAAAHc/exN14m8aghc/s1600/bench_win.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://4.bp.blogspot.com/-5M9fONppT1k/T8jz5U7cLbI/AAAAAAAAAHc/exN14m8aghc/s640/bench_win.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;A reasonable conclusion to this is that winning teams generally have a stronger bench that they can rely on more.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Improvements in Machine Learning&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;After adding some variables, I moved on to adjusting the models that I used.  I had initially spent very little time on the machine learning framework, and most of the time on the data, and that did not change here, but I was able to tweak what I was predicting.  Initially, I was predicting a binary value- whether a team won or not.  I adjusted this to predict the ratio between a team's score and another team's score.  This gave the machine learning algorithms a lot more information than a 1/0 target, and also had the benefit of being a normal distribution, as this quantile-quantile plot shows:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-J5MAZ6tw4ck/T8j00t4OjXI/AAAAAAAAAHo/bYKuPgeQLLQ/s1600/qqplot_spread.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://3.bp.blogspot.com/-J5MAZ6tw4ck/T8j00t4OjXI/AAAAAAAAAHo/bYKuPgeQLLQ/s640/qqplot_spread.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;The straight red line is a normal distribution, and this is very close, which makes it an ideal target variable.  After the machine learning algorithms predicted this, I was able to combine the results via a simple mean method and convert them to binary win/loss values.  This made the algorithm much more accurate than before.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Updated Season Accuracy Results&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;With the improvements, the accuracy now comes to 63.6% for the season, which is a reasonable improvement over the previous results.  This results in this confusion matrix:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-YdanJpR7fKo/T8j11ZOuEJI/AAAAAAAAAH0/Thgy2mZCI4Y/s1600/confmatupdate1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://2.bp.blogspot.com/-YdanJpR7fKo/T8j11ZOuEJI/AAAAAAAAAH0/Thgy2mZCI4Y/s640/confmatupdate1.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;The main hindrance to accuracy is only being able to get team names and home/away information for future games.  If it was possible to get more information, such as officials, lineups, etc, it would be possible to make a much more accurate model.  Having more data from past seasons would also help a lot, and I might look into getting that.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Predictions for Upcoming Games&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;As before, I will leave you with predictions for the two upcoming games.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-GDCtiX_VSuA/T8j3WQ_e4yI/AAAAAAAAAIA/hLRYtlAqJVo/s1600/predictions_update1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="201" src="http://2.bp.blogspot.com/-GDCtiX_VSuA/T8j3WQ_e4yI/AAAAAAAAAIA/hLRYtlAqJVo/s640/predictions_update1.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;Here, the algorithm is predicting a Boston win/Miami loss, and an Oklahoma City win/San Antonio loss.  Let's see how it plays out!&lt;br/&gt;&lt;br/&gt; I can do a decent amount of analysis on the data, so please let me know if you want to see something specific next time.  I'm going to make posts predicting all of the games in the series and the finals.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/5415591293840639618/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5415591293840639618'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5415591293840639618'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html' title='Predicting NBA Playoff Games - Results and Update 1'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://4.bp.blogspot.com/-5M9fONppT1k/T8jz5U7cLbI/AAAAAAAAAHc/exN14m8aghc/s72-c/bench_win.png' height='72' width='72'/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-202561860220765707</id><published>2012-05-30T12:07:00.002-07:00</published><updated>2012-06-05T17:36:11.753-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='predict'/><category scheme='http://www.blogger.com/atom/ns#' term='basketball'/><category scheme='http://www.blogger.com/atom/ns#' term='predictions'/><category scheme='http://www.blogger.com/atom/ns#' term='NBA'/><category scheme='http://www.blogger.com/atom/ns#' term='data analysis'/><category scheme='http://www.blogger.com/atom/ns#' term='finals'/><category scheme='http://www.blogger.com/atom/ns#' term='ggplot'/><category scheme='http://www.blogger.com/atom/ns#' term='Statistics'/><category scheme='http://www.blogger.com/atom/ns#' term='regression'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Predicting the NBA Finals with R</title><content type='html'>This is the initial post about the algorithm.  See updates &lt;a href="http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html"&gt;1&lt;/a&gt;, &lt;a href="http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-2-and.html"&gt;2&lt;/a&gt;, and &lt;a href="http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-3-4-2.html"&gt;3&lt;/a&gt; for more.  The algorithm is currently 4-2 in the playoffs!&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I was struck by Martin O'Leary's recent post on predicting the &lt;a href="http://mewo2.github.com/nerdery/2012/05/20/ive-got-eurosong-fever-ted/"&gt;Eurovision finals&lt;/a&gt;, which led me to decide that I would try to predict NBA games using mathematical models.  As the finals are ongoing, this is a quite timely decision!  You can read through everything or scroll to the end for finals predictions and accuracy results.&lt;br /&gt;&lt;br /&gt;This model is necessarily very different (read: nothing in common at all) from Martin's, because the underlying concept that we are predicting, and how it is derived, are very different.  The outcome of NBA games are determined by a variety of factors, such as the scoring ability of each team, whether the teams are playing at home or away, and whether or not any players are injured.  It stands to reason that several of these factors can be modeled, and used to predict whether a team will win or lose a given game.  But, we are getting ahead of ourselves.  The first step, as always, is to acquire the data.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Getting the Data&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Game data is actually quite difficult to get in machine readable form.  This is likely because while there is a major demand for data that can be casually read and looked over, there isn't a major public demand for sports data feeds.  As I lacked enough of a budget to become an ESPN "partner", I had to resort to more utilitarian methods, which gave me data in essentially a "box score" format.  This format gave the statistics for each team for each game.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;  Reformatting the Box Scores&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The majority of the problem at this point is actually reformatting the data to make it into a useful form for analysis.  A box score is good, but if we want to predict who will win a game in the future, it isn't very useful.  The problem is in getting from something like the table below to something that an algorithm can read in:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-cgP9vK8llOs/T8Zoe0tT9_I/AAAAAAAAAF0/Tey9iBbBcE0/s1600/box_Table.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="342" src="http://3.bp.blogspot.com/-cgP9vK8llOs/T8Zoe0tT9_I/AAAAAAAAAF0/Tey9iBbBcE0/s640/box_Table.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;As always, this is where the not so glamorous side of data analysis comes in; it takes a lot of time to convert data into a useful form. &lt;br /&gt;&lt;br /&gt;&lt;b&gt;  Computing Summary Statistics&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I first computed summary statistics for each team for each game, such as rebounds per game, blocks per game, etc:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-2PsyfC86Oa0/T8Zoj3NiigI/AAAAAAAAAGA/QaNy5lSSCyQ/s1600/game_boxes.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="294" src="http://2.bp.blogspot.com/-2PsyfC86Oa0/T8Zoj3NiigI/AAAAAAAAAGA/QaNy5lSSCyQ/s640/game_boxes.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;This is truncated, and a lot of variables are left out, but it gives you an idea of how it came about.  Basically, I summarized the box scores and converted them into "observations", where each teams performance becomes one row.  I computed indicators such as rebounds per player, turnovers per player, etc.&lt;br /&gt;&lt;br /&gt;The next problem with this is that there is very little predictive ability in just the previous game's statistics.  In order to predict future games, we need to know a teams performance for the whole season.  At this point, I split the data up by team, and treated the season for each team as a time series.  This allowed me to compute summary statistics for the whole season, and for the last 10 games for each team.  We could see the rebounds per game, for example, for the whole season.  &lt;br /&gt;&lt;br /&gt;Here is how the data looked at this point:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-ZHgtTaPbrrA/T8Zon1zTwEI/AAAAAAAAAGM/Nq-2lTsiiV8/s1600/summary_box.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="214" src="http://3.bp.blogspot.com/-ZHgtTaPbrrA/T8Zon1zTwEI/AAAAAAAAAGM/Nq-2lTsiiV8/s640/summary_box.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;Note that this is truncated in terms of both rows and columns.  Essentially, for each game, the running statistics for the team up to that point are available.&lt;br /&gt;&lt;br /&gt;For example, here are Denver's season running averages for assists, points, and rebounds:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-mE6Yser1hQc/T8Zn7qHV6wI/AAAAAAAAAFo/ufOF3So4KJ8/s1600/denver_box.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://4.bp.blogspot.com/-mE6Yser1hQc/T8Zn7qHV6wI/AAAAAAAAAFo/ufOF3So4KJ8/s640/denver_box.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;And here are Denver's 10 day back averages:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-itO6rXx0LPc/T8Zo_ipMzfI/AAAAAAAAAGY/Lp9467Z0YTw/s1600/denver_10_day.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://3.bp.blogspot.com/-itO6rXx0LPc/T8Zo_ipMzfI/AAAAAAAAAGY/Lp9467Z0YTw/s640/denver_10_day.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;As you can see, there are some interesting patterns in the 10 day back statistics that do not appear in the season running averages.  This is why it is useful to have both, and to take the ratio between the two.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Putting all the data together&lt;/b&gt;&lt;br /&gt;&lt;br/&gt;The next step was to put together the statistics for each team for each game with the similar statistics for their opponent.  Once this was accomplished, the result could be fed into predictive algorithms.  This left me with a table where each row contained data for a team, and for its opponent, in terms of their season performance.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Finally, Some Machine Learning&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I was finally able to put all of the data together, and predict game winners based on this.  I used all of the games for the season (minus 10 games for each team at the beginning needed to initialize the 10 period means), and used cross validation to predict results.  This causes some issues because it has future data to predict "past" data, but should still be relatively accurate.&lt;br /&gt;&lt;br /&gt;For the actual machine learning framework, I used a simple combination of three different classification algorithms (they were combined via their median).  The predictions were made as 1 (the team won), or 0 (the team lost).&lt;br /&gt;&lt;br /&gt;This resulted in the following table:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-9H8guwmXRK4/T8ZqwpuoyjI/AAAAAAAAAGk/L-RQXL0n-0M/s1600/conf_mat.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://3.bp.blogspot.com/-9H8guwmXRK4/T8ZqwpuoyjI/AAAAAAAAAGk/L-RQXL0n-0M/s640/conf_mat.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;The x-axis is predicted result, and the y axis is the actual result.  As you can see, the predicted result matches the actual result in the majority of the cases.&lt;br /&gt;&lt;br /&gt;To be specific, the algorithm correctly predicted 1323 winners/losers, and incorrectly predicted 823 winners/losers over the whole season.  This gives it a 61.5% prediction accuracy for the season, which is pretty good for a days work!&lt;br /&gt;&lt;br /&gt;Now, if we feed in data for the first 2/3rds of the season, and we predict the final 1/3 of the season, we get the following matrix:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-fE4rfmmGEm4/T8Zrs0pgLVI/AAAAAAAAAGw/d2bRGRnyTyk/s1600/conf_mat_future.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="555" src="http://4.bp.blogspot.com/-fE4rfmmGEm4/T8Zrs0pgLVI/AAAAAAAAAGw/d2bRGRnyTyk/s640/conf_mat_future.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;This is pretty similar to the cross validated error, and the prediction accuracy comes to about 60.5%.&lt;br /&gt;&lt;br /&gt;The accuracy is likely lower than it should be simply because the predictions are being made for 1 month or more at a time.  If predictions were restricted to one day/game ahead, accuracy would be much improved.&lt;br/&gt;&lt;br/&gt; &lt;b&gt;Potential Improvements&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; Of course, this is a very rough system, and both the data and the methods can use a lot of refinement.  61% is a good lower bound for accuracy, but with some work, it could go up significantly.  The main improvements that could be made are in the data acquisition, in the variable calculation, and in the final models that are used to calculate the win/loss.&lt;br /&gt;&lt;br /&gt;The system can also be modified to predict points or point spreads, which might make things a bit better as well.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;And of Course, Predictions for the Finals!&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; And, last but not least, I will leave you with some NBA finals predictions.  I am currently predicting one day/game ahead for the finals, but I might work on altering this to predict further in advance if needed.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-zUTg29tujsY/T8Zu_RvrnmI/AAAAAAAAAHA/6fkLdd4iPfw/s1600/predictions.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="212" src="http://1.bp.blogspot.com/-zUTg29tujsY/T8Zu_RvrnmI/AAAAAAAAAHA/6fkLdd4iPfw/s640/predictions.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;So, here it is predicting that both Miami and San Antonio will win their next game.  This seems strange in practice, but we will see how it plays out!</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/202561860220765707/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/202561860220765707'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/202561860220765707'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/predicting-nba-finals-with-r.html' title='Predicting the NBA Finals with R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-cgP9vK8llOs/T8Zoe0tT9_I/AAAAAAAAAF0/Tey9iBbBcE0/s72-c/box_Table.png' height='72' width='72'/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1621926639359048504</id><published>2012-05-08T08:20:00.000-07:00</published><updated>2012-05-08T08:20:53.773-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='zips'/><category scheme='http://www.blogger.com/atom/ns#' term='zip codes'/><category scheme='http://www.blogger.com/atom/ns#' term='coordinates'/><category scheme='http://www.blogger.com/atom/ns#' term='mapping'/><category scheme='http://www.blogger.com/atom/ns#' term='radiation'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><category scheme='http://www.blogger.com/atom/ns#' term='plotting'/><title type='text'>Mapping US Radiation Levels in R</title><content type='html'>I have posted previously about the open data available on Socrata (https://opendata.socrata.com/), and I was looking at the site again today when I stumbled upon a listing of levels of various radioactive isotopes by US city and state.  The data is available at https://opendata.socrata.com/Government/Sorted-RadNet-Laboratory-Analysis/w9fb-tgv6 .  You will need to click export, and then download it as a csv.  &lt;br /&gt; &lt;br/&gt; I was struck by how the data was in a very nice format for analysis.  I initially wanted to look at summary statistics, but I have wanted to try some mapping for a while now, and this seemed like the right time.  &lt;br /&gt; &lt;br/&gt; &lt;b&gt; Reading in the Data &lt;/b&gt;&lt;br /&gt;&lt;br/&gt; To begin exploring and mapping the data, we first need to download it.  After downloading, it can be read in using the read.csv command:  &lt;pre&gt;&lt;br /&gt;rad_levels&lt;-read.csv("Sorted_RadNet_Laboratory_Analysis.csv",&lt;br /&gt;stringsAsFactors=FALSE,strip.white=TRUE,header=TRUE,quote="")&lt;br /&gt;&lt;/pre&gt; This gives us a nice data frame where each row is an observation of a different type (Drinking Water, Precipitation, etc), and the columns contain location data and data on levels of various isotopes. &lt;br /&gt; &lt;br /&gt; &lt;b&gt; Associating Coordinates with the Data &lt;/b&gt;&lt;br /&gt;&lt;br/&gt; Unfortunately, we are not provided with any coordinates, and in order to map this data, we will need to figure out the coordinates associated with each observation.  Thankfully, I recently wrote a quick function to map place names to coordinates.   &lt;pre&gt;&lt;br /&gt;does_zip_exist&lt;-function()&lt;br /&gt;{&lt;br /&gt;zip_exists&lt;-FALSE&lt;br /&gt;if(exists("zips"))&lt;br /&gt;{&lt;br /&gt; zip_exists&lt;-TRUE&lt;br /&gt; if(!nrow(zips)==43623)&lt;br /&gt;  zip_exists&lt;-FALSE&lt;br /&gt;}&lt;br /&gt;zip_exists&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;get.coordinates.name&lt;-function(city,state)&lt;br /&gt;{&lt;br /&gt;city&lt;-tolower(city)&lt;br /&gt;state&lt;-tolower(state)&lt;br /&gt;if(!does_zip_exist())&lt;br /&gt; load("zips.RData",.GlobalEnv)&lt;br /&gt;if(nchar(state)==2)&lt;br /&gt;{&lt;br /&gt;zip_row&lt;-head(zips[zips$state_abbr==state &amp; zips$city %in% city,],1)&lt;br /&gt;}else&lt;br /&gt;{&lt;br /&gt;zip_row&lt;-head(zips[zips$state %in% state &amp; zips$city %in% city,],1)&lt;br /&gt;}&lt;br /&gt;if(nrow(zip_row)==0)&lt;br /&gt; zip_row&lt;-c("NA",00000,rep("NA",4),rep(0,3))&lt;br /&gt;zip_row&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; These two functions together will enable you to get the coordinates of a given US city and state.  They both depend on a file called zips.RData, which can be downloaded at http://dl.dropbox.com/u/20597506/zips.RData .  The file must be present in your R working directory for the functions to work.  I wrote these functions really quickly, and don't feel like rewriting them now, so please feel free to improve their performance if you want.  I will make a longer post about them later if needed.  &lt;br /&gt; &lt;br/&gt; Now, we have our radiation data, and the ability to associate the place names in the data to coordinates.  What we need to do now is to perform that association.  &lt;pre&gt;&lt;br /&gt;rad_level_coords&lt;-apply(rad_levels,1,function(x) get.coordinates.name(x[2],x[1]))&lt;br /&gt;rad_level_coords&lt;-do.call(rbind,rad_level_coords)&lt;br /&gt;rad_level_frame&lt;-cbind(rad_levels,rad_level_coords)&lt;br /&gt;&lt;/pre&gt; This code will take each row of the radiation levels data and find the coordinates associated with the place name.  It will then combine the coordinate data into a data frame, and merge it with the radiation data.  &lt;br /&gt; &lt;br /&gt; &lt;b&gt;Cleaning up the Data&lt;/b&gt;&lt;br /&gt;&lt;br/&gt; One problem at this stage is that some of the place names in the radiation data will not properly map to coordinates.  The get.coordinates.name function outputs a zip code of 00000 in this case.  Thus, we can remove rows that do not have proper coordinates by filtering out the rows with a zip code of 00000.  &lt;pre&gt;&lt;br /&gt;rad_level_frame&lt;-rad_level_frame[rad_level_frame$zip!=00000,]&lt;br /&gt;&lt;/pre&gt; We can also check the classes of the columns in rad_level_frame to ensure that the coordinates are numeric.  &lt;pre&gt;&lt;br /&gt;sapply(rad_level_frame,class)&lt;br /&gt;&lt;/pre&gt; Unfortunately, they are not numeric columns, and they will need to be in order to use them to generate a map, so we will need to convert them:  &lt;pre&gt;&lt;br /&gt;rad_level_frame$lat&lt;-as.numeric(rad_level_frame$lat)&lt;br /&gt;rad_level_frame$long&lt;-as.numeric(rad_level_frame$long)&lt;br /&gt;&lt;/pre&gt; &lt;b&gt;Setting up the Map&lt;/b&gt;&lt;br /&gt;&lt;br/&gt; We now need to do some preliminary setup before we get started on making the map.  We need to define which type(s) of measurements that we want to plot, and what isotope levels that we want to plot.  For this first plot, we will look at I-131 in drinking water.  &lt;pre&gt;&lt;br /&gt;types&lt;-c("Air Cartridge","Air Filter","Drinking Water","Precipitation","Milk")&lt;br /&gt;current_type&lt;-types[3]&lt;br /&gt;target&lt;-rad_level_frame$I.131&lt;br /&gt;&lt;/pre&gt; The types variable simply lists the types of measurements that are in the radiation data for convenience, and the current_type and target variables will allow us to simplify the code a bit. &lt;br/&gt; &lt;br/&gt; Since we want to plot the different levels of radiation in Drinking Water, it will help if we can bin the variable.  Binning allows us to split up an interval into discrete units.  In this case, binning will help us by changing the target variable into a set of colors.  These colors will range from "green" (least radioactive) to red (most radioactive), and will allow us to plot the data.  &lt;pre&gt;&lt;br /&gt;binned_target&lt;-cut(as.numeric(target[target!="Non-detect" &amp; &lt;br /&gt;rad_level_frame$Sample.Type %in% current_type]), &lt;br /&gt;breaks=5, labels=c("green","blue","yellow","orange","red"))&lt;br /&gt;&lt;/pre&gt; Cut is a function that will create a factor from a continuous numeric variable.  Note that we have filtered out the instances of the target where nothing could be detected by removing the target variable when its value is "Non-detect".  This could also be handled by changing the "Non-detect" to a zero, but as it is unclear whether "Non-detect" means a zero value, or whether it indicates equipment failure or something else, it is best to remove it entirely.  We have also removed observations where the measurement type is not "Drinking Water".  This will ensure that we only plot observations of drinking water radiation levels.  Breaks specifies that we want to separate the data into 5 categories, which have been assigned labels according to the color that they will be plotted in. &lt;br /&gt; &lt;br /&gt; &lt;b&gt; Mapping the Data &lt;/b&gt;&lt;br /&gt;&lt;br/&gt; Now, we are ready to create our base plot of the United States:  &lt;pre&gt;&lt;br /&gt;plot(as.numeric(zips$long[zips$long&lt; -60]),&lt;br /&gt;as.numeric(zips$lat[zips$long&lt; -60]),&lt;br /&gt;type="p",col="gray40",pch=20,cex=0.2,xlab="",ylab="")&lt;br /&gt;&lt;/pre&gt; This will plot all the longitudes and latitudes from the zips.RData file in a gray color, which gives us a good US map.  Specifying that the longitude be under -60 removes some of the island possessions of the US, which unnecessarily stretch out the map.  The gray provides a good neutral color over which to plot our radiation levels.  The pch option gives us a closed circle plotting symbol, and the cex option makes the individual points fairly small. &lt;br /&gt; &lt;br /&gt; Now, we are ready to plot our radiation levels over the base map:  &lt;pre&gt;&lt;br /&gt;points(as.numeric(rad_level_frame$long[target!="Non-detect"&amp;&lt;br /&gt; rad_level_frame$Sample.Type %in% current_type]),&lt;br /&gt;as.numeric(rad_level_frame$lat[target!="Non-detect"&amp; &lt;br /&gt;rad_level_frame$Sample.Type %in% current_type]),&lt;br /&gt;type="p",col=as.character(binned_target),pch=20,cex=1)&lt;br /&gt;&lt;/pre&gt; The points function adds points to a plot generated by the plot function.  We have only plotted the latitudes and longitudes for the radiation level observations which are not "Non-detect", and which match our type, which is "Drinking Water".  We also set the color (col) using the binned_target variable that we created earlier.  The cex value is set higher so that the points appear large relative to the map. &lt;br /&gt; &lt;br /&gt; We should end up with this:&lt;br /&gt; &lt;br /&gt;&lt;b&gt; Drinking Water Radiation Map &lt;/b&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-1BjhhCr94rs/T6k2hFKT9pI/AAAAAAAAAEM/5mCr2E8Zy4M/s1600/drinking_water.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="347" width="400" src="http://3.bp.blogspot.com/-1BjhhCr94rs/T6k2hFKT9pI/AAAAAAAAAEM/5mCr2E8Zy4M/s400/drinking_water.png" /&gt;&lt;/a&gt;&lt;/div&gt; We can change our current_type variable and run the plotting again to generate the rest of the plots:  &lt;b&gt; Air Filter Radiation Map &lt;/b&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-Bet80m7x2lE/T6k3RYXIDsI/AAAAAAAAAEY/aB9q2-AQN3M/s1600/air_filter.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="347" width="400" src="http://1.bp.blogspot.com/-Bet80m7x2lE/T6k3RYXIDsI/AAAAAAAAAEY/aB9q2-AQN3M/s400/air_filter.png" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;b&gt; Precipitation Radiation Map &lt;/b&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-gXk4U0WigwU/T6k3fmhroxI/AAAAAAAAAEk/NXepNvmcJ3Y/s1600/precipitation.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="347" width="400" src="http://1.bp.blogspot.com/-gXk4U0WigwU/T6k3fmhroxI/AAAAAAAAAEk/NXepNvmcJ3Y/s400/precipitation.png" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;b&gt; Milk Radiation Map &lt;/b&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-GIzUl0rQuWI/T6k3rBwnTOI/AAAAAAAAAEw/6XBwV1I0yDE/s1600/milk.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="347" width="400" src="http://3.bp.blogspot.com/-GIzUl0rQuWI/T6k3rBwnTOI/AAAAAAAAAEw/6XBwV1I0yDE/s400/milk.png" /&gt;&lt;/a&gt;&lt;/div&gt; You can do more mapping with the other targets if you wish at this point.  Here is the full code:  &lt;pre&gt;&lt;br /&gt;setwd("~")&lt;br /&gt;rad_levels&lt;-read.csv("Sorted_RadNet_Laboratory_Analysis.csv",&lt;br /&gt;stringsAsFactors=FALSE,strip.white=TRUE,header=TRUE,quote="")&lt;br /&gt;rad_level_coords&lt;-apply(rad_levels,1,function(x) get.coordinates.name(x[2],x[1]))&lt;br /&gt;rad_level_coords&lt;-do.call(rbind,rad_level_coords)&lt;br /&gt;rad_level_frame&lt;-cbind(rad_levels,rad_level_coords)&lt;br /&gt;&lt;br /&gt;rad_level_frame&lt;-rad_level_frame[rad_level_frame$zip!=00000,]&lt;br /&gt;rad_level_frame$lat&lt;-as.numeric(rad_level_frame$lat)&lt;br /&gt;rad_level_frame$long&lt;-as.numeric(rad_level_frame$long)&lt;br /&gt;types&lt;-c("Air Cartridge","Air Filter","Drinking Water","Precipitation","Milk")&lt;br /&gt;current_type&lt;-c(types[5])&lt;br /&gt;target&lt;-rad_level_frame$I.131&lt;br /&gt;&lt;br /&gt;binned_target&lt;-cut(as.numeric(target[target!="Non-detect" &amp; &lt;br /&gt;rad_level_frame$Sample.Type %in% current_type]), breaks=5,&lt;br /&gt; labels=c("green","blue","yellow","orange","red"))&lt;br /&gt;&lt;br /&gt;plot(as.numeric(zips$long[zips$long&lt; -60]),&lt;br /&gt;as.numeric(zips$lat[zips$long&lt; -60]),type="p",col="gray40",&lt;br /&gt;pch=20,cex=0.2,xlab="",ylab="")&lt;br /&gt;&lt;br /&gt;points(as.numeric(rad_level_frame$long[target!="Non-detect"&amp; &lt;br /&gt;rad_level_frame$Sample.Type %in% current_type]),&lt;br /&gt;as.numeric(rad_level_frame$lat[target!="Non-detect"&amp; &lt;br /&gt;rad_level_frame$Sample.Type %in% current_type]),type="p",&lt;br /&gt;col=as.character(binned_target),pch=20,cex=2.5)&lt;br /&gt;&lt;/pre&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/1621926639359048504/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/mapping-us-radiation-levels-in-r.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/1621926639359048504'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/1621926639359048504'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/mapping-us-radiation-levels-in-r.html' title='Mapping US Radiation Levels in R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-1BjhhCr94rs/T6k2hFKT9pI/AAAAAAAAAEM/5mCr2E8Zy4M/s72-c/drinking_water.png' height='72' width='72'/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3169893668697261422</id><published>2012-05-08T05:03:00.000-07:00</published><updated>2012-05-08T05:03:16.963-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='packages'/><category scheme='http://www.blogger.com/atom/ns#' term='deployment'/><category scheme='http://www.blogger.com/atom/ns#' term='production'/><category scheme='http://www.blogger.com/atom/ns#' term='install'/><category scheme='http://www.blogger.com/atom/ns#' term='load'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Loading and/or Installing Packages Programmatically</title><content type='html'>In R, the traditional way to load packages can sometimes lead to situations where several lines of code need to be written just to load packages.  These lines can cause errors if the packages are not installed, and can also be hard to maintain, particularly during deployment.&lt;br /&gt;&lt;br /&gt; Fortunately, there is a way to create a function in R that will automatically load our packages for us.  In this post, I will walk you through conceiving and creating such a function.   &lt;br /&gt; &lt;br /&gt; In order to write a function that checks if a package is installed, loads it if it is, and installs it if it isn't, we first need a way to check if a package is installed.  Thankfully, this function does the job:  &lt;pre&gt;&lt;br /&gt;is_installed &lt;- function(mypkg) is.element(mypkg, installed.packages()[,1])&lt;br /&gt;&lt;/pre&gt; The above function comes from a post on the R mailing list, although I do not know if it is the original source. This function will test if a given function name is in the list of installed packages.  We can access the list directly by using installed.packages()[,1] , and we can use the function by trying is_installed("foreach") . &lt;br /&gt; &lt;br /&gt;     Now that we know how to test if a package is installed or not, we can move on to writing the function.  At this moment, we have two hurdles.  The first is how to load a package from a character vector of names.  The second is how to install a package programmatically. Typically, loading a package will look like this:  &lt;pre&gt;&lt;br /&gt;library(MASS)&lt;br /&gt;&lt;/pre&gt; Fortunately for us, there is a character.only option in the library function that allows us to specify the package name as a string.  &lt;pre&gt;&lt;br /&gt;library("MASS",character.only=TRUE)&lt;br /&gt;&lt;/pre&gt; The above gives us the functionality that we need to pass the name of a package to a function as a string and have it loaded.  Now, we need to find how to install packages, which can be done with the install.packages function:  &lt;pre&gt;&lt;br /&gt;install.packages("MASS",repos="http://lib.stat.cmu.edu/R/CRAN")&lt;br /&gt;&lt;/pre&gt; Explicitly setting the repo will avoid having R ask us for it when the function is executed for the first time.  I chose statlib for convenience, but feel free to use any repo you like. &lt;br /&gt; &lt;br /&gt; Now, we have a way to test if a package is installed, a way to install the package, and a way to load the package.  All we need to do is wrap it up with an if statement.  &lt;pre&gt;&lt;br /&gt;if(!is_installed(package_name))&lt;br /&gt;{&lt;br /&gt;   install.packages(package_name,repos="http://lib.stat.cmu.edu/R/CRAN")&lt;br /&gt;}&lt;br /&gt;library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE)&lt;br /&gt;&lt;/pre&gt; The above if statement will test to see if a package is installed, and then install it if it isn't.  It will then load the package. &lt;br /&gt; &lt;br/&gt; This gets us most of the way to what we want, but if we want to pass a character vector to the function and have it load multiple packages at once, we need to wrap everything in a for loop.  &lt;pre&gt;&lt;br /&gt;for(package_name in package_names)&lt;br /&gt;{&lt;br /&gt;  if(!is_installed(package_name))&lt;br /&gt;  {&lt;br /&gt;     install.packages(package_name,repos="http://lib.stat.cmu.edu/R/CRAN")&lt;br /&gt;  }&lt;br /&gt;  library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE)&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; This for loop will perform the operations that we need on the character vector package_names.  Now, we can just wrap everything into a neat function that is passed a character vector called package_names.  &lt;pre&gt;&lt;br /&gt;load_or_install&lt;-function(package_names)&lt;br /&gt;{&lt;br /&gt;  for(package_name in package_names)&lt;br /&gt;  {&lt;br /&gt;    if(!is_installed(package_name))&lt;br /&gt;    {&lt;br /&gt;       install.packages(package_name,repos="http://lib.stat.cmu.edu/R/CRAN")&lt;br /&gt;    }&lt;br /&gt;    library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE)&lt;br /&gt;  }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;  We can call the function with the following syntax (substitute the function names with your own):  &lt;pre&gt;&lt;br /&gt;load_or_install(c("foreach","MASS","doParallel"))&lt;br /&gt;&lt;/pre&gt; And with that, we are done, and now have a function that can load or install packages as needed from a character vector.  This function will terminate with an error if the install.packages function or the library function cannot find the specified package name, but you can fix that by using a try statement, which is an exercise that I will leave to you for the moment.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/3169893668697261422/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3169893668697261422'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3169893668697261422'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html' title='Loading and/or Installing Packages Programmatically'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7097400540850195138</id><published>2012-02-09T09:12:00.000-08:00</published><updated>2012-02-09T13:48:26.681-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='foreach'/><category scheme='http://www.blogger.com/atom/ns#' term='randomForest'/><category scheme='http://www.blogger.com/atom/ns#' term='doSnow'/><category scheme='http://www.blogger.com/atom/ns#' term='parallel'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Monitoring Progress Inside a Foreach Loop</title><content type='html'>The foreach package for R is excellent, and allows for code to easily be run in parallel.  One problem with foreach is that it creates new RScript instances for each iteration of the loop, which prevents status messages from being logged to the console output.  This is particularly frustrating during long-running tasks, when we are often unsure how much longer we need to wait, or even if the code is doing what it is intended to.  The solution to this can be found in the sink() function.  This function redirects output to a file.  I will show you a simple example of this using the iris data set.  The code below will execute without printing any status messages, even though do.trace is enabled, which periodically displays the status of the randomForest. The random forest code is slightly adapted from one of the foreach package examples.  &lt;pre&gt;&lt;br /&gt;library(foreach)&lt;br /&gt;library(doSNOW)&lt;br /&gt;library(randomForest)&lt;br /&gt;data(iris)&lt;br /&gt;&lt;br /&gt;cores&lt;-2&lt;br /&gt;&lt;br /&gt;num_trees&lt;-ceiling(2000/cores)&lt;br /&gt;c1&lt;-makeCluster(cores)&lt;br /&gt;registerDoSNOW(c1)&lt;br /&gt;&lt;br /&gt;rf_fit&lt;-foreach(ntree=rep(num_trees,cores),.combine=combine,&lt;br /&gt;.packages=c("randomForest")) %dopar% {&lt;br /&gt;randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) &lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;stopCluster(c1)  &lt;br /&gt;&lt;br /&gt;&lt;/pre&gt; We can easily correct this with the sink function:  &lt;pre&gt;&lt;br /&gt;library(foreach)&lt;br /&gt;library(doSNOW)&lt;br /&gt;library(randomForest)&lt;br /&gt;data(iris)&lt;br /&gt;&lt;br /&gt;cores&lt;-2&lt;br /&gt;&lt;br /&gt;num_trees&lt;-ceiling(2000/cores)&lt;br /&gt;c1&lt;-makeCluster(cores)&lt;br /&gt;registerDoSNOW(c1)&lt;br /&gt;&lt;br /&gt;writeLines(c(""), "log.txt")&lt;br /&gt;&lt;br /&gt;rf_fit&lt;-foreach(ntree=rep(num_trees,cores),.combine=combine,&lt;br /&gt;.packages=c("randomForest")) %dopar% {&lt;br /&gt;sink("log.txt", append=TRUE)&lt;br /&gt;randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) &lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;stopCluster(c1)&lt;br /&gt;&lt;/pre&gt; The writeLines function will clear out the file "log.txt" before the loop is run, ensuring that only output that is relevant to the current run is displayed when the file is opened.  The log file can be opened at any time during the run, and the progress can be checked.  We can even print the number of the iteration as we go through:  &lt;pre&gt;&lt;br /&gt;rf_fit&lt;-foreach(iteration=1:cores,ntree=rep(num_trees,cores),&lt;br /&gt;.combine=combine,.packages=c("randomForest")) %dopar% {&lt;br /&gt;sink("log.txt", append=TRUE)&lt;br /&gt;cat(paste("Starting iteration",iteration,"\n"))&lt;br /&gt;randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) &lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; This is useful when you have significantly more iterations than processor cores, and you want to know how far along you are.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/7097400540850195138/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/02/monitoring-progress-inside-foreach-loop.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7097400540850195138'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7097400540850195138'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/02/monitoring-progress-inside-foreach-loop.html' title='Monitoring Progress Inside a Foreach Loop'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5897587368307715451</id><published>2012-01-30T18:24:00.000-08:00</published><updated>2012-01-31T02:16:53.941-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Windows'/><category scheme='http://www.blogger.com/atom/ns#' term='LaTeX'/><category scheme='http://www.blogger.com/atom/ns#' term='Sweave'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Using LaTeX, R, and Sweave to Create Reports in Windows</title><content type='html'>&lt;a href="http://www.latex-project.org/"&gt;LaTeX&lt;/a&gt; is a typesetting system that can easily be used to create reports and scientific articles, and has excellent formatting options for displaying code and mathematical formulas.  &lt;a href="http://www.statistik.lmu.de/%7Eleisch/Sweave/"&gt;Sweave&lt;/a&gt; is a package in base R that can execute R code embedded in LaTeX files and display the output.  This can be used to generate reports and quickly fix errors when needed.&lt;br /&gt;&lt;br /&gt;There are some barriers to entry with LaTeX that seem much steeper than they actually are.  In this article, I will show you how to setup LaTeX and an IDE for it in Windows, and how to start developing for it.  Note that you will at least R version 2.14 to follow this entire article, because the command line sweave pdf export option was added then.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Install MiKTeX and TeXnicCenter&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;LaTeX documents are edited in a text editor, then compiled by a compiler, and finally are displayed in a PDF or postscript viewer.  We will begin by installing a LaTeX compiler for Windows, &lt;a href="http://miktex.org/"&gt;MiKTeX&lt;/a&gt;.  Once you grab the installer, you can go ahead and install it.  Please see the bottom of the post for the full edit, but installing LaTeX in paths with spaces in them can cause issues.  As such, I would recommend installing to a directory without them.&lt;br /&gt;&lt;br /&gt;Our next step will be to install &lt;a href="http://www.texniccenter.org/"&gt;TeXnicCenter&lt;/a&gt;, an IDE for LaTeX.  After installing TeXnicCenter and starting it for the first time, you will be asked to find your LaTeX executable file in the configuration wizard. This can generally be found in the folder C:\Program Files\MiKTeX 2.9\miktex\bin by default, or a custom path if you did not use spaces in your path.  You can leave the fields for PostScript viewer blank, unless you need the functionality.  You should now be finished with the installation.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Setup TeXnicCenter to Work with Sweave&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Now, we can setup TeXnicCenter to use Sweave directly.  This will require at least R 2.14.  To do this, click on the Build menu and go to Define Output Profiles.  Hit the "Add" button in the bottom left to create a new output profile.  You can name the profile anything you like.  I named mine "Sweave".   &lt;br /&gt;&lt;br /&gt;Now we need to configure the new output profile.  The first tab should look similar to mine if you use 64-bit R, but your R path will be different if you use 32-bit or used a non-default installation directory.  The directory will need to match yours.  The command line arguments box should read: CMD Sweave --pdf %nm .  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-jCXziuoRBIM/TydIiHrghJI/AAAAAAAAADQ/dusNBwWVyiY/s1600/sweave2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-jCXziuoRBIM/TydIiHrghJI/AAAAAAAAADQ/dusNBwWVyiY/s1600/sweave2.png" /&gt;&lt;/a&gt;&lt;/div&gt;Now, we only need to worry about the viewer tab.  I use Foxit Reader, so my settings are below.  I suggest using the same settings as you find in your LaTeX=&amp;gt;PDF output profile.  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-CczH5S2fDhE/TydMaScDCSI/AAAAAAAAADc/KiA8QIxil2k/s1600/sweave3.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-CczH5S2fDhE/TydMaScDCSI/AAAAAAAAADc/KiA8QIxil2k/s1600/sweave3.png" /&gt;&lt;/a&gt;&lt;/div&gt;Once we have all that set, you can go ahead and write your first document.  You will need to name the document something.Rnw. To build the file, you will have to select "Sweave"(or whatever you named your profile" in the drop down box below the build item on the menu bar, and then select Build-&amp;gt;Current File-&amp;gt;Build and View to build your first file.  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-I4bxBXMUbss/TydN7mQEvVI/AAAAAAAAADo/u8TUvXex2lQ/s1600/sweave4.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://4.bp.blogspot.com/-I4bxBXMUbss/TydN7mQEvVI/AAAAAAAAADo/u8TUvXex2lQ/s1600/sweave4.png" /&gt;&lt;/a&gt;&lt;/div&gt;The above shows a trivial R script.  &amp;lt;&amp;lt;&amp;gt;&amp;gt;= signifies the start of an R script, and @ signifies the end. &lt;br /&gt;&lt;br /&gt;&lt;b&gt;Further Reading&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;To learn more about Sweave, I suggest reading the &lt;a href="http://www.stat.uni-muenchen.de/%7Eleisch/Sweave/Sweave-manual.pdf"&gt;user manual&lt;/a&gt;. &lt;a href="http://stackoverflow.com/questions/8366193/writing-big-documents-with-sweave-is-it-possible-to-do-as-with-latex"&gt;Here&lt;/a&gt; is a good discussion of workflow with big documents and Sweave.  To learn more about LaTeX, I suggest the &lt;a href="http://en.wikibooks.org/wiki/LaTeX"&gt;LaTeX wikibook&lt;/a&gt;, and &lt;a href="http://tobi.oetiker.ch/lshort/lshort.pdf"&gt;The Not So Short Introduction to LaTeX&lt;/a&gt;.  &lt;a href="http://amath.colorado.edu/documentation/LaTeX/basics/example.html"&gt;This page&lt;/a&gt; shows how to make a basic document in LaTeX, and can be a good template to work off of.  This &lt;a href="http://sachaem47.fortyseven.versio.nl/latexcourse/lec4/SweaveInstall.pdf"&gt;link&lt;/a&gt; gave me some of the concepts used above, and also has instructions for how to use Stangle with TeXnicCenter.   &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Edit&lt;/b&gt; A commenter has pointed out that using a directory path with spaces in it can create bugs in LaTeX.  You can find more information on this &lt;a href="http://tex.stackexchange.com/questions/4315/include-image-with-spaces-in-path-directory-to-be-processed-with-dvips"&gt;here&lt;/a&gt;, and &lt;a href="http://groups.google.com/group/latexusersgroup/browse_thread/thread/f78aab8bdaedcdf7?pli=1"&gt;here&lt;/a&gt;.  I have updated my post.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/5897587368307715451/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5897587368307715451'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5897587368307715451'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html' title='Using LaTeX, R, and Sweave to Create Reports in Windows'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://2.bp.blogspot.com/-jCXziuoRBIM/TydIiHrghJI/AAAAAAAAADQ/dusNBwWVyiY/s72-c/sweave2.png' height='72' width='72'/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3736701907267312270</id><published>2012-01-26T19:13:00.000-08:00</published><updated>2012-01-26T19:13:19.281-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='foreach'/><category scheme='http://www.blogger.com/atom/ns#' term='loop'/><category scheme='http://www.blogger.com/atom/ns#' term='parallel'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Parallel R Model Prediction Building and Analytics</title><content type='html'>Modifying R code to run in parallel can lead to huge performance gains.  Although a significant amount of code can easily be run in parallel, there are some learning techniques, such as the Support Vector Machine, that cannot be easily parallelized.  However, there is an often overlooked way to speed up these and other models.  It involves executing the code that generates predictions and other analytics in parallel, instead of executing the model building phase in parallel, which is sometimes impossible.  I will show you how this can be done in this post.&lt;br/&gt;&lt;br/&gt; First, we will set up our variables.  The setup is fairly similar to the one I have used in other posts, but note that the length of the vectors has been increased by a magnitude of 100 to more easily show how much time can be saved by parallelizing the prediction building phase.  &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;y&lt;-c(1:100000)&lt;br /&gt;x1&lt;-c(1:100000)*runif(100000,min=0,max=2)&lt;br /&gt;x2&lt;-c(1:100000)*runif(100000,min=0,max=2)&lt;br /&gt;x3&lt;-c(1:100000)*runif(100000,min=0,max=2)&lt;br /&gt;&lt;br /&gt;all_data&lt;-data.frame(y,x1,x2,x3)&lt;br /&gt;positions &lt;- sample(nrow(all_data),size=floor((nrow(all_data)/4)*3))&lt;br /&gt;training&lt;- all_data[positions,]&lt;br /&gt;testing&lt;- all_data[-positions,]&lt;br /&gt;&lt;/pre&gt; We now have a testing set of 25,000 rows, and a training set of 75,000 rows, which is somewhat linear.  We will train an SVM on this data.  Note that it may take more than 10 minutes to train an SVM on this data, particularly if you have an older computer.  If you do have an older computer, feel free to reduce the number of rows in the data frame as needed.  &lt;pre&gt;&lt;br /&gt;library(e1071)&lt;br /&gt;svm_fit&lt;-svm(y~x1+x2+x3,data=training)&lt;br /&gt;svm_predictions&lt;-predict(svm_fit,newdata=testing)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-svm_predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; After we have built the model, we generate predictions for it, which yields an error of 13045.9 for me(although this may be different for your data).  Our next step is timing the prediction phase to see how long it takes.  &lt;pre&gt;&lt;br /&gt;system.time(predict(svm_fit,newdata=testing))&lt;br /&gt;&lt;/pre&gt; On my system, this took 35.1 seconds.&lt;br/&gt;&lt;br/&gt; We are now ready to set up our parallel infrastructure.  I run Windows, and will use foreach and doSNOW here, although you can certainly use other parallel packages here if you prefer.  You can read &lt;a href="http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html"&gt;this post&lt;/a&gt; if you want an introduction to foreach, doSNOW, and doMC.  If you do not elect to use doSNOW, you will not need to use the stopCluster() function that appears in some of the code below.  &lt;pre&gt;&lt;br /&gt;library(foreach)&lt;br /&gt;library(doSNOW)&lt;br /&gt;cl&lt;-makeCluster(4) #change the 4 to your number of CPU cores&lt;br /&gt;registerDoSNOW(cl)  &lt;br /&gt;&lt;/pre&gt; Now, we have the groundwork for our parallel foreach loop, but we need to find a way to split the data up in order to perform predictions on small sets of data in parallel.    &lt;pre&gt;&lt;br /&gt;num_splits&lt;-4&lt;br /&gt;split_testing&lt;-sort(rank(1:nrow(testing))%%4)&lt;br /&gt;&lt;/pre&gt; This will create a numeric vector that can be used to split the testing data frame into 4 parts.  I suggest setting num_splits to some multiple of your number of CPU cores in order to execute the below foreach loop as quickly as possible.  Now that we have a way to split the data up, we can go ahead and create a loop that will generate predictions in parallel.  &lt;pre&gt;&lt;br /&gt;svm_predictions&lt;-foreach(i=unique(split_testing),&lt;br /&gt;.combine=c,.packages=c("e1071")) %dopar% {&lt;br /&gt;as.numeric(predict(svm_fit,newdata=testing[split_testing==i,]))&lt;br /&gt;}&lt;br /&gt;stopCluster(c1)&lt;br /&gt;&lt;/pre&gt; It is very important that the .packages argument be used to load the package that corresponds to the prediction function you are going to use in the loop, or R will get confused about which prediction function to use and generate an error.  The .combine argument tells the foreach loop to combine the outputs of the foreach loop into a vector.  A hidden argument that defaults to true ensures that all the outputs remain in order.&lt;br/&gt;&lt;br/&gt; Now, we test to make sure that everything is okay by checking what the error value is:  &lt;pre&gt;&lt;br /&gt;error&lt;-sqrt((sum((testing$y-svm_predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; I got 13045.9, which matches the value I got before, and confirms that both the parallel and non-parallel prediction routines return the exact same results.&lt;br/&gt;&lt;br/&gt; Now, we can create a function and time it to see how fast the parallel technique is:  &lt;pre&gt;&lt;br /&gt;parallel_predictions&lt;-function(fit,testing)&lt;br /&gt;{&lt;br /&gt;cl&lt;-makeCluster(4)&lt;br /&gt;registerDoSNOW(cl)&lt;br /&gt;num_splits&lt;-4&lt;br /&gt;split_testing&lt;-sort(rank(1:nrow(testing))%%4)&lt;br /&gt;predictions&lt;-foreach(i=unique(split_testing),&lt;br /&gt;.combine=c,.packages=c("e1071")) %dopar% {&lt;br /&gt;as.numeric(predict(fit,newdata=testing[split_testing==i,]))&lt;br /&gt;}&lt;br /&gt;stopCluster(cl)&lt;br /&gt;predictions&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;system.time(parallel_predictions(svm_fit,testing))&lt;br /&gt;&lt;/pre&gt; This takes 12.76 seconds on my system, which is significantly faster than the non-parallel implementation.&lt;br/&gt;&lt;br/&gt; This technique can be extended to other analytics functions that can be run after the model is built, and it can generate predictions for any model, not just for the svm that the example uses.  While creating the model can take up much more time than generating predictions, it is not always feasible to parallelize model creation.  Running the prediction phase in parallel, particularly on high dimensional data, can save significant time.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/3736701907267312270/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3736701907267312270'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3736701907267312270'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html' title='Parallel R Model Prediction Building and Analytics'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7517490718162120166</id><published>2012-01-23T18:47:00.000-08:00</published><updated>2012-01-23T20:49:34.138-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='data analysis'/><category scheme='http://www.blogger.com/atom/ns#' term='government'/><category scheme='http://www.blogger.com/atom/ns#' term='spending'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Analyzing US Government Contract Awards in R</title><content type='html'>As I was exploring open data sources, I came across &lt;a href="http://usaspending.gov/"&gt;USA spending&lt;/a&gt;.  This site contains information on US government contract awards and other disbursements, such as grants and loans.  In this post, we will look at data on contracts awarded in the state of Maryland in the fiscal year 2011, which is available by selecting "Maryland" as the state where the contract was received and awarded &lt;a href="http://usaspending.gov/data?carryfilters=on"&gt;here&lt;/a&gt;.  I will use Maryland as a proxy for the nation, as the data set for the whole nation will be a bit more unwieldy to analyze, and the USA spending site appears to need a significant amount of time to generate the data file for it.  We may take a look at the data for the whole nation later on. &lt;br /&gt;&lt;br /&gt;First, we download the data file(leave all the options on the usa spending site the same, except select Maryland as the state where the contracts were received and performed when you download the file if you want to follow along), and read it into R:  &lt;br /&gt;&lt;pre&gt;spend&amp;lt;-read.csv(file="marylandspendingbasic.csv",&lt;br /&gt;head=TRUE,sep=",",stringsAsFactors=FALSE,strip.white=TRUE)&lt;br /&gt;&lt;/pre&gt;By using the names() function, we can see that the data file has a lot of interesting columns.  One column is called extentcompeted, and has values indicating how much competition was involved in the bidding process for the contracts.  Understandably, having an open bidding process is in the public interest as it can reduce taxpayer costs.  We will start by looking at competition in the bidding process for contracts:  &lt;br /&gt;&lt;pre&gt;library(ggplot2)&lt;br /&gt;competition&amp;lt;-tapply(spend$ExtentCompeted,spend$ExtentCompeted,&lt;br /&gt;function(x) length(x)/length(spend$ExtentCompeted))&lt;br /&gt;&lt;br /&gt;qplot(names(competition),weight=competition*100,xlab="Competition Type",&lt;br /&gt;ylab="Percent of Contracts",fill=names(competition)) &lt;br /&gt;+ opts(axis.text.x = theme_blank()) &lt;br /&gt;+ scale_fill_discrete("Competition Type")&lt;br /&gt;&lt;/pre&gt;This lets us see what percentage of the data falls into each competition category, and then graphs it using the excellent ggplot2 package.&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-juBIDA_U5jo/Tx4UTUZZmVI/AAAAAAAAACs/JYhrg-HsobA/s1600/competition.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://3.bp.blogspot.com/-juBIDA_U5jo/Tx4UTUZZmVI/AAAAAAAAACs/JYhrg-HsobA/s1600/competition.png" /&gt;&lt;/a&gt;&lt;/div&gt;This shows us that about 58% of contracts went through a full competitive bidding process, whereas only approximately 15% of the contracts did not undergo any kind of competition process.&lt;br /&gt;&lt;br /&gt;Now, lets see how many dollars of spending fall into each competition category:  &lt;br /&gt;&lt;pre&gt;comp_dollars&amp;lt;-tapply(spend$DollarsObligated,spend$ExtentCompeted,&lt;br /&gt;function(x) sum(x)/sum(spend$DollarsObligated,na.rm=TRUE))&lt;br /&gt;qplot(names(comp_dollars),weight=comp_dollars*100,xlab="Competition Type",&lt;br /&gt;ylab="Percentage of Dollars Obligated",fill=names(comp_dollars)) &lt;br /&gt;+ opts(axis.text.x = theme_blank()) &lt;br /&gt;+ scale_fill_discrete("Competition Type")&lt;br /&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-qS7ggV--ErM/Tx4OGl6XebI/AAAAAAAAACY/Pd4_DX-GWRM/s1600/comp_dollars.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-qS7ggV--ErM/Tx4OGl6XebI/AAAAAAAAACY/Pd4_DX-GWRM/s1600/comp_dollars.png" /&gt;&lt;/a&gt;&lt;/div&gt;This plot tells a very different story, showing that about 31% of all dollars that were spent by the government went to contracts that did not involve competition. Further, only 44.5% of all dollars that were obligated went to contracts that were bid on under a fair and open competitive bidding process.&lt;br /&gt;&lt;br /&gt;This indicates that large contracts tend to receive less bidding than small contracts.  This is strange, as large contracts are the ones that are more likely to have reduced costs as a result of competitive bidding, simply because saving 5% of a larger sum is preferable to saving 5% of a smaller sum.  Let's take a look at where these large no-bid contracts are going.&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;companies&amp;lt;-sort(tapply(spend$DollarsObligated,spend$RecipientName,sum))&lt;br /&gt;top_companies&amp;lt;-tail(companies/sum(spend$DollarsObligated),10)*100&lt;br /&gt;sum(spend$DollarsObligated/1e9)&lt;br /&gt;qplot(names(top_companies),weight=top_companies,xlab="Company",&lt;br /&gt;ylab="Percentage of Total Dollars Obligated",fill=names(top_companies)) &lt;br /&gt;+ opts(axis.text.x = theme_blank()) &lt;br /&gt;+ scale_fill_discrete("Company Name") &lt;br /&gt;+ opts(legend.text = theme_text(size = 8))&lt;br /&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-LOlrhnotsDA/Tx4VdLZhxOI/AAAAAAAAAC4/W8eG1lTJBLs/s1600/companies.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://2.bp.blogspot.com/-LOlrhnotsDA/Tx4VdLZhxOI/AAAAAAAAAC4/W8eG1lTJBLs/s1600/companies.png" /&gt;&lt;/a&gt;&lt;/div&gt; First, we note that 15.7 billion dollars in federal contracts were obligated to vendors in the state of Maryland in the fiscal year 2011.  Next, we note that about 11.5% of the total federal money allocated in contracts went to Lockheed Martin!  In fact, the top 10 companies in terms of dollar value of contracts received were given 41% of all the contracted dollars, which amounts to about 6.5 billion dollars.  The top 100 contractors received 72% of all contracted dollars.  Now, it is becoming clearer where the large no-bid contracts are going.  We will look more in depth at the contracts that did not involve competitive bidding to zero in on the issue:  &lt;pre&gt;&lt;br /&gt;nc_spend&lt;-spend[spend$ExtentCompeted=="Not Competed under SAP" &lt;br /&gt;| spend$ExtentCompeted=="Not Competed" &lt;br /&gt;| spend$ExtentCompeted=="Not Available for Competition" &lt;br /&gt;| spend$ExtentCompeted=="Full and Open Competition after exclusion of sources",]&lt;br /&gt;nc_companies&lt;-sort(tapply(nc_spend$DollarsObligated,nc_spend$RecipientName,sum))&lt;br /&gt;nc_top_companies&lt;-tail(nc_companies/sum(nc_spend$DollarsObligated),10)*100&lt;br /&gt;sum(nc_spend$DollarsObligated)/1e9&lt;br /&gt;sum(tail(nc_companies,10))/sum(nc_spend$DollarsObligated)&lt;br /&gt;&lt;/pre&gt;  This tells us that out of the 7.6 billion dollars in government contracts that were disbursed with limited or no competition, 55% of these went to the top 10 contractors who received contracts with limited or no competition.  This shows that large contractors tend to receive a much greater share of no compete contracts than other contractors, as the top 10 contractors only received 41% of all federal dollars, yet received 55% of contracts with limited competition.  &lt;pre&gt;&lt;br /&gt;mean(nc_spend$DollarsObligated)&lt;br /&gt;mean(spend[spend$ExtentCompeted=="Full and Open Competition",]$DollarsObligated)&lt;br /&gt;&lt;/pre&gt; This shows that the average contract size with no or limited competition was $253507.1, whereas the average contract size with full and open competition was $113936.3, meaning that, on average, contracts twice as large are given out with no competition.&lt;br/&gt;&lt;br/&gt;   I will leave this analysis here for now, but please feel free to continue on if you wish.  This is a very interesting data set, and I may come back to it if I have time down the line.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/7517490718162120166/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7517490718162120166'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7517490718162120166'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html' title='Analyzing US Government Contract Awards in R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-juBIDA_U5jo/Tx4UTUZZmVI/AAAAAAAAACs/JYhrg-HsobA/s72-c/competition.png' height='72' width='72'/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5452396106797238718</id><published>2012-01-20T12:02:00.000-08:00</published><updated>2012-01-20T12:02:19.249-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='diagnostics'/><category scheme='http://www.blogger.com/atom/ns#' term='regression'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>R Regression Diagnostics Part 1</title><content type='html'>Linear regression can be a fast and powerful tool to model complex phenomena.  However, it makes several assumptions about your data, and quickly breaks down when these assumptions, such as the assumption that a linear relationship exists between the predictors and the dependent variable, break down.  In this post, I will introduce some diagnostics that you can perform to ensure that your regression does not violate these basic assumptions.  To begin with, I highly suggest reading &lt;a href="http://www.duke.edu/~rnau/testing.htm"&gt;this article&lt;/a&gt; on the major assumptions that linear regression is predicated on.&lt;br/&gt;&lt;br/&gt; We will make use of the same variables as we did in the &lt;a href="http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html"&gt;intro to ensemble learning&lt;/a&gt; post:&lt;br/&gt;&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;y&lt;-c(1:1000)&lt;br /&gt;x1&lt;-c(1:1000)*runif(1000,min=0,max=2)&lt;br /&gt;x2&lt;-(c(1:1000)*runif(1000,min=0,max=2))^2&lt;br /&gt;x3&lt;-log(c(1:1000)*runif(1000,min=0,max=2))&lt;br /&gt;&lt;/pre&gt; Note that x2 and x3 are significantly nonlinear by design(due to the squared and log modifications), and will cause linear regression to make spurious estimates.&lt;br/&gt;&lt;br/&gt; Component residual plots, an extension of partial residual plots, are a good way to see if the predictors have a linear relationship to the dependent variable.  A partial residual plot essentially attempts to model the residuals of one predictor against the dependent variable.  A component residual plot adds a line indicating where the line of best fit lies.  A significant difference between the residual line and the component line indicates that the predictor does not have a linear relationship with the dependent variable.  A good way to generate these plots in R is the car package.&lt;br/&gt;&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;library(car)&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3)&lt;br /&gt;crPlots(lm_fit)&lt;br /&gt;&lt;/pre&gt; Your plot should look like this:  &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-CclCuFhXsuA/TxnDKGAXCxI/AAAAAAAAABw/Glk3q3Mc9fM/s1600/crplots.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="400" width="400" src="http://3.bp.blogspot.com/-CclCuFhXsuA/TxnDKGAXCxI/AAAAAAAAABw/Glk3q3Mc9fM/s400/crplots.png" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;br/&gt; Looking at the plot, we see that 2 of the predictors(x2 and x3) are significantly non-normal, based on the differences between the component and the residual lines.  In order to "correct" these differences, we can attempt to alter the predictors.  Typical alterations are sqrt(x), 1/x, log(x), and n^x.  In this situation, we already know how the data are nonlinear, so we will be able to easily apply the appropriate transformation.  In a "real-world" situation, it may take trial and error to come up with an appropriate transformation to make the predictor appear more linear.  If none of the transformations work, you may have to consider not using the predictor, or switching to a nonlinear model.&lt;br/&gt;&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;library(car)&lt;br /&gt;lm_fit&lt;-lm(y~x1+sqrt(x2)+exp(x3))&lt;br /&gt;crPlots(lm_fit)&lt;br /&gt;&lt;/pre&gt; The above code modifies the linear model by using the square root of x2 as a predictor, and e^x3 as a predictor, which cancel out the squared transformation on x2 and the natural log transformation on x3, respectively.  Your plot should look like this:&lt;br/&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-uvVRMzNsE_s/TxnGQYv8jJI/AAAAAAAAAB8/S1ThQ0oft5o/s1600/crplots2.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="400" width="400" src="http://1.bp.blogspot.com/-uvVRMzNsE_s/TxnGQYv8jJI/AAAAAAAAAB8/S1ThQ0oft5o/s400/crplots2.png" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;br/&gt; As you can see, this plot shows a much more linear relationship between x2 and y and x3 and y.  However, the component and residual lines for the predictors x1, x2, and x3 do not show a perfect overlap.  We will look more into this in a later post.&lt;br/&gt;&lt;br/&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/5452396106797238718/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/r-regression-diagnostics-part-1.html#comment-form' title='1 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5452396106797238718'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/5452396106797238718'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/r-regression-diagnostics-part-1.html' title='R Regression Diagnostics Part 1'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://3.bp.blogspot.com/-CclCuFhXsuA/TxnDKGAXCxI/AAAAAAAAABw/Glk3q3Mc9fM/s72-c/crplots.png' height='72' width='72'/><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8388541914380065480</id><published>2012-01-19T12:30:00.000-08:00</published><updated>2012-01-19T17:59:29.384-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Finance'/><category scheme='http://www.blogger.com/atom/ns#' term='Bailout'/><category scheme='http://www.blogger.com/atom/ns#' term='Banking'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Analyzing Federal Government Bailout Recipients in R</title><content type='html'>I was searching for open data recently, and stumbled on &lt;a href="http://opendata.socrata.com/"&gt;Socrata&lt;/a&gt;. Socrata has a lot of interesting data sets, and while I was browsing around, I found a data set on federal bailout recipients.  &lt;a href="http://opendata.socrata.com/Government/Bailout-Recipients/gbdy-vjgr"&gt;Here&lt;/a&gt; is the data set.  However, data sets on Socrata are not always the most recent versions, so I followed a link to the data source at &lt;a href="http://projects.propublica.org/bailout/list/index"&gt;Propublica&lt;/a&gt;, where I was able to find a data set that was last updated on January 17, 2012.  I downloaded the data in csv format.  In the rest of this post, I will perform basic analysis on this data, and show that R can be used to do the same analysis as Excel in a much simpler and more powerful way.&lt;br /&gt;&lt;br /&gt;The data contains several columns, the most salient of which are the name of the company, its location, its industry, the amount of bailout funds received, and the amount repaid.&lt;br /&gt;&lt;br /&gt;First, we read in the data:  &lt;br /&gt;&lt;pre&gt;bailout&amp;lt;-read.csv(file="bailout.csv",head=TRUE,sep=",",&lt;br /&gt;stringsAsFactors=FALSE,strip.white=TRUE)&lt;br /&gt;&lt;/pre&gt;Next, we generate a variable called owed that calculates how much the company owes the government, or how much the government has made in profit from the company, and create a new data frame that is sorted based on how much is owed the government:  &lt;br /&gt;&lt;pre&gt;bailout&amp;lt;-transform(bailout,owed=bailout$total_disbursed&lt;br /&gt;-bailout$total_revenue-bailout$total_payback)&lt;br /&gt;ordered_bailout&amp;lt;-bailout[with(bailout,order(-owed)),]&lt;br /&gt;&lt;/pre&gt;Now, we can use ggplot2 to create a nice looking bar chart, and use dev.copy() to export it(you do not need to run the last two lines, as they are solely for exporting the chart):  &lt;br /&gt;&lt;pre&gt;library(ggplot2)&lt;br /&gt;qplot(name,weight=owed/1e9, data = ordered_bailout[1:4,], &lt;br /&gt;geom = "bar", xlab = "Institution Name",ylab="Amount Owed in Billions")&lt;br /&gt;dev.copy(png,"most_owed.png")&lt;br /&gt;dev.off()&lt;br /&gt;&lt;/pre&gt;This results in this chart:  &lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-1QVSKEGphIY/Txhyu30riRI/AAAAAAAAABA/pvoOwBkzqI0/s1600/most_owed.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="400" src="http://1.bp.blogspot.com/-1QVSKEGphIY/Txhyu30riRI/AAAAAAAAABA/pvoOwBkzqI0/s400/most_owed.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;As you can see, Fannie Mae, Freddie Mac, GM, and AIG all still owe huge amounts of money to the government.  Now, we look at the top companies that the government made a profit from:  &lt;br /&gt;&lt;pre&gt;qplot(name,weight=abs(owed/1e9), data = tail(ordered_bailout,4), &lt;br /&gt;geom = "bar", xlab = "Institution Name",&lt;br /&gt;ylab="Government Profit in Billions")&lt;br /&gt;&lt;/pre&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-hxMrlpXHUjk/TxhzakvX0mI/AAAAAAAAABM/A1-VNtMG60o/s1600/most_repaid.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="400" src="http://4.bp.blogspot.com/-hxMrlpXHUjk/TxhzakvX0mI/AAAAAAAAABM/A1-VNtMG60o/s400/most_repaid.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;As we can see, banks have given the government a significant profit. &lt;br /&gt;&lt;br /&gt;When we look at the sum of the money owed to the government, we see that &lt;b&gt;242.81 billion&lt;/b&gt; dollars have yet to be repaid:  &lt;br /&gt;&lt;pre&gt;sum(ordered_bailout$owed)/1e9&lt;br /&gt;&lt;/pre&gt;Using the tapply function reveals the following(each heading is a sector name, and the number below is how much they owe the government in billions):  &lt;br /&gt;&lt;pre&gt;tapply(bailout$owed/1e9,bailout$category,sum)&lt;/pre&gt;&lt;pre&gt;&amp;nbsp;&lt;/pre&gt;&lt;pre&gt;                      Bank              FHA Refinance Fund &lt;br /&gt;                   -11.36362217                      0.05000000 &lt;br /&gt;         SBA Security Purchases              State Housing Orgs &lt;br /&gt;                     0.06002858                      0.65537461 &lt;br /&gt;              Mortgage Servicer      Financial Services Company &lt;br /&gt;                     1.93507502                     10.47640782 &lt;br /&gt;                Investment Fund                    Auto Company &lt;br /&gt;                    13.32980009                     28.53374026 &lt;br /&gt;              Insurance Company  Government-Sponsored Enterprise &lt;br /&gt;                    48.36100458                    150.77000000 &lt;br /&gt;&lt;/pre&gt; As you can see, the government sponsored enterprises(Fannie Mae and Freddie Mac), and the insurance companies(primarily AIG), owe the most while the banks have made a profit for the government.  I will not inject any analysis into this, as we are only looking at the numbers here.&lt;br/&gt;&lt;br/&gt; Now, we will look at how much is owed by the top 10 states:  &lt;pre&gt;&lt;br /&gt;states&lt;-sort(tapply(bailout$owed/1e9,bailout$state,sum))&lt;br /&gt;qplot(tail(names(states),10),weight=tail(states,10))&lt;br /&gt;&lt;/pre&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-_4tMgivu_48/Txh3zd4e2YI/AAAAAAAAABk/IA9wjCIP5I8/s1600/owed_by_state.png" imageanchor="1" style="margin-left:1em; margin-right:1em"&gt;&lt;img border="0" height="400" width="400" src="http://2.bp.blogspot.com/-_4tMgivu_48/Txh3zd4e2YI/AAAAAAAAABk/IA9wjCIP5I8/s400/owed_by_state.png" /&gt;&lt;/a&gt;&lt;/div&gt; Not surprisingly, considering that Fannie Mae and Freddie Mac are based in DC, DC owes the most, followed by Virginia.  Puerto Rico is a very interesting addition to the top 10 states/territories that owe the government, and further inspection reveals this:  &lt;pre&gt;&lt;br /&gt;bailout[bailout$state=="PR",]$name&lt;br /&gt;[1] "Popular, Inc."                "First BanCorp"               &lt;br /&gt;[3] "RG Mortgage Corporation"      "Scotiabank de Puerto Rico"   &lt;br /&gt;[5] "Banco Popular de Puerto Rico"&lt;br /&gt;&lt;/pre&gt; Apparently the banking sector in Puerto Rico did not do well in the financial crisis!  &lt;br/&gt;&lt;br/&gt;  I noticed an interesting column in the data called "is_stress_tested", which took on a false value, a true value, or a NULL value.  I am not an expert on banking, and if someone can please shed more light on the stress test, I would appreciate it, but I believe that stress testing is a method that discovers how prone to failure the institution is.&lt;br/&gt;&lt;br/&gt; Now, when we see how much money the institutions that have had stress testing performed versus those that have not owe, we get the following:  &lt;pre&gt;&lt;br /&gt;tapply(bailout$owed/1e9,bailout$is_stress_tested,sum)&lt;br /&gt;              false      true &lt;br /&gt; 13.31646 242.71883 -13.22748 &lt;br /&gt;&lt;/pre&gt; This tells us that institutions that have not been stress tested owe 242 billion to the government, whereas those that have been stress tested have made a profit of 13.2 billion for the government.  I am not sure if stress testing can be performed on an institution like Fannie Mae, but it seems odd that the testing has not been done on those institutions that collectively owe 242 billion dollars.&lt;br/&gt;&lt;br/&gt; Finally, looking at the percentage of disbursed funds that are still outstanding by sector reveals the following(each heading is a sector name, and the number below is the percentage of the bailout funds that they still owe):  &lt;pre&gt;&lt;br /&gt;sort(tapply(bailout$owed,bailout$category,sum)/&lt;br /&gt;tapply(bailout$total_disbursed,bailout$category,sum))&lt;br /&gt;                           Bank          SBA Security Purchases &lt;br /&gt;                    -0.04811175                      0.16305670 &lt;br /&gt;                   Auto Company      Financial Services Company &lt;br /&gt;                     0.46090662                      0.46762480 &lt;br /&gt;              Insurance Company Government-Sponsored Enterprise &lt;br /&gt;                     0.66995920                      0.82478118 &lt;br /&gt;                Investment Fund              FHA Refinance Fund &lt;br /&gt;                     0.84152859                      1.00000000 &lt;br /&gt;              Mortgage Servicer              State Housing Orgs &lt;br /&gt;                     1.00000000                      1.00000000 &lt;br /&gt;&lt;/pre&gt; In general, it appears that Investment Funds, Government-Sponsored Enterprises, and Insurance Companies have been poor at repaying the money they were lent.  The high values for Government Sponsored Enterprises and Insurance Companies are explained by the high outstanding amounts from Fannie Mae, Freddie Mac, and AIG, but the trend in Investment Funds in more interesting.  Despite relatively small bailouts(and I emphasize the relatively, because 15.8 billion dollars were disbursed to Investment Funds), it appears that money has been extremely slow to come back to the Government, with 84% of the funds still outstanding.&lt;br/&gt;&lt;br/&gt; With that, I conclude this post.  I hope that this has shown you how simple data analysis can be done in R in an quick and efficient manner, and I hope that I have also been able to do interesting analyses and draw interesting facts from this data.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/8388541914380065480/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8388541914380065480'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8388541914380065480'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html' title='Analyzing Federal Government Bailout Recipients in R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://1.bp.blogspot.com/-1QVSKEGphIY/Txhyu30riRI/AAAAAAAAABA/pvoOwBkzqI0/s72-c/most_owed.png' height='72' width='72'/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175</id><published>2012-01-19T09:31:00.000-08:00</published><updated>2012-01-19T09:35:07.132-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Machine Learning'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>An Intro to Ensemble Learning in R</title><content type='html'>&lt;b&gt;Introduction&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; This post incorporates parts of &lt;a href="http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html"&gt;yesterday's post&lt;/a&gt; about bagging.  If you are unfamiliar with bagging, I suggest that you read it before continuing with this article.&lt;br/&gt;&lt;br/&gt; I would like to give a basic overview of ensemble learning.  Ensemble learning involves combining multiple predictions derived by different techniques in order to create a stronger overall prediction.  For example, the predictions of a random forest, a support vector machine, and a simple linear model may be combined to create a stronger final prediction set.  The key to creating a powerful ensemble is model diversity.  An ensemble with two techniques that are very similar in nature will perform more poorly than a more diverse model set.&lt;br/&gt;&lt;br/&gt;    Some ensemble learning techniques, such as Bayesian model combination and stacking, attempt to weight the models prior to combining them.  I will save the discussion of how to use these techniques for another day, and will instead focus on simple model combination.&lt;br/&gt;&lt;br/&gt; &lt;b&gt;Initial Setup&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; We will begin with similar variables to those that I used yesterday in the introduction to bagging.  However, I have altered x2 and x3 to introduce distinct nonlinear tendencies, in order to evaluate the performance of nonlinear learning techniques.&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;y&lt;-c(1:1000)&lt;br /&gt;x1&lt;-c(1:1000)*runif(1000,min=0,max=2)&lt;br /&gt;x2&lt;-(c(1:1000)*runif(1000,min=0,max=2))^2&lt;br /&gt;x3&lt;-log(c(1:1000)*runif(1000,min=0,max=2))&lt;br /&gt;&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3)&lt;br /&gt;summary(lm_fit)&lt;br /&gt;&lt;/pre&gt; As you can see, the R-squared value is now .6658, indicating that the predictors have less of a linear correlation to the dependent variable than the predictors from the bagging intro yesterday.  &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;all_data&lt;-data.frame(y,x1,x2,x3)&lt;br /&gt;positions &lt;- sample(nrow(all_data),size=floor((nrow(all_data)/4)*3))&lt;br /&gt;training&lt;- all_data[positions,]&lt;br /&gt;testing&lt;- all_data[-positions,]&lt;br /&gt;&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training)&lt;br /&gt;predictions&lt;-predict(lm_fit,newdata=testing)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; Dividing the data into training and testing sets and using a simple linear model to make predictions about the testing set yields a root mean squared error of 177.36.&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;library(foreach)&lt;br /&gt;length_divisor&lt;-6&lt;br /&gt;iterations&lt;-5000&lt;br /&gt;predictions&lt;-foreach(m=1:iterations,.combine=cbind) %do% {&lt;br /&gt;training_positions &lt;- sample(nrow(training), size=floor((nrow(training)/length_divisor)))&lt;br /&gt;train_pos&lt;-1:nrow(training) %in% training_positions&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training[train_pos,])&lt;br /&gt;predict(lm_fit,newdata=testing)&lt;br /&gt;}&lt;br /&gt;predictions&lt;-rowMeans(predictions)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; Creating 5000 simple linear models and averaging the results creates a prediction error of 177.2591, which is marginally superior to the initial results.  &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Creating the First Ensemble&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; To create our initial ensemble, we will use the linear model, and a random forest.  A random forest can be a powerful learning technique.  It creates multiple decision trees from randomized sets of predictors and observations.  It will be less useful here, as we only have three predictors, but will still illustrate the general point.&lt;br/&gt;&lt;br/&gt; Now, we will test the efficacy of a random forest on our data:  &lt;pre&gt;&lt;br /&gt;library(randomForest)&lt;br /&gt;rf_fit&lt;-randomForest(y~x1+x2+x3,data=training,ntree=500)&lt;br /&gt;predictions&lt;-predict(rf_fit,newdata=testing)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; My error value came to 134.98 with the above code.  As you can see, random forests can make much better predictions than linear models.  In this case, the linear model could not deal with the nonlinear predictors, whereas the random forest could.  One should beware the trap of overfitting, however, as while random forests are much less prone to it than a single decision tree, it can still occur in very noisy data.&lt;br/&gt;&lt;br/&gt; Note that a random forest already incorporates the idea of bagging into the basic algorithm, so we will gain little to nothing by running a random forest through our bagging function.  What we will do instead is this:  &lt;pre&gt;&lt;br /&gt;length_divisor&lt;-6&lt;br /&gt;iterations&lt;-5000&lt;br /&gt;predictions&lt;-foreach(m=1:iterations,.combine=cbind) %do% {&lt;br /&gt;training_positions &lt;- sample(nrow(training), size=floor((nrow(training)/length_divisor)))&lt;br /&gt;train_pos&lt;-1:nrow(training) %in% training_positions&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training[train_pos,])&lt;br /&gt;predict(lm_fit,newdata=testing)&lt;br /&gt;}&lt;br /&gt;lm_predictions&lt;-rowMeans(predictions)&lt;br /&gt;&lt;br /&gt;library(randomForest)&lt;br /&gt;rf_fit&lt;-randomForest(y~x1+x2+x3,data=training,ntree=500)&lt;br /&gt;rf_predictions&lt;-predict(rf_fit,newdata=testing)&lt;br /&gt;predictions&lt;-(lm_predictions+rf_predictions)/2&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; This combines the results of the linear model and the random forest using equal weights.  Not surprisingly, considering that the linear model is weaker than the random forest, we end up with an error of 147.97.  While this is not a fantastic result, it does illustrate ensemble learning.&lt;br/&gt;&lt;br/&gt; &lt;b&gt;Improving the Performance of Our Ensemble&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; From here, we have two options.  We can either combine the predictions of the linear model and the random forest in different ratios until we achieve a better result(I would do this with caution in the real world, and use one hold out set to find the optimal weights, and test the weightings on another hold out set, as this can lead to overfitting if done improperly), or we can replace the linear model with a better one.  We will try both, but first we will try combining the results in different ratios.  Our prior knowledge of the error rates tells us to use a small ratio for the linear model.&lt;br/&gt;&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;predictions&lt;-(lm_predictions+rf_predictions*9)/10&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; This results in an error rate of 136.23, which is not an improvement on the random Forest alone.  Next, we replace the linear model with a support vector machine(svm) from the e1071 package, which provides an R interface to libSVM.  A support vector machine is based on some complicated mathematics, but is basically a stronger machine learning technique that can pick up nonlinear tendencies in the data, depending on what kind of kernel is used.&lt;br/&gt; &lt;pre&gt;&lt;br /&gt;library(e1071)&lt;br /&gt;svm_fit&lt;-svm(y~x1+x2+x3,data=training)&lt;br /&gt;svm_predictions&lt;-predict(svm_fit,newdata=testing)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-svm_predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; The error when using an svm is 129.87, which is superior to both the random forest and the linear models.  Next we will try using the svm with our bagging function.  &lt;pre&gt;&lt;br /&gt;length_divisor&lt;-6&lt;br /&gt;iterations&lt;-5000&lt;br /&gt;predictions&lt;-foreach(m=1:iterations,.combine=cbind) %do% {&lt;br /&gt;training_positions &lt;- sample(nrow(training), size=floor((nrow(training)/length_divisor)))&lt;br /&gt;train_pos&lt;-1:nrow(training) %in% training_positions&lt;br /&gt;svm_fit&lt;-svm(y~x1+x2+x3,data=training[train_pos,])&lt;br /&gt;predict(svm_fit,newdata=testing)&lt;br /&gt;}&lt;br /&gt;svm2_predictions&lt;-rowMeans(predictions)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-svm2_predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; The error with 5000 iterations of an svm model is 141.14. In this case, it appears that the svm performs better without bagging techniques.  It may be that there is too little data for it to be effective when used like this.  However, the time it takes an svm increases exponentially(I believe) with more observations, so sometimes various techniques, including reduction in the number of observations or features, will need to be performed to improve svm performance to tolerable levels.  Going forward, we will use the results of the single svm for the rest of this article.  &lt;pre&gt;&lt;br /&gt;predictions&lt;-(svm_predictions+rf_predictions)/2&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; When we equally combine the svm predictions from the single model with the random forest predictions, we get an error rate of 128.8, which is superior to either the svm model alone, or the random forest model alone.  &lt;pre&gt;&lt;br /&gt;predictions&lt;-(svm_predictions*2+rf_predictions)/3&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;error&lt;br /&gt;&lt;/pre&gt; If we tweak the ratios to emphasize the stronger svm model, we lower our error to 128.34.  &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; As we have seen, ensemble learning can outperform any one single model when used properly.  A good exercise to continue on from here would be to see if other models can improve performance when added to the ensemble.  Please feel free to email me or leave a comment if you see something incorrect or have any questions.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4026568941847141175/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html#comment-form' title='7 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' title='An Intro to Ensemble Learning in R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>7</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8266828013921432350</id><published>2012-01-19T08:06:00.000-08:00</published><updated>2012-01-19T08:06:20.516-08:00</updated><app:control xmlns:app='http://purl.org/atom/app#'><app:draft>yes</app:draft></app:control><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><title type='text'>An Introduction to Open Data Analysis Using DC Government Purchases</title><content type='html'>I decided to see what information could be discovered using open data sets.  &lt;a href="http://opendata.socrata.com/"&gt;Socrata&lt;/a&gt; is a good starting point, and I quickly found a data set that listed all purchases made on DC government credit cards from January 2009 to April 2009.  Further investigation led to a more comprehensive data set located &lt;a href="http://ocp.dc.gov/DC/OCP/e-Library/What+We+Buy/What+We+Buy-Purchase+Card+Transactions"&gt;here&lt;/a&gt;, which contains data through June of 2010.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8266828013921432350'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8266828013921432350'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7265748392976191068</id><published>2012-01-18T07:49:00.000-08:00</published><updated>2012-01-18T15:17:23.985-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Improve Predictive Performance in R with Bagging</title><content type='html'>Bagging, aka bootstrap aggregation, is a relatively simple way to increase the power of a predictive statistical model by taking multiple random samples(with replacement) from your training data set, and using each of these samples to construct a separate model and separate predictions for your test set.  These predictions are then averaged to create a, hopefully more accurate, final prediction value.&lt;br/&gt;&lt;br/&gt;   One can quickly intuit that this technique will be more useful when the predictors are more unstable.  In other words, if the random samples that you draw from your training set are very different, they will generally lead to very different sets of predictions.  This greater variability will lead to a stronger final result.  When the samples are extremely similar, all of the predictions derived from the samples will likewise be extremely similar, making bagging a bit superfluous.&lt;br/&gt;&lt;br/&gt;     Taking smaller samples from your training set will induce greater instability, but taking samples that are too small will result in useless models.  Generally, some fraction of your training set between 1/50th and 1/2 will be useful for bagging purposes(of course, this greatly depends on how many observations are in your training set).  The smaller your bagging samples, the more samples you will need to collect and the more models you will need to generate to create more stability in the final predictors.&lt;br/&gt;&lt;br/&gt;   While there are some libraries that will take care of bagging for you in R, writing your own function allows you a greater degree of control and understanding over the process, and it is a relatively quick exercise.&lt;br/&gt;&lt;br/&gt;   Okay, enough theoretical framework.  Lets jump into the code.  Anyone who wants more theory can consult &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.7654&amp;rep=rep1&amp;type=pdf"&gt;this paper&lt;/a&gt; about bagging.&lt;br/&gt;&lt;br/&gt;     I'm going to construct a relatively trivial example where a dependent variable, y, can be predicted by some combination of the independent variables x1, x2, and x3.&lt;br/&gt;&lt;br/&gt;   &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;y&lt;-c(1:1000)&lt;br /&gt;x1&lt;-c(1:1000)*runif(1000,min=0,max=2)&lt;br /&gt;x2&lt;-c(1:1000)*runif(1000,min=0,max=2)&lt;br /&gt;x3&lt;-c(1:1000)*runif(1000,min=0,max=2)&lt;br /&gt;&lt;/pre&gt; As you can see, y is a sequence of the values from 1 to 1000.  x1, x2, and x3 are permutations of y, but with random errors added.  runif generates a specified number of random numbers from 0 to 1, unless a min and max are specified, in which case the numbers fall between those values.  Each of the x sequences will roughly approximate y, but with random errors thrown in.  The set.seed function is simply to ensure that the subsequent random number generation proceeds in a predictable fashion, so that your results match mine.&lt;br/&gt;&lt;br/&gt;   Fitting a linear model to the variables results in an R squared of .7042:  &lt;pre&gt;&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3)&lt;br /&gt;summary(lm_fit)&lt;br /&gt;&lt;/pre&gt; Now we will see how well the x values predict y.  First, we designate a random sample of y to be our "test" set.  The rest will be the training set.  &lt;pre&gt;&lt;br /&gt;set.seed(10)&lt;br /&gt;all_data&lt;-data.frame(y,x1,x2,x3)&lt;br /&gt;positions &lt;- sample(nrow(all_data),size=floor((nrow(all_data)/4)*3))&lt;br /&gt;training&lt;- all_data[positions,]&lt;br /&gt;testing&lt;- all_data[-positions,]&lt;br /&gt;&lt;/pre&gt; The above code places all of our variables into a data frame, then randomly selects 3/4 of the data to be the training set, and places the rest into the testing set.&lt;br/&gt;&lt;br/&gt; We are now able to generate predictions for the testing set by creating a linear model on the training set and applying it to the testing set.  We are also able to calculate the prediction error by subtracting the actual values from the predicted values (the error calculation here is root mean squared error):  &lt;pre&gt;&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training)&lt;br /&gt;predictions&lt;-predict(lm_fit,newdata=testing)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;&lt;/pre&gt; The calculated error should be 161.15.&lt;br/&gt;&lt;br/&gt; The next step is to run a function that implements bagging.  In order to do this, I will be using the foreach package.  Although I will not use it in parallel mode, this code is designed for parallel execution, and I highly recommend reading &lt;a href="http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html"&gt;my post&lt;/a&gt; about how to do it if you do not know how.  &lt;pre&gt;&lt;br /&gt;library(foreach)&lt;br /&gt;length_divisor&lt;-4&lt;br /&gt;iterations&lt;-1000&lt;br /&gt;predictions&lt;-foreach(m=1:iterations,.combine=cbind) %do% {&lt;br /&gt;training_positions &lt;- sample(nrow(training), size=floor((nrow(training)/length_divisor)))&lt;br /&gt;train_pos&lt;-1:nrow(training) %in% training_positions&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training[train_pos,])&lt;br /&gt;predict(lm_fit,newdata=testing)&lt;br /&gt;}&lt;br /&gt;predictions&lt;-rowMeans(predictions)&lt;br /&gt;error&lt;-sqrt((sum((testing$y-predictions)^2))/nrow(testing))&lt;br /&gt;&lt;/pre&gt; The above code randomly samples 1/4 of the training set in each iteration, and generates predictions for the testing set based the sample.  It will execute the number of time specified by iterations.  When iterations was set to 10, I received an error value of 161.10.  At 300 iterations, error went to 161.12, at 500 iterations, error went to 161.19, at 1000 iterations, error went to 161.13, and at 5000 iterations, error went to 161.07.  Eventually, bagging will converge, and more iterations will not help any further.  However, the potential for improvement in results exists.  You should be extremely cautious and assess the stability of the results before deploying this approach, however, as too few iterations or too large a length divisor can cause extremely unstable results.  This example is trivial, but this can lead to better results in a more "real-world" application.&lt;br/&gt;&lt;br/&gt; Finally, we can place this code into a function to wrap it up nicely:  &lt;pre&gt;&lt;br /&gt;bagging&lt;-function(training,testing,length_divisor=4,iterations=1000)&lt;br /&gt;{&lt;br /&gt;predictions&lt;-foreach(m=1:iterations,.combine=cbind) %do% {&lt;br /&gt;training_positions &lt;- sample(nrow(training), size=floor((nrow(training)/length_divisor)))&lt;br /&gt;train_pos&lt;-1:nrow(training) %in% training_positions&lt;br /&gt;lm_fit&lt;-lm(y~x1+x2+x3,data=training[train_pos,])&lt;br /&gt;predict(lm_fit,newdata=testing)&lt;br /&gt;}&lt;br /&gt;rowMeans(predictions)&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; As you can see, bagging can be a useful tool when used correctly.  Although this is a trivial example, you can replace the data and even replace the simple linear model with more powerful models to make this function more useful.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/7265748392976191068/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html#comment-form' title='4 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html' title='Improve Predictive Performance in R with Bagging'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4707394308354424625</id><published>2012-01-17T16:59:00.000-08:00</published><updated>2012-01-18T07:51:32.634-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Finance'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Time Based Arbitrage Opportunities in Tick Data</title><content type='html'>I recently posted an &lt;a href="http://viksalgorithms.blogspot.com/2012/01/introduction-to-kaggle-algorithmic.html"&gt;introduction&lt;/a&gt; to the &lt;a href="http://www.kaggle.com/c/AlgorithmicTradingChallenge"&gt;Kaggle Algorithmic Trading Challenge&lt;/a&gt;, which I competed in.&lt;br /&gt;&lt;br /&gt;I said that I would post about my experiences, and this is hopefully the first of a series.  We were given tick data from the London Stock Exchange(specifically, the FTSE 100) over random time intervals during parts of 37 days.  Each data row that we were given corresponded to a liquidity shock event and the surrounding bid/ask prices.  Given 50 bid prices and 50 ask prices prior to the liquidity shock event, we had to predict the next 50 bid and ask prices.&lt;br /&gt;&lt;br /&gt;I, and others, noticed that there were distinct areas of high volatility in the tick data at certain times of day.  Specifically, 10:15, 13:30, and 15:00 (london time) all showed abnormal volatility.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://4.bp.blogspot.com/-pGmuLzJO6kQ/TxYNiAenKRI/AAAAAAAAAAc/Vtxg4XvRBBE/s1600/trade_time_vs_residuals.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="287" src="http://4.bp.blogspot.com/-pGmuLzJO6kQ/TxYNiAenKRI/AAAAAAAAAAc/Vtxg4XvRBBE/s400/trade_time_vs_residuals.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;The above chart plots the residuals of a simple linear model that predicts the average of the first five bid and ask prices after the liquidity shock event at t=50 against the time the trade was made.  The x-axis(time the trade was made) is in the units of hours after 8:00, so a value of 2 on the x-axis corresponds to 10:00.  The residuals of the linear fit indicate how "hard" the values of the bid-ask time series were to predict, which is a proxy for volatility.  The below R code generated the plot:  &lt;br /&gt;&lt;pre&gt;lm_fit&amp;lt;-lm(current_formula,data=cbind(extracted_training_dependent_variable,&lt;/pre&gt;&lt;pre&gt;extracted_training_data))&lt;br /&gt;plot(x=extracted_training_data$trade_time_50,y=lm_fit$residuals)&lt;br /&gt;&lt;/pre&gt;This can also be observed when plotting the normalized standard deviation of the bid series against trade time, although the spike at 10:15 is harder to see:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-fqF2vP7nCK0/TxYRpfUN_dI/AAAAAAAAAAk/7R90oy4Gtx0/s1600/sd_vs_trade_time.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="286" src="http://1.bp.blogspot.com/-fqF2vP7nCK0/TxYRpfUN_dI/AAAAAAAAAAk/7R90oy4Gtx0/s400/sd_vs_trade_time.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;Although the spike in volatility at 13:30 appears to be caused by the opening of the NASDAQ and NYSE, it is unclear what the other two spikes might represent, although other markets may be opening at those times.  Specifically, the spikes appear to be caused by other markets/traders valuing the stocks differently, which leads to an opportunity for profit taking.&lt;br /&gt;&lt;br /&gt;Here is a typical series in which a spike occurs:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-X8K_opJvEEc/TxYVhrxgenI/AAAAAAAAAAs/XcVRj5x3Ck0/s1600/example_time_series.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="287" src="http://1.bp.blogspot.com/-X8K_opJvEEc/TxYVhrxgenI/AAAAAAAAAAs/XcVRj5x3Ck0/s400/example_time_series.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;In this series, a liquidity shock occurs when x=50(although there are several shocks prior to this).  The whole sequence from x=1 to x=50 takes place over 13 seconds, and each x value is a distinct trade or quote event.  The red points are ask prices, and the blue points are bid prices.  As you can see, there appears to be a significant arbitrage opportunity over a fairly long time scale as the ask prices lower, perhaps in response to the volatile bid prices(which may be a result of traders from other areas eating up the available liquidity on the bid side), then rise again after the shock at t=50. &lt;br /&gt;&lt;br /&gt;This same pattern exists in other tick data time series that were given to competitors in this challenge, which could make for potentially interesting arbitrage opportunities if it is able to be exploited.&lt;br /&gt;&lt;br /&gt;Another interesting feature of the tick data when plotted over time is the fact that there is much higher volatility earlier in the day.&amp;nbsp; This corresponds with several papers that noted the same phenomenon.&amp;nbsp; When training a predictive model on tick data, a significant portion of the days error(30-40%) can occur in the first hour of trading.&amp;nbsp; This points to the need for specialized models to be developed for the first hour of trading, but it also points to the fact that the first hour, is, in some ways, relatively unpredictable.&amp;nbsp; A model that does not attempt to make predictions in the first hour may make significantly more "correct" guesses and thus significantly more profit than one that does not.&lt;br /&gt;&lt;br /&gt;The below plot is an easier way to look at this.&amp;nbsp; It uses the tapply function to find the mean normalized residual for each one-minute time slice in the trading day:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-k5dKWN6LjPM/TxYY4-CcZEI/AAAAAAAAAA0/m6KGaa0tG3s/s1600/tapply_error_plot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="286" src="http://3.bp.blogspot.com/-k5dKWN6LjPM/TxYY4-CcZEI/AAAAAAAAAA0/m6KGaa0tG3s/s400/tapply_error_plot.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As you can see, there is more than 4 times as much prediction error(remember, a proxy for volatility) in the first minute as there is in the last.&lt;br /&gt;&lt;br /&gt;I will wrap this post up at this point.&amp;nbsp; I will post more of my findings soon.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4707394308354424625/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html#comment-form' title='2 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4707394308354424625'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4707394308354424625'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html' title='Time Based Arbitrage Opportunities in Tick Data'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media='http://search.yahoo.com/mrss/' url='http://4.bp.blogspot.com/-pGmuLzJO6kQ/TxYNiAenKRI/AAAAAAAAAAc/Vtxg4XvRBBE/s72-c/trade_time_vs_residuals.png' height='72' width='72'/><thr:total>2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682</id><published>2012-01-17T14:41:00.000-08:00</published><updated>2012-01-18T07:51:22.273-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Linux'/><category scheme='http://www.blogger.com/atom/ns#' term='Windows'/><category scheme='http://www.blogger.com/atom/ns#' term='parallel'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Parallel R Loops for Windows and Linux</title><content type='html'>Parallel computation may seem difficult to implement and a pain to use, but it is actually quite simple to use.  The foreach package provides the basic loop structure, which can utilize various parallel backends to execute the loop in parallel.  First, let's go over the basic structure of a foreach loop.  To get the foreach package, run the following command:  &lt;br /&gt;&lt;pre&gt;install.packages("foreach")&lt;/pre&gt;Then, initialize the library:  &lt;br /&gt;&lt;pre&gt;library(foreach)&lt;/pre&gt;A basic, nonparallel, foreach loop looks like this:  &lt;br /&gt;&lt;pre&gt;foreach(i=1:10) %do% {&lt;br /&gt;&lt;br /&gt;#loop contents here&lt;br /&gt;&lt;br /&gt;}&lt;/pre&gt;To execute the loop in parallel, the %do% command must be replaced with %dopar%:  &lt;br /&gt;&lt;pre&gt;foreach(i=1:10) %dopar% {&lt;br /&gt;&lt;br /&gt;#loop contents here&lt;br /&gt;&lt;br /&gt;}&lt;/pre&gt;To capture the return values of the loop:  &lt;br /&gt;&lt;pre&gt;list&amp;lt;-foreach(i=1:10) %do% {&lt;br /&gt;i&lt;br /&gt;}&lt;/pre&gt;Note that the foreach loop returns a list of values by default.  The foreach package will always return a result with the items in the same order as the counter, even when running in parallel.  For example, the above loop will return a list with indices 1 through 10, each containing the same value as their index(1 to 10).&lt;br /&gt;&lt;br /&gt;In order to return the results as a matrix, you will need to alter the .combine behavior of the foreach loop.  This is done in the following code:  &lt;br /&gt;&lt;pre&gt;matrix&amp;lt;-foreach(i=1:10,.combine=cbind) %do% {&lt;br /&gt;i&lt;br /&gt;}&lt;/pre&gt;This will return a matrix with 10 columns, with values in order from 1 to 10.&lt;br /&gt;&lt;br /&gt;Likewise, this will return a matrix with 10 rows:  &lt;br /&gt;&lt;pre&gt;matrix&amp;lt;-foreach(i=1:10,.combine=rbind) %do% {&lt;br /&gt;i&lt;br /&gt;}&lt;/pre&gt;This can be done with multiple return values to create n x k matrices.  For example, this will return a 10 x 2 matrix:  &lt;br /&gt;&lt;pre&gt;matrix&amp;lt;-foreach(i=1:10,.combine=rbind) %do% {&lt;br /&gt;c(i,i)&lt;br /&gt;}&lt;/pre&gt;&lt;br /&gt;&lt;b&gt;Parallel Backends&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;In order to run the foreach loop in parallel(using the %dopar% command), you will need to install and register a parallel backend.  Because windows does not support forking, the same backend that works a linux or an OS X environment will not work for windows.  Under linux, the doMC package provides a convenient parallel backend.&lt;br /&gt;&lt;br /&gt;Here is how to use the package(of course, you need to install doMC first):  &lt;br /&gt;&lt;pre&gt;library(foreach)&lt;br /&gt;library(doMC)&lt;br /&gt;registerDoMC(2)  #change the 2 to your number of CPU cores  &lt;br /&gt;&lt;br /&gt;foreach(i=1:10) %dopar% {&lt;br /&gt;&lt;br /&gt;#loop contents here&lt;br /&gt;&lt;br /&gt;}&lt;/pre&gt;Under windows, the doSNOW package is very convenient, although it has some issues.  I do not recommend the doSMP package, as it has significant issues.  &lt;br /&gt;&lt;pre&gt;library(doSNOW)&lt;br /&gt;library(foreach)&lt;br /&gt;cl&amp;lt;-makeCluster(2) #change the 2 to your number of CPU cores&lt;br /&gt;registerDoSNOW(cl)&lt;br /&gt;&lt;br /&gt;foreach(i=1:10) %dopar% {&lt;br /&gt;&lt;br /&gt;#loop contents here&lt;br /&gt;&lt;br /&gt;} &lt;/pre&gt;&lt;br /&gt;Edit:&amp;nbsp; Thanks to an alert reader, I noticed that I neglected to add in the code to stop the clusters.&amp;nbsp; This will need to be run after you finish executing all of your parallel code if you are using doSNOW. &lt;pre&gt;&lt;br /&gt;stopCluster(cl)&lt;br /&gt;&lt;/pre&gt;  Also please note that you will need to set the parameter in the makeCluster and registerDoMC functions to the number of CPU cores that your computer possesses, or less if you do not want to use all of your CPU cores. &lt;br/&gt;&lt;br/&gt;I hope that this has been a good introduction to parallel loops in R.  The new version of R(2.14), also includes the parallel package, which I will discuss further in a later post.  You can find more information on the packages mentioned in this article on CRAN.  &lt;a href="http://cran.r-project.org/web/packages/foreach/index.html"&gt;Foreach&lt;/a&gt;, &lt;a href="http://cran.r-project.org/web/packages/doSNOW/index.html"&gt;doSNOW&lt;/a&gt;, and &lt;a href="http://cran.r-project.org/web/packages/doMC/index.html"&gt;doMC&lt;/a&gt; can all be found there.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/2026240887464657682/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html#comment-form' title='5 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' title='Parallel R Loops for Windows and Linux'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>5</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-218547080807202456</id><published>2012-01-16T15:52:00.000-08:00</published><updated>2012-01-18T07:51:09.882-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Machine Learning'/><category scheme='http://www.blogger.com/atom/ns#' term='Statistics'/><title type='text'>Resources for Learning Statistics and Data Mining</title><content type='html'>There are an abundance of resources on the web to help novices teach themselves statistics and machine learning, but they can be hard to track down.  Below are a few resources that have helped me in the past.&lt;br&gt;&lt;br&gt; 1. &lt;b&gt;&lt;a href="http://www.khanacademy.org/#statistics"&gt;Khan Academy&lt;/a&gt;&lt;/b&gt; has a series of excellent videos on the basics of statistics and probability.  They range from 5 minutes to 20 minutes in length.  You can create an account to track your progress if you desire. Although good for the basics, Khan Academy does not cover advanced topics.&lt;br&gt;&lt;br&gt; 2. &lt;b&gt;&lt;a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/"&gt;The Elements of Statistical Learning&lt;/a&gt;&lt;/b&gt;, a free ebook, is a relatively in-depth overview of the major concepts and techniques involved in machine learning.  You will need to learn inferential statistics before reading this book, as it assumes that the reader understands the basics.&lt;br&gt;&lt;br&gt;   3. &lt;b&gt;&lt;a href="http://www.statsoft.com/textbook/"&gt;The Statsoft Online Statistics Textbook&lt;/a&gt;&lt;/b&gt; is a good resource on statistics and machine learning.  A reader will need a basic understanding of statistics and probability before reading this text.  It is not particularly in depth, but is good for an overview of major concepts, or as a refresher.&lt;br&gt;&lt;br&gt; 4. &lt;b&gt;&lt;a href="http://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA&amp;feature=plcp"&gt;Machine Learning Videos from mathematicalmonk&lt;/a&gt;&lt;/b&gt; can be found on youtube(the link is to the full playlist).  Although I have not viewed all of these videos personally, they appear to be a good overview of machine learning techniques.&lt;br&gt;&lt;br&gt; 5. &lt;b&gt;&lt;a href="http://www.ml-class.org/"&gt;Andrew Ng's Online Machine Learning Class&lt;/a&gt;&lt;/b&gt; is a simplified version of the Stanford class CS229.  It glosses over most of the mathematics involved, but is a very good introductory resource for machine learning.  Mlclass also features quizzes and programming exercises that create additional engagement.  I would recommend that the reader study basic statistics before attempting this class.&lt;br&gt;&lt;br&gt; 6. &lt;b&gt;&lt;a href="http://faculty.vassar.edu/lowry/webtext.html"&gt;Concepts and Applications of Inferential Statistics&lt;/a&gt;&lt;/b&gt; is written by a professor at Vassar, and is a good introduction to basic statistics.&lt;br&gt;&lt;br&gt; 7. &lt;b&gt;&lt;a href="http://onlinestatbook.com/2/index.html"&gt;Online Stat Book&lt;/a&gt;&lt;/b&gt;  is a good resource for basic statistical concepts.  It has lots of interactive exercises interspersed throughout the text.&lt;br&gt;&lt;br&gt; 8. &lt;b&gt;&lt;a href="http://www.math.umass.edu/~lavine/Book/book.pdf"&gt;Introduction to Statistical Thought&lt;/a&gt;&lt;/b&gt; covers basic probability and statistics, and covers some more advanced topics such as time series and survival analysis. It also has R code that the reader can implement.&lt;br&gt;&lt;br&gt; 9.  &lt;b&gt;&lt;a href="http://joshua.smcvt.edu/linearalgebra/"&gt;Linear Algebra&lt;/a&gt;&lt;/b&gt;, a free to download ebook, provides an overview of linear algebra equivalent to an initial undergraduate course.  I have not read through it yet.&lt;br&gt;&lt;br&gt; 10. &lt;b&gt;&lt;a href="http://ocw.mit.edu/index.htm"&gt;MIT Opencourseware&lt;/a&gt;&lt;/b&gt; is an excellent site that has full video lecture series and problem sets for hundreds of classes including linear algebra, calculus, and statistics.&lt;br&gt;&lt;br&gt;  &lt;b&gt;Further Reading&lt;/b&gt;&lt;br&gt;&lt;br&gt; &lt;a href="http://steve-yegge.blogspot.com/2006/03/math-for-programmers.html"&gt;This&lt;/a&gt; is an interesting blog post on how to learn mathematics as a programmer.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/218547080807202456/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/resources-for-learning-statistics-and.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/218547080807202456'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/218547080807202456'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/resources-for-learning-statistics-and.html' title='Resources for Learning Statistics and Data Mining'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4281820141342452496</id><published>2012-01-10T19:00:00.000-08:00</published><updated>2012-01-18T07:50:55.439-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Finance'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Time Series Cointegration in R</title><content type='html'>Cointegration can be a valuable tool in determining the mean reverting properties of 2 time series.  A full description of cointegration can be found on &lt;a href="http://en.wikipedia.org/wiki/Cointegration"&gt;Wikipedia&lt;/a&gt;.  Essentially, it seeks to find stationary linear combinations of the two vectors.&lt;br&gt;&lt;br&gt; The below R code, which has been modified from &lt;a href="http://quanttrader.info/public/testForCoint.html"&gt;here&lt;/a&gt;, will test two series for integration and return the p-value indicating the likelihood of correlation.  It runs significantly faster than the original code, however.  I used this for relatively short time series(50 observations), and while it functioned relatively quickly for small numbers of series, it became cumbersome to use when attempting to serially cointegrate over 100k pairs of bid-ask price series when using it with an mapply function.  So scaling up may be an issue.  &lt;pre class="ruby"&gt;&lt;br /&gt;library(tseries)&lt;br /&gt;cointegration&lt;-function(x,y)&lt;br /&gt;{&lt;br /&gt;vals&lt;-data.frame(x,y)&lt;br /&gt;beta&lt;-coef(lm(vals[,2]~vals[,1]+0,data=vals))[1]&lt;br /&gt;(adf.test(vals[,2]-beta*vals[,1], alternative="stationary", k=0))$p.value&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt; This runs an &lt;a href="http://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test"&gt;augmented Dickey-Fuller test&lt;/a&gt; and will return a p-value indicating whether the series are mean-reverting or not.  You can use the typical p-value as a test of significance if you like(ie, a p-value below .05 indicates a mean-reverting spread), or you can use an alternate value. This assumes that your two series were observed at the same time points.  The &lt;a href="http://quanttrader.info/public/testForCoint.html"&gt;original post&lt;/a&gt; that this code as modified from contains a further description of cointegration, along with more time series data type handling.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4281820141342452496/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-series-cointegration-in-r.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4281820141342452496'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4281820141342452496'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-series-cointegration-in-r.html' title='Time Series Cointegration in R'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8113942018418819016</id><published>2012-01-10T18:38:00.000-08:00</published><updated>2012-01-18T07:50:47.496-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Ruby'/><category scheme='http://www.blogger.com/atom/ns#' term='Windows'/><title type='text'>Playing MP3s in Ruby on Windows</title><content type='html'>I recently needed to play music in Ruby in order to create an alarm clock of sorts.  The code to do this was (surprise surprise) fairly simple, but I want to post it for posterity.  &lt;pre class="ruby"&gt;&lt;br /&gt;require 'win32ole'&lt;br /&gt;player = WIN32OLE.new('WMPlayer.OCX')&lt;br /&gt;player.OpenPlayer('C:\alarm.mp3')&lt;br /&gt;&lt;/pre&gt; This will open a new instance of Windows Media Player that plays the selected song.&lt;br&gt;&lt;br&gt; Note that win32ole should be installed by default, and does not need to be installed as a gem.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/8113942018418819016/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/playing-mp3s-in-ruby-on-windows.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8113942018418819016'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/8113942018418819016'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/playing-mp3s-in-ruby-on-windows.html' title='Playing MP3s in Ruby on Windows'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4664167274065059563</id><published>2012-01-10T18:35:00.000-08:00</published><updated>2012-01-18T07:50:37.283-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Finance'/><category scheme='http://www.blogger.com/atom/ns#' term='Kaggle'/><category scheme='http://www.blogger.com/atom/ns#' term='Algorithms'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Introduction to Kaggle Algorithmic Trading Challenge</title><content type='html'>I recently participated in the Kaggle Algorithmic Trading Competition under the username VikP.  For those who do not know what &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt; is, it is a web site where individuals and corporations can host data analysis competitions.  This particular &lt;a href="http://www.kaggle.com/c/AlgorithmicTradingChallenge"&gt;competition&lt;/a&gt; involved the prediction of how the prices of 50,000 observations of 102 different securities at the tick level recovered after both buyer and seller initiated liquidity shocks.&lt;br&gt;&lt;br&gt; Each competitor was provided with approximately 750,000 rows of training data, each of which corresponded to a separate liquidity shock event. Each row contained observations of the bid and ask prices and an event indicator(trade event or quote event) for the 50 time points immediately preceding the liquidity shock, and the bid and ask prices alone for the 50 events immediately following the event.  There was also metadata provided, such as the number of shares that were traded to create the liquidity shock event and whether the event was buyer or seller initiated.&lt;br&gt;&lt;br&gt; There were some limitations to what predictions could be made from the data, notably because volume information was missing for each trade.  Because the evaluation metric was RMSE, which valued higher prices stocks much more highly lower priced ones, the competition became heavily dependent on filtering outliers.&lt;br&gt;&lt;br&gt; I had a great time participating, enjoyed the high level of competition, and highly recommend Kaggle competitions to aspiring data miners.  During the competition, I gained many insights into tick data and into analysis of financial data.  I will share some of these insights in the next week or so, but I just wanted to introduce the competition first!&lt;br&gt;&lt;br&gt;</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4664167274065059563/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/introduction-to-kaggle-algorithmic.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4664167274065059563'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4664167274065059563'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/introduction-to-kaggle-algorithmic.html' title='Introduction to Kaggle Algorithmic Trading Challenge'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4542393457123998720</id><published>2012-01-10T18:20:00.001-08:00</published><updated>2012-01-18T07:50:23.032-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Ruby'/><category scheme='http://www.blogger.com/atom/ns#' term='Windows'/><category scheme='http://www.blogger.com/atom/ns#' term='parallel'/><title type='text'>Create a New Ruby Process in Windows</title><content type='html'>I recently had a problem whereby I needed one Ruby program to spawn another Ruby program, but I did not need or want the two programs to interact after the second program was instantiated.  I solved this issue by using the system function in Ruby and the Windows start command.&lt;br&gt; &lt;pre class="ruby"&gt;system('start ruby.exe C:\script.rb')&lt;/pre&gt; This will create a new Ruby window, which will run the specified script, and close when it is complete.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/4542393457123998720/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/create-new-ruby-process-in-windows.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4542393457123998720'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/4542393457123998720'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/create-new-ruby-process-in-windows.html' title='Create a New Ruby Process in Windows'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3086433290133376275</id><published>2012-01-10T18:08:00.000-08:00</published><updated>2012-01-18T07:50:06.463-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#post'/><category scheme='http://www.blogger.com/atom/ns#' term='Ruby'/><category scheme='http://www.blogger.com/atom/ns#' term='Windows'/><category scheme='http://www.blogger.com/atom/ns#' term='R'/><title type='text'>Using R in Ruby</title><content type='html'>Integrating R into more traditional programming languages can be incredibly rewarding due to R's powerful built-in statistical tools, but it can also be extremely frustrating at times.  Thankfully, like much else to do with Ruby, integrating R and Ruby is quite a simple process.  To begin, install the gem rinruby and require it in your script.&lt;br&gt;&lt;br&gt; &lt;pre class="ruby"&gt;gem install rinruby&lt;/pre&gt; &lt;pre class="ruby"&gt;&lt;br /&gt;require 'rubygems'&lt;br /&gt;require 'rinruby'&lt;br /&gt;&lt;/pre&gt; There is no further installation or configuration required.  To evaluate an R expression, use the R.eval command.&lt;br&gt; &lt;pre class="ruby"&gt; R.eval "test=1*1"&lt;/pre&gt; To get a value from R, use the R.pull command.  &lt;pre class="ruby"&gt; test=R.pull "test" &lt;/pre&gt; If you are running large code blocks, it is easy to use the R.eval command with the R source function.  This will evaluate the R instructions in a given text file.  &lt;pre class="ruby"&gt;R.eval "source('C:/randomscript.R')"&lt;/pre&gt; If you are receiving the below error when attempting to load packages, your R library path is not properly set in your rinruby created instance of R.    &lt;pre class="ruby"&gt;Error in library(x) : there is no package called 'x'&lt;/pre&gt;  You can correct this using the R .libPaths() function.  &lt;pre class="ruby"&gt;R.eval('.libPaths("C:/Users/R library path")')&lt;/pre&gt; To find out what your R library paths are, you should use the Sys.getenv function in R.  &lt;pre class="ruby"&gt;Sys.getenv(c("R_LIBS", "R_LIBS_USER"))&lt;/pre&gt; You should make sure that you set your library paths in the R instance created by rinruby to the same paths that appear when you run the Sys.getenv command in your R GUI.  A list of other useful environment variables can be found &lt;a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/EnvVar.html"&gt;here&lt;/a&gt;.  Another useful function to use in conjunction with rinruby is the setwd command.  &lt;pre class="ruby"&gt;R.eval('setwd("C:/Users/Documents")')&lt;/pre&gt; If you replace the file path above with your R working directory path, rinruby will save files in the right spot.&lt;br&gt;&lt;br&gt; &lt;b&gt;Notes:&lt;/b&gt;&lt;br&gt;&lt;br&gt; When typing file paths into Ruby for use in R, one should always use the forward slash(/), or four backslashes(\\\\), as single and double backslashes will not escape properly.&lt;br&gt;&lt;br&gt; Note that using the R.pull command with multiple R instructions(or using it with the source() function) will often cause long runtimes, or will not complete.  Using the R.eval command to evaluate scripts, and then using the R.pull command to pull variables out after the script has finished executing will solve this issue.&lt;br&gt;&lt;br&gt; You can find more information on this at the &lt;a href="https://sites.google.com/a/ddahl.org/rinruby-users/"&gt;rinruby&lt;/a&gt; site.</content><link rel='replies' type='application/atom+xml' href='http://viksalgorithms.blogspot.com/feeds/3086433290133376275/comments/default' title='Post Comments'/><link rel='replies' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-r-in-ruby.html#comment-form' title='0 Comments'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3086433290133376275'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/posts/default/3086433290133376275'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-r-in-ruby.html' title='Using R in Ruby'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1357340698867684017</id><published>2012-01-18T03:03:23.452-08:00</published><updated>2012-01-18T03:03:23.452-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks, a useful introduction. This will be very u...</title><content type='html'>Thanks, a useful introduction. This will be very useful.&lt;br /&gt;I&amp;#39;ve just looked up the doSNOW package, there it says you should close the cluster before exiting R, is this very important?&lt;br /&gt;e.g. stopCluster(cl)</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/1357340698867684017'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/1357340698867684017'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html?showComment=1326884603452#c1357340698867684017' title=''/><author><name>Tom</name><uri>http://www.blogger.com/profile/07032784592547436653</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-103356724'/><gd:extendedProperty name='blogger.displayTime' value='January 18, 2012 at 3:03 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6310444034151696797</id><published>2012-01-18T03:42:59.502-08:00</published><updated>2012-01-18T03:42:59.502-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for your comment, Tom.  Although I have nev...</title><content type='html'>Thanks for your comment, Tom.  Although I have never used the stopCluster() function, I will have to defer to the package documentation in this situation.  I will edit my post to reflect this.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/6310444034151696797'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/6310444034151696797'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html?showComment=1326886979502#c6310444034151696797' title=''/><author><name>Vik</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 18, 2012 at 3:42 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-9147493302653463225</id><published>2012-01-18T15:18:58.783-08:00</published><updated>2012-01-18T15:18:58.783-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>I initially calculated the error incorrectly, and ...</title><content type='html'>I initially calculated the error incorrectly, and thus posted incorrect results.  I fixed this issue, and it should be fine now.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/9147493302653463225'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/9147493302653463225'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html?showComment=1326928738783#c9147493302653463225' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7265748392976191068' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 18, 2012 at 3:18 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7812617740158218066</id><published>2012-01-18T22:57:23.830-08:00</published><updated>2012-01-18T22:57:23.830-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hi, 

13:30 and 15:00 London time and is the time ...</title><content type='html'>Hi, &lt;br /&gt;&lt;br /&gt;13:30 and 15:00 London time and is the time that US economic data is released (08:30 and 10:00 eastern time respectively). This very often causes significant price movement. Not sure about 10:15.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4707394308354424625/comments/default/7812617740158218066'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4707394308354424625/comments/default/7812617740158218066'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html?showComment=1326956243830#c7812617740158218066' title=''/><author><name>Rob Hayward</name><uri>http://www.blogger.com/profile/07408506131607521085</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4707394308354424625' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4707394308354424625' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-636663703'/><gd:extendedProperty name='blogger.displayTime' value='January 18, 2012 at 10:57 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1552997203631764170</id><published>2012-01-19T09:32:59.991-08:00</published><updated>2012-01-19T09:32:59.991-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for the insight, Rob.  The 10:15 spike may ...</title><content type='html'>Thanks for the insight, Rob.  The 10:15 spike may just be noise, but there are a few too many outliers there for me to think it happened by chance.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4707394308354424625/comments/default/1552997203631764170'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4707394308354424625/comments/default/1552997203631764170'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html?showComment=1326994379991#c1552997203631764170' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/time-based-arbitrage-opportunities-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4707394308354424625' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4707394308354424625' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 19, 2012 at 9:32 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1487428325735585902</id><published>2012-01-20T10:52:53.820-08:00</published><updated>2012-01-20T10:52:53.820-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>very interesting read, seeing the data that you us...</title><content type='html'>very interesting read, seeing the data that you usually just see through the light of articles/journalist.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/8388541914380065480/comments/default/1487428325735585902'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/8388541914380065480/comments/default/1487428325735585902'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html?showComment=1327085573820#c1487428325735585902' title=''/><author><name>Jonas Wallin</name><uri>http://www.blogger.com/profile/08287649218894750860</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-8388541914380065480' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/8388541914380065480' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-440403945'/><gd:extendedProperty name='blogger.displayTime' value='January 20, 2012 at 10:52 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1067618466605155191</id><published>2012-01-22T16:51:45.589-08:00</published><updated>2012-01-22T16:51:45.589-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>RTextTools (http://cran.r-project.org/web/packages...</title><content type='html'>RTextTools (http://cran.r-project.org/web/packages/RTextTools/index.html) implements ensemble learning for text classification applications with nine algorithms.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/1067618466605155191'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/1067618466605155191'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1327279905589#c1067618466605155191' title=''/><author><name>timjurka</name><uri>http://www.blogger.com/profile/08383189583302771115</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1186847530'/><gd:extendedProperty name='blogger.displayTime' value='January 22, 2012 at 4:51 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-599651488123570358</id><published>2012-01-22T18:46:04.085-08:00</published><updated>2012-01-22T18:46:04.085-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Can you think of any other machine learning algori...</title><content type='html'>Can you think of any other machine learning algorithms that are so easy to parallelize?  Random forests are the only other example I can think of off the top of my head.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/599651488123570358'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/599651488123570358'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html?showComment=1327286764085#c599651488123570358' title=''/><author><name>Zach Mayer</name><uri>http://www.blogger.com/profile/17305384425953877966</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7265748392976191068' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-407276770'/><gd:extendedProperty name='blogger.displayTime' value='January 22, 2012 at 6:46 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-763456045216465670</id><published>2012-01-23T08:17:08.051-08:00</published><updated>2012-01-23T08:17:08.051-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks Jonas.  Would you be interested in seeing m...</title><content type='html'>Thanks Jonas.  Would you be interested in seeing more articles like this?  I definitely had fun writing this one.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/8388541914380065480/comments/default/763456045216465670'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/8388541914380065480/comments/default/763456045216465670'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html?showComment=1327335428051#c763456045216465670' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/analyzing-federal-bailout-recipients-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-8388541914380065480' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/8388541914380065480' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 23, 2012 at 8:17 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3301582514743635062</id><published>2012-01-23T08:22:55.999-08:00</published><updated>2012-01-23T08:22:55.999-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hey Zach:

Random forests are easy to run in paral...</title><content type='html'>Hey Zach:&lt;br /&gt;&lt;br /&gt;Random forests are easy to run in parallel because they rely on random selection of observations, and the splits are based on randomly selected subsets of predictors. This lends itself to multiple independent iterations.  Any other machine learning technique that uses the same principles can theoretically be run in parallel fairly easily.  However, I cannot think of any commonly used techniques(except for some &amp;quot;big data&amp;quot; implementations of certain algorithms) that lend themselves to easy parallelization.  On the other hand, the prediction/analytics phase after model generation does lend itself to easy parallelization, which gives me an idea for a post.  Thanks for the idea!</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/3301582514743635062'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/3301582514743635062'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html?showComment=1327335775999#c3301582514743635062' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7265748392976191068' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 23, 2012 at 8:22 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8552502691554071743</id><published>2012-01-24T06:16:49.458-08:00</published><updated>2012-01-24T06:16:49.458-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>The answer, rather than inherent evil, lies in the...</title><content type='html'>The answer, rather than inherent evil, lies in the nature of what &amp;quot;large&amp;quot; contracts represent for state governments.  These are largely state wide efforts, construction and IT notably, where there are few competent companies.  Given size and commitments, not all will be in a position to bid at any one time.  If a company has proven itself (for now, that&amp;#39;s a good thing) on prior contracts, then it will have an advantage over other large companies.  Work scope itself eliminates small companies.  I have no brief for L-M (they did some IT work for a department in my state...), by the way.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7517490718162120166/comments/default/8552502691554071743'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7517490718162120166/comments/default/8552502691554071743'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html?showComment=1327414609458#c8552502691554071743' title=''/><author><name>Robert Young</name><uri>http://www.blogger.com/profile/09056808374481236610</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='33' height='26' src='http://1.bp.blogspot.com/-Er9cWHhacCw/TgFoChg_y8I/AAAAAAAAACI/vqa9fbA02ko/s220/mecrop.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7517490718162120166' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7517490718162120166' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-2088302279'/><gd:extendedProperty name='blogger.displayTime' value='January 24, 2012 at 6:16 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1567620423710938011</id><published>2012-01-24T06:27:10.574-08:00</published><updated>2012-01-24T06:27:10.574-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for your insight, Robert.  I tried as much ...</title><content type='html'>Thanks for your insight, Robert.  I tried as much as possible to refrain from making any value judgements about the awards, other than stating that it is generally in the public interest to have a fair bidding process, as I don&amp;#39;t really have the experience in the sector to zero in on the reasons behind the provision of no-bid contracts.  It an interesting fact that 10 companies received 41% of the total federal spending in the state of Maryland for fiscal year 2011, but, as you said, it doesn&amp;#39;t necessarily signify anything.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7517490718162120166/comments/default/1567620423710938011'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7517490718162120166/comments/default/1567620423710938011'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html?showComment=1327415230574#c1567620423710938011' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/analyzing-us-government-contract-awards.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7517490718162120166' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7517490718162120166' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 24, 2012 at 6:27 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8341277391847397269</id><published>2012-01-27T05:04:06.776-08:00</published><updated>2012-01-27T05:04:06.776-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>You should definitively have a look at the iterato...</title><content type='html'>You should definitively have a look at the iterators and itertools packages to split more efficiently (i.e. less coding and more memory efficient) your matrix (see e.g iSplitRows)</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3736701907267312270/comments/default/8341277391847397269'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3736701907267312270/comments/default/8341277391847397269'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html?showComment=1327669446776#c8341277391847397269' title=''/><author><name>Antoine</name><uri>http://www.blogger.com/profile/06473004784530528598</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-3736701907267312270' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/3736701907267312270' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-990084139'/><gd:extendedProperty name='blogger.displayTime' value='January 27, 2012 at 5:04 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4971584912553295352</id><published>2012-01-30T18:24:57.648-08:00</published><updated>2012-01-30T18:24:57.648-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thank, Antoine.  I will definitely look into that....</title><content type='html'>Thank, Antoine.  I will definitely look into that.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3736701907267312270/comments/default/4971584912553295352'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3736701907267312270/comments/default/4971584912553295352'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html?showComment=1327976697648#c4971584912553295352' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-model-prediction-building.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-3736701907267312270' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/3736701907267312270' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 30, 2012 at 6:24 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1003092335906643997</id><published>2012-01-31T01:01:11.271-08:00</published><updated>2012-01-31T01:01:11.271-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Please, for the love of god, don&amp;#39;t install R a...</title><content type='html'>Please, for the love of god, don&amp;#39;t install R and LaTeX in Program Files. The space in the path can cause some really annoying bugs. I recommend creating a C:\bin folder for both of them as this will ensure that they can always find one another.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5897587368307715451/comments/default/1003092335906643997'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5897587368307715451/comments/default/1003092335906643997'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html?showComment=1328000471271#c1003092335906643997' title=''/><author><name>Disgruntled PhD</name><uri>http://www.blogger.com/profile/00926204336056169207</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5897587368307715451' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5897587368307715451' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-2121144216'/><gd:extendedProperty name='blogger.displayTime' value='January 31, 2012 at 1:01 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5653971096055863632</id><published>2012-01-31T02:26:44.537-08:00</published><updated>2012-01-31T02:26:44.537-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for this.  I looked more into it, and found...</title><content type='html'>Thanks for this.  I looked more into it, and found that installing LaTeX into a path with spaces can cause issues.  I updated my post accordingly.  I haven&amp;#39;t had any issue with R and paths with spaces.  I found this discussion when I looked into the issue:  http://r.789695.n4.nabble.com/inline-Rcpp-Problem-with-space-in-path-under-Windows-td3689982.html .  However, the R for Windows FAQ doesn&amp;#39;t recommend installing into a path with spaces, as you said.  It might be worth reinstalling to a directory with no spaces in the name.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5897587368307715451/comments/default/5653971096055863632'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5897587368307715451/comments/default/5653971096055863632'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html?showComment=1328005604537#c5653971096055863632' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/using-latex-r-and-sweave-to-create.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5897587368307715451' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5897587368307715451' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='January 31, 2012 at 2:26 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-9012359193202571531</id><published>2012-02-09T01:46:01.106-08:00</published><updated>2012-02-09T01:46:01.106-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Very well written, I am a novice in this stuff but...</title><content type='html'>Very well written, I am a novice in this stuff but your post was very succinct and informative. Keep sharing.&lt;br /&gt;&lt;br /&gt;~&lt;br /&gt;Shreyes</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/9012359193202571531'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/9012359193202571531'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1328780761106#c9012359193202571531' title=''/><author><name>Shreyes</name><uri>http://www.blogger.com/profile/02952702110986035135</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='18' src='http://4.bp.blogspot.com/-qxonS7T4lDQ/Tnu9rDO6QhI/AAAAAAAAAOM/H6gH-qheti4/s220/DSC09046.JPG'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-538773572'/><gd:extendedProperty name='blogger.displayTime' value='February 9, 2012 at 1:46 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5996601052266137118</id><published>2012-02-09T09:14:02.600-08:00</published><updated>2012-02-09T09:14:02.600-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks, Shreyes.  Good luck in the competition ;)....</title><content type='html'>Thanks, Shreyes.  Good luck in the competition ;).</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/5996601052266137118'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/5996601052266137118'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1328807642600#c5996601052266137118' title=''/><link rel='related' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/9012359193202571531'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='February 9, 2012 at 9:14 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1652482378496640478</id><published>2012-02-13T03:07:41.778-08:00</published><updated>2012-02-13T03:07:41.778-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>I am not that good in terms of mathematics nor in ...</title><content type='html'>I am not that good in terms of mathematics nor in any computation at all, but this one I think is a good intro into learning about some part inf finance. &lt;a href="http://www.360training.com" rel="nofollow"&gt;ethics and compliance training programs&lt;/a&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/1652482378496640478'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/1652482378496640478'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1329131261778#c1652482378496640478' title=''/><author><name>W. Home</name><uri>http://www.blogger.com/profile/18096929382280011780</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-521655336'/><gd:extendedProperty name='blogger.displayTime' value='February 13, 2012 at 3:07 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8211885676403463521</id><published>2012-03-17T11:44:56.893-07:00</published><updated>2012-03-17T11:44:56.893-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>I would like to know your opinion - if I improve s...</title><content type='html'>I would like to know your opinion - if I improve svm and randomForest with some searching for otimal parameters (e.g. through cross validation) do you think result of their ensemble will be improved in comparision with predictions made with default values for both models(I think, that you used default values)? And thanks for sharing, your blog is very good written. Milan</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/8211885676403463521'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/8211885676403463521'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1332009896893#c8211885676403463521' title=''/><author><name>gutompf</name><uri>http://www.blogger.com/profile/07598953103632713457</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1493593939'/><gd:extendedProperty name='blogger.displayTime' value='March 17, 2012 at 11:44 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-39415960577594285</id><published>2012-04-08T18:13:43.984-07:00</published><updated>2012-04-08T18:13:43.984-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hello Milan:

Thank you for the kind words, and so...</title><content type='html'>Hello Milan:&lt;br /&gt;&lt;br /&gt;Thank you for the kind words, and sorry that I took so long to get back to you.  Optimal parameter selection will certainly improve ensemble performance, although probably not to the exact extent that it will improve single model performance (in simple ensembling strategies, a single model can sometimes be much improved but not have much effect on the overall ensemble).  One caveat with parameter tuning, which cross validation can address, is the problem of overfitting.  Sometimes tuned parameters do not generalize well, and this can hurt single model and ensemble performance.  More advanced ensembling methods, such as stacking, will be less sensitive to this than a relatively more naive model that simply averages the constituent models.  The R packages e1071 and caret can be very helpful when selecting optimal parameters.&lt;br /&gt;&lt;br /&gt;Vik</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/39415960577594285'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/39415960577594285'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1333934023984#c39415960577594285' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='April 8, 2012 at 6:13 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5602377748483668754</id><published>2012-05-09T04:04:29.408-07:00</published><updated>2012-05-09T04:04:29.408-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Sorry if this is not relevant. Is there a way of u...</title><content type='html'>Sorry if this is not relevant. Is there a way of using a repository hosted on a local disk?</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3169893668697261422/comments/default/5602377748483668754'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3169893668697261422/comments/default/5602377748483668754'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html?showComment=1336561469408#c5602377748483668754' title=''/><author><name>Narayana Yaddanapudi</name><uri>http://www.blogger.com/profile/02610010044708402691</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-3169893668697261422' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/3169893668697261422' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-210969467'/><gd:extendedProperty name='blogger.displayTime' value='May 9, 2012 at 4:04 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4735770605891643448</id><published>2012-05-30T12:32:26.691-07:00</published><updated>2012-05-30T12:32:26.691-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hello Narayana:

Sorry for the late response.  You...</title><content type='html'>Hello Narayana:&lt;br /&gt;&lt;br /&gt;Sorry for the late response.  You can install a package from a file on local disk using install.packages by setting repos to NULL and using the filename instead of the package name.  Hope this helps!&lt;br /&gt;&lt;br /&gt;Vik</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3169893668697261422/comments/default/4735770605891643448'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/3169893668697261422/comments/default/4735770605891643448'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html?showComment=1338406346691#c4735770605891643448' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-3169893668697261422' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/3169893668697261422' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='May 30, 2012 at 12:32 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7325294154269778209</id><published>2012-06-02T06:32:09.520-07:00</published><updated>2012-06-02T06:32:09.520-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Mr Paruchuri,
As claimed in Soccernomics, the beau...</title><content type='html'>Mr Paruchuri,&lt;br /&gt;As claimed in Soccernomics, the beautiful game suffers from a lack of this rigorous, mathematical approach to applying economics and statistics to it. Perhaps you could apply your skills to the English Premiere League?</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5415591293840639618/comments/default/7325294154269778209'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5415591293840639618/comments/default/7325294154269778209'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html?showComment=1338643929520#c7325294154269778209' title=''/><author><name>Hasan</name><uri>http://www.blogger.com/profile/09631681950518688098</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5415591293840639618' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5415591293840639618' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1138717000'/><gd:extendedProperty name='blogger.displayTime' value='June 2, 2012 at 6:32 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-5117280683930420166</id><published>2012-06-02T08:42:39.893-07:00</published><updated>2012-06-02T08:42:39.893-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hello Hasan,

Thanks a lot for your comment, and t...</title><content type='html'>Hello Hasan,&lt;br /&gt;&lt;br /&gt;Thanks a lot for your comment, and the suggestion.  That is certainly a very interesting proposition, and I can look into it.  Do you have any good data sources for EPL match results?  A data api would be ideal, but I can also try web scraping, depending on the site.  Sadly, getting the data and getting it into the proper format for machine learning is more work than the machine learning code itself!&lt;br /&gt;&lt;br /&gt;Vik</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5415591293840639618/comments/default/5117280683930420166'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5415591293840639618/comments/default/5117280683930420166'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html?showComment=1338651759893#c5117280683930420166' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/predicting-nba-playoff-games-results.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5415591293840639618' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5415591293840639618' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='June 2, 2012 at 8:42 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1588443924298666819</id><published>2012-06-13T04:15:59.161-07:00</published><updated>2012-06-13T04:15:59.161-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hi, nice post. Which libraries are you using here?...</title><content type='html'>Hi, nice post. Which libraries are you using here?</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/1588443924298666819'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/1588443924298666819'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html?showComment=1339586159161#c1588443924298666819' title=''/><author><name>Gerrard_D</name><uri>http://www.blogger.com/profile/06134097507841297988</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-9007696286801530443' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-692537660'/><gd:extendedProperty name='blogger.displayTime' value='June 13, 2012 at 4:15 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-1914822474779168708</id><published>2012-06-13T04:19:09.609-07:00</published><updated>2012-06-13T04:19:09.609-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hello Gerrard:

I am using a function called load_...</title><content type='html'>Hello Gerrard:&lt;br /&gt;&lt;br /&gt;I am using a function called load_or_install that I discussed in a previous post (http://viksalgorithms.blogspot.com/2012/05/loading-andor-installing-packages.html), along with this code:&lt;br /&gt;&lt;br /&gt;load_or_install(c(&amp;quot;RODBC&amp;quot;,&amp;quot;corpora&amp;quot;,&amp;quot;ggplot2&amp;quot;,&amp;quot;tm&amp;quot;,&amp;quot;foreach&amp;quot;,&amp;quot;RColorBrewer&amp;quot;,&amp;quot;wordcloud&amp;quot;))&lt;br /&gt;&lt;br /&gt;Hope this helps.&lt;br /&gt;&lt;br /&gt;Vik</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/1914822474779168708'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/1914822474779168708'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html?showComment=1339586349609#c1914822474779168708' title=''/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-9007696286801530443' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='June 13, 2012 at 4:19 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-298427420333828823</id><published>2012-06-14T05:54:27.617-07:00</published><updated>2012-06-14T05:54:27.617-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for this great analysis and the code to rep...</title><content type='html'>Thanks for this great analysis and the code to replicate (or extend) it. That said, I think the punchlines get lost in the muddle of those word clouds. A horizontal bar chart or dot plot with the words shown in descending order by frequency would be so much easier to follow.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/298427420333828823'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/298427420333828823'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html?showComment=1339678467617#c298427420333828823' title=''/><author><name>Jay Ulfelder</name><uri>http://www.blogger.com/profile/18370458560135965232</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='21' height='32' src='http://2.bp.blogspot.com/-1pGJcouTHEo/TnNSQRpnNkI/AAAAAAAAA4E/uNByf5r1FsI/s220/Jay%2B2008.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-9007696286801530443' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1088821037'/><gd:extendedProperty name='blogger.displayTime' value='June 14, 2012 at 5:54 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3877413983063400781</id><published>2012-06-14T06:00:35.999-07:00</published><updated>2012-06-14T06:00:35.999-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for the feedback, Jay.  I&amp;#39;ll use those ...</title><content type='html'>Thanks for the feedback, Jay.  I&amp;#39;ll use those types of chart in the next post I want to make (which will compare Foreign Service/diplomatic writers to normal writers to see how word usage differs) and/or update this post with them.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/3877413983063400781'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/3877413983063400781'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html?showComment=1339678835999#c3877413983063400781' title=''/><link rel='related' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/9007696286801530443/comments/default/298427420333828823'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/finding-word-use-patterns-in-wikileaks.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-9007696286801530443' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/9007696286801530443' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='June 14, 2012 at 6:00 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-336900314758901076</id><published>2012-07-13T13:19:57.490-07:00</published><updated>2012-07-13T13:19:57.490-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Good work done... however how do you perform it wh...</title><content type='html'>Good work done... however how do you perform it when we want to do it for generic purpose....</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/336900314758901076'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/336900314758901076'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1342210797490#c336900314758901076' title=''/><author><name>Insatiable Warrior</name><uri>http://www.blogger.com/profile/09179095130515216596</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-21861293'/><gd:extendedProperty name='blogger.displayTime' value='July 13, 2012 at 1:19 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7444481213766260230</id><published>2012-07-16T15:00:25.557-07:00</published><updated>2012-07-16T15:00:25.557-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks.  This can be adapted to a generic purpose ...</title><content type='html'>Thanks.  This can be adapted to a generic purpose by altering the code, and changing the data source.  All of the analysis is generic, so this would be fairly easy to do.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/7444481213766260230'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/7444481213766260230'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1342476025557#c7444481213766260230' title=''/><link rel='related' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/336900314758901076'/><author><name>Vik Paruchuri</name><uri>http://www.blogger.com/profile/14436550620869189803</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-825849095'/><gd:extendedProperty name='blogger.displayTime' value='July 16, 2012 at 3:00 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-796990448660424950</id><published>2012-07-20T01:03:56.871-07:00</published><updated>2012-07-20T01:03:56.871-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>This technique is seriously vulnerable to 
artifac...</title><content type='html'>This technique is seriously vulnerable to &lt;br /&gt;artifacts. See an example at&lt;br /&gt;http://cpacker.org/culturomics</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/796990448660424950'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/796990448660424950'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1342771436871#c796990448660424950' title=''/><author><name>Charles Packer</name><uri>http://www.blogger.com/profile/03402255228713233324</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-23674233'/><gd:extendedProperty name='blogger.displayTime' value='July 20, 2012 at 1:03 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-520723472348634586</id><published>2012-08-09T19:38:34.467-07:00</published><updated>2012-08-09T19:38:34.467-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thanks for the post. It is helpful. I am learning ...</title><content type='html'>Thanks for the post. It is helpful. I am learning econometrics and always wonder how to check if these assumptions are satisfied. I look forward to reading other posts on this topic. Can I use the same method for panel data?&lt;br /&gt;&lt;br /&gt;Jing</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5452396106797238718/comments/default/520723472348634586'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5452396106797238718/comments/default/520723472348634586'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/r-regression-diagnostics-part-1.html?showComment=1344566314467#c520723472348634586' title=''/><author><name>Jing</name><uri>http://www.blogger.com/profile/00912884965348946769</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='31' src='http://4.bp.blogspot.com/-Wv2gsk8_6eQ/Tx-6ckBE0hI/AAAAAAAAD60/h5RTWQ4Sjtw/s220/earth440.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/r-regression-diagnostics-part-1.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5452396106797238718' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5452396106797238718' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1070133291'/><gd:extendedProperty name='blogger.displayTime' value='August 9, 2012 at 7:38 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6239824990435914005</id><published>2012-08-10T18:37:28.191-07:00</published><updated>2012-08-10T18:37:28.191-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hello
How do you find out about the competitions, ...</title><content type='html'>Hello&lt;br /&gt;How do you find out about the competitions, please?&lt;br /&gt;Thanks,&lt;br /&gt;Erin</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/6239824990435914005'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/6239824990435914005'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html?showComment=1344649048191#c6239824990435914005' title=''/><author><name>Erin</name><uri>http://www.blogger.com/profile/05155409012016419121</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7711723266060788067' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7711723266060788067' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-695633695'/><gd:extendedProperty name='blogger.displayTime' value='August 10, 2012 at 6:37 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3643866690387358073</id><published>2012-09-18T11:13:15.912-07:00</published><updated>2012-09-18T11:13:15.912-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Hi Vik,

Great writeup and very helpful!  One comm...</title><content type='html'>Hi Vik,&lt;br /&gt;&lt;br /&gt;Great writeup and very helpful!  One comment, I believe we typically sample with replacement as you initially mentioned in your first paragraph.  Your code examples however sample without replacement.  May want to add (replace=TRUE) in the sample function to keep consistency and avoid confusion.   </content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/3643866690387358073'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7265748392976191068/comments/default/3643866690387358073'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html?showComment=1347991995912#c3643866690387358073' title=''/><author><name>Yuda Zhu</name><uri>http://www.blogger.com/profile/11302519835937228190</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='32' src='//lh6.googleusercontent.com/-5mLIz7h3w58/AAAAAAAAAAI/AAAAAAAAACE/EfF8hCQiddk/s512-c/photo.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/build-your-own-bagging-function-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7265748392976191068' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7265748392976191068' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-595755111'/><gd:extendedProperty name='blogger.displayTime' value='September 18, 2012 at 11:13 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-298732386027874166</id><published>2012-09-28T01:52:32.053-07:00</published><updated>2012-09-28T01:52:32.053-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thank you so much. These are great learning materi...</title><content type='html'>Thank you so much. These are great learning materials for beginning model ensemblers =)</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/298732386027874166'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/4026568941847141175/comments/default/298732386027874166'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html?showComment=1348822352053#c298732386027874166' title=''/><author><name>kx</name><uri>http://www.blogger.com/profile/14527314400356292033</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/intro-to-ensemble-learning-in-r.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-4026568941847141175' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/4026568941847141175' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-254059910'/><gd:extendedProperty name='blogger.displayTime' value='September 28, 2012 at 1:52 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-918244307245821332</id><published>2012-12-15T07:15:39.688-08:00</published><updated>2012-12-15T07:15:39.688-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Investing in hedge funds will enable some lucky ma...</title><content type='html'> Investing in hedge funds will enable some lucky managers to enjoy an early retirement on their yachts. &lt;a href="http://creditcardppi.co/ppi-claims-and-employment-status" rel="nofollow"&gt;Best PPI Advice&lt;/a&gt;&lt;br /&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/6109649427445817068/comments/default/918244307245821332'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/6109649427445817068/comments/default/918244307245821332'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-3-4-2.html?showComment=1355584539688#c918244307245821332' title=''/><author><name>Muhammad Amir</name><uri>http://www.blogger.com/profile/01813150344575149922</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='32' src='//lh3.googleusercontent.com/-fKv12iZd6BM/AAAAAAAAAAI/AAAAAAAAACQ/UiNXDcuTOp4/s512-c/photo.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/nba-playoff-predictions-update-3-4-2.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-6109649427445817068' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/6109649427445817068' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-318683454'/><gd:extendedProperty name='blogger.displayTime' value='December 15, 2012 at 7:15 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6867506676204256013</id><published>2012-12-15T07:19:36.431-08:00</published><updated>2012-12-15T07:19:36.431-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>“It was one of the biggest violations of data prot...</title><content type='html'>“It was one of the biggest violations of data protection laws that we had ever seen. &lt;a href="http://ppiadvice.net/ways-ppi-was-mis-sold" rel="nofollow"&gt;Best PPI Claims Service&lt;/a&gt;&lt;br /&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7889460415866570979/comments/default/6867506676204256013'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7889460415866570979/comments/default/6867506676204256013'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/nba-playoffs-update-5-5-4.html?showComment=1355584776431#c6867506676204256013' title=''/><author><name>Muhammad Amir</name><uri>http://www.blogger.com/profile/01813150344575149922</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='32' src='//lh3.googleusercontent.com/-fKv12iZd6BM/AAAAAAAAAAI/AAAAAAAAACQ/UiNXDcuTOp4/s512-c/photo.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/nba-playoffs-update-5-5-4.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7889460415866570979' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7889460415866570979' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-318683454'/><gd:extendedProperty name='blogger.displayTime' value='December 15, 2012 at 7:19 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6173117995872339148</id><published>2013-01-07T16:17:10.508-08:00</published><updated>2013-01-07T16:17:10.508-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Your &amp;quot;total participants over time&amp;quot; grap...</title><content type='html'>Your &amp;quot;total participants over time&amp;quot; graph has mislabeled axis &amp;amp; title.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/6173117995872339148'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/6173117995872339148'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html?showComment=1357604230508#c6173117995872339148' title=''/><author><name>Paul Hobbs</name><uri>http://www.blogger.com/profile/02042967976252150460</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7711723266060788067' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7711723266060788067' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-2027090091'/><gd:extendedProperty name='blogger.displayTime' value='January 7, 2013 at 4:17 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-2981980291363771522</id><published>2013-02-07T03:44:41.093-08:00</published><updated>2013-02-07T03:44:41.093-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>This information is really useful! It’s just what ...</title><content type='html'>This information is really useful! It’s just what I was searching for&lt;br /&gt;&lt;a href="http://www.topqualityessays.com.au/" rel="nofollow"&gt; essays help&lt;/a&gt;&lt;br /&gt;&lt;a href="http://www.topqualityessays.com.au/" rel="nofollow"&gt;top quality essays&lt;/a&gt;&lt;br /&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/2981980291363771522'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/2981980291363771522'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1360237481093#c2981980291363771522' title=''/><author><name>alex james</name><uri>http://www.blogger.com/profile/05693635606005411798</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-182520809'/><gd:extendedProperty name='blogger.displayTime' value='February 7, 2013 at 3:44 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6987812949507319263</id><published>2013-02-10T06:18:17.009-08:00</published><updated>2013-02-10T06:18:17.009-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Vik,

I tried an experiment using the method outli...</title><content type='html'>Vik,&lt;br /&gt;&lt;br /&gt;I tried an experiment using the method outlined in your article. On my machine, running Windows 7, the serial version runs faster than the parallel (see outputs below). Any thoughts on why this might be?&lt;br /&gt;&lt;br /&gt;&amp;gt; detectCores()&lt;br /&gt;[1] 4&lt;br /&gt;&amp;gt; cl = makeCluster(4)&lt;br /&gt;&amp;gt; registerDoSNOW(cl)&lt;br /&gt;&amp;gt; system.time(foreach(x = letters, .combine=paste) %dopar% paste(rep(x, n)))&lt;br /&gt;   user  system elapsed &lt;br /&gt;  13.68    0.96   15.58 &lt;br /&gt;&amp;gt; system.time(foreach(x = letters, .combine=paste) %do% paste(rep(x, n)))&lt;br /&gt;   user  system elapsed &lt;br /&gt;  10.79    0.31   11.11 &lt;br /&gt;&amp;gt; stopCluster(cl)&lt;br /&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/6987812949507319263'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/6987812949507319263'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html?showComment=1360505897009#c6987812949507319263' title=''/><author><name>z man</name><uri>http://www.blogger.com/profile/11661089203132617805</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1891794778'/><gd:extendedProperty name='blogger.displayTime' value='February 10, 2013 at 6:18 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-7587457860355031059</id><published>2013-02-10T06:19:35.039-08:00</published><updated>2013-02-10T06:19:35.039-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>By the way, the value of n in the code quoted in m...</title><content type='html'>By the way, the value of n in the code quoted in my previous comment was 10^6.</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/7587457860355031059'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/7587457860355031059'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html?showComment=1360505975039#c7587457860355031059' title=''/><author><name>z man</name><uri>http://www.blogger.com/profile/11661089203132617805</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1891794778'/><gd:extendedProperty name='blogger.displayTime' value='February 10, 2013 at 6:19 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-8139207430653353354</id><published>2013-02-16T04:44:07.886-08:00</published><updated>2013-02-16T04:44:07.886-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>That’s a wonderful stuff to read out. I liked that...</title><content type='html'>That’s a wonderful stuff to read out. I liked that.&lt;br /&gt;&lt;a href="http://www.assignmentprovider.co.uk/" rel="nofollow"&gt;Assignment Help&lt;/a&gt;&lt;br /&gt;&lt;a href="http://www.assignmentprovider.co.uk/" rel="nofollow"&gt;Assignment Writing&lt;/a&gt;&lt;br /&gt;</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/8139207430653353354'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/8139207430653353354'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1361018647886#c8139207430653353354' title=''/><author><name>william doe</name><uri>http://www.blogger.com/profile/13851679302817961398</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-433515737'/><gd:extendedProperty name='blogger.displayTime' value='February 16, 2013 at 4:44 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-3271522044870416405</id><published>2013-03-06T07:24:42.669-08:00</published><updated>2013-03-06T07:24:42.669-08:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>z man, I can&amp;#39;t answer your question, but here ...</title><content type='html'>z man, I can&amp;#39;t answer your question, but here is some relevant info:&lt;br /&gt;&lt;br /&gt;http://stackoverflow.com/questions/11617506/domc-in-r-and-foreach-loop-not-working&lt;br /&gt;&lt;br /&gt;Thanks Vik for a useful post!</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/3271522044870416405'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/2026240887464657682/comments/default/3271522044870416405'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html?showComment=1362583482669#c3271522044870416405' title=''/><author><name>Patrick</name><uri>http://www.blogger.com/profile/06658507365116962150</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/01/parallel-r-loops-for-windows-and-linux.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-2026240887464657682' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/2026240887464657682' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-614691322'/><gd:extendedProperty name='blogger.displayTime' value='March 6, 2013 at 7:24 AM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-4302592630182613787</id><published>2013-04-08T21:32:54.866-07:00</published><updated>2013-04-08T21:32:54.866-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Nice post!

&amp;quot;As fresh people begin to compete...</title><content type='html'>Nice post!&lt;br /&gt;&lt;br /&gt;&amp;quot;As fresh people begin to compete, it seems that older users stop using the platform, whether it be from boredom, lack of time, or another issue. &amp;quot; - Can we relate it to how they perform in rank?  For the top people (say top 10%) who stop, I wonder if a Kaggle badge recognition was all they were looking for. For the bottom maybe 70% (benchmark level), perhaps they were newcomers to data science and gave up? </content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/4302592630182613787'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/7711723266060788067/comments/default/4302592630182613787'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html?showComment=1365481974866#c4302592630182613787' title=''/><author><name>Ling</name><uri>http://www.blogger.com/profile/05722710625359148148</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='http://img2.blogblog.com/img/b16-rounded.gif'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/08/how-many-data-scientists-are-there.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-7711723266060788067' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/7711723266060788067' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-540424722'/><gd:extendedProperty name='blogger.displayTime' value='April 8, 2013 at 9:32 PM'/></entry><entry><id>tag:blogger.com,1999:blog-8712623323474199769.post-6876474103089325936</id><published>2013-04-25T09:21:41.590-07:00</published><updated>2013-04-25T09:21:41.590-07:00</updated><category scheme='http://schemas.google.com/g/2005#kind' term='http://schemas.google.com/blogger/2008/kind#comment'/><title type='text'>Thank you very much for releasing this. It certain...</title><content type='html'>Thank you very much for releasing this. It certainly says a lot about method (which is very useful for me and my research). Cheers!</content><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/6876474103089325936'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8712623323474199769/5153720746573977786/comments/default/6876474103089325936'/><link rel='alternate' type='text/html' href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html?showComment=1366906901590#c6876474103089325936' title=''/><author><name>jalski</name><uri>http://www.blogger.com/profile/11632596291643796051</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='32' height='31' src='http://4.bp.blogspot.com/-_cHK_gIX480/Tvo_yOfLm7I/AAAAAAAAAI4/5-bEJ3WQLZE/s220/384650_10150442700668549_695798548_8853054_1988220047_n.jpg'/></author><thr:in-reply-to href='http://viksalgorithms.blogspot.com/2012/06/tracking-us-sentiments-over-time-in.html' ref='tag:blogger.com,1999:blog-8712623323474199769.post-5153720746573977786' source='http://www.blogger.com/feeds/8712623323474199769/posts/default/5153720746573977786' type='text/html'/><gd:extendedProperty name='blogger.itemClass' value='pid-1256236225'/><gd:extendedProperty name='blogger.displayTime' value='April 25, 2013 at 9:21 AM'/></entry></feed>